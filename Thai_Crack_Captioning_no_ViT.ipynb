{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6BEFxhkWfKm"
   },
   "source": [
    "## Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22700,
     "status": "ok",
     "timestamp": 1749879453241,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "CTPHueo8VLu7",
    "outputId": "ac3696b2-111d-47d1-aa75-2bea0fefae58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi5GTL7V34Vp"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12989,
     "status": "ok",
     "timestamp": 1749879466230,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "pbm2DCED33su",
    "outputId": "f530e9f6-63cd-4d6f-d1fb-308fbb9f2f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "drive  sample_data  thsarabunnew-webfont.ttf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pythainlp   rouge-score nltk deepcut optuna gradio -q\n",
    "!wget -q https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
    "!ls\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge2lPFmJ37hB"
   },
   "source": [
    "## CNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9492,
     "status": "ok",
     "timestamp": 1749879475724,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "0nwZHBkBWhYN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet50\", pretrained=True, weight_path=None, map_location=None):\n",
    "        super().__init__()\n",
    "        backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "        # Initial layers: Conv1, BN, ReLU, MaxPool\n",
    "        self.conv1 = backbone.conv1\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.relu = backbone.relu\n",
    "        self.maxpool = backbone.maxpool\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "\n",
    "        # Load weights from custom path\n",
    "        if weight_path is not None:\n",
    "            state = torch.load(weight_path, map_location=map_location)\n",
    "            # ถ้า save state dict แบบ model.state_dict()\n",
    "            if 'state_dict' in state:\n",
    "                # รองรับ checkpoint บางแบบ เช่น {'state_dict': ...}\n",
    "                state = state['state_dict']\n",
    "            self.load_state_dict(state, strict=False)  # strict=False ในกรณีที่ layer ไม่ตรงเป๊ะ\n",
    "            print(f\"Loaded CNN weights from {weight_path}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWd3vZ3Y390e"
   },
   "source": [
    "### test CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7667,
     "status": "ok",
     "timestamp": 1749879483394,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "6j1JPjxPX00G",
    "outputId": "c6a78133-a6af-4dd7-d230-50584789d724"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN weights from /content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\n",
      "Test passed: Feature shape is correct.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ใส่ path ไฟล์ weights (หรือ None ถ้าไม่โหลด custom weight)\n",
    "weight_path = \"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\"  # เปลี่ยนเป็น path จริง ถ้ามี\n",
    "\n",
    "# สร้าง encoder (โหลด weights ถ้าระบุ)\n",
    "encoder = CNNEncoder(\"resnet50\", pretrained=False, weight_path=weight_path, map_location='cpu')\n",
    "\n",
    "# ทดสอบ forward\n",
    "dummy_input = torch.randn(2, 3, 224, 224)  # batch_size=2, RGB, 224x224\n",
    "Fc = encoder(dummy_input)\n",
    "# assert ตรวจสอบ shape\n",
    "expected_shape = (2, 2048, 7, 7)  # สำหรับ resnet50\n",
    "assert Fc.shape == expected_shape, f\"Expected {expected_shape}, got {Fc.shape}\"\n",
    "\n",
    "print(\"Test passed: Feature shape is correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzpkWTMQ4BsZ"
   },
   "source": [
    "## FasterRCNNBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749879483413,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "cb6JPPFLX1WA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "class FasterRCNNBackbone(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_path=None, out_size=14, map_location='cpu'):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        if pretrained_path is not None:\n",
    "            state = torch.load(pretrained_path, map_location=map_location)\n",
    "            try:\n",
    "                self.model.load_state_dict(state)\n",
    "                print(f\"Loaded FasterRCNN weights from {pretrained_path}\")   # <-- เพิ่มตรงนี้\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Cannot load state dict: {e}\")\n",
    "        self.backbone = self.model.backbone\n",
    "        self.out_channels = self.backbone.out_channels\n",
    "        self.out_size = out_size\n",
    "        self.feature_adj = nn.Sequential(\n",
    "            nn.Conv2d(self.out_channels, 256, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, boxes=None, detect=False):\n",
    "        if detect:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                x = [img for img in x]\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            features = self.backbone(x)['0']  # (B, 256, H, W)\n",
    "            B = features.shape[0]\n",
    "            device = features.device\n",
    "            if boxes is None:\n",
    "                H, W = features.shape[2], features.shape[3]\n",
    "                boxes = [torch.tensor([[0, 0, W-1, H-1]], dtype=torch.float, device=device) for _ in range(B)]\n",
    "                boxes = torch.cat([\n",
    "                    torch.cat([torch.full((1, 1), i, device=device), b], dim=1)\n",
    "                    for i, b in enumerate(boxes)\n",
    "                ], dim=0)\n",
    "            roi_features = roi_align(features, boxes, output_size=(self.out_size, self.out_size))\n",
    "            roi_features = self.feature_adj(roi_features)\n",
    "            return roi_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaHVHHa34DdP"
   },
   "source": [
    "### test FasterRCNNBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7872,
     "status": "ok",
     "timestamp": 1749879491294,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "a_PYL9H0Yy5C",
    "outputId": "15624bd1-844d-4097-81f2-fd0ba06e1d71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 209MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FasterRCNN weights from /content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_classes = 4  # background + bridge_damage (แก้ตาม dataset)\n",
    "pretrained_path = \"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\"\n",
    "faster_rcnn_encoder = FasterRCNNBackbone(num_classes, pretrained_path=pretrained_path, map_location='cpu')\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "Ff = faster_rcnn_encoder(dummy_input)\n",
    "# สมมติขนาด output ที่คาดหวัง เช่น (2, 256, 56, 56) ขึ้นกับ backbone\n",
    "assert Ff.shape[0] == 2 and Ff.shape[1] == 256, f\"Unexpected shape: {Ff.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvwfGW9d4Fel"
   },
   "source": [
    "## FeatureFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1749879491297,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "0AM961IDZDVC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureFusion(nn.Module):\n",
    "    def __init__(self, in_cnn, in_frcnn, in_vit, out_c=512, reduction=16):\n",
    "        \"\"\"\n",
    "        in_cnn: channels of CNN feature map (Fc)\n",
    "        in_frcnn: channels of Faster R-CNN feature map (Ff)\n",
    "        in_vit: channels of ViT feature map (Fv)\n",
    "        out_c: common channel dimension after adjustment\n",
    "        reduction: for bottleneck in attention mechanism\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.adj_conv_cnn = nn.Conv2d(in_cnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_frcnn = nn.Conv2d(in_frcnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_vit = nn.Conv2d(in_vit, out_c, kernel_size=1)\n",
    "\n",
    "        self.pw_conv1 = nn.Conv2d(3*out_c, 3*out_c // reduction, kernel_size=1)\n",
    "        self.pw_conv2 = nn.Conv2d(3*out_c // reduction, 3*out_c, kernel_size=1)\n",
    "\n",
    "    def forward(self, Fc, Ff, Fv):\n",
    "        \"\"\"\n",
    "        Fc: (B, in_cnn, H, W)  -- CNN feature\n",
    "        Ff: (B, in_frcnn, H, W) -- Faster R-CNN feature\n",
    "        Fv: (B, in_vit, 1, 1)   -- ViT feature (หรือ (B, in_vit, H, W) ก็ได้)\n",
    "        Returns: (B, 3*out_c, H, W) fused features (weighted)\n",
    "        \"\"\"\n",
    "        # 1. ปรับขนาด channel ให้เท่ากัน\n",
    "        Fc_adj = self.adj_conv_cnn(Fc)     # (B, out_c, H, W)\n",
    "        Ff_adj = self.adj_conv_frcnn(Ff)   # (B, out_c, H, W)\n",
    "        Fv_adj = self.adj_conv_vit(Fv)     # (B, out_c, 1, 1) (อาจจะเป็น (B, out_c, H, W) ก็ได้)\n",
    "\n",
    "        # 2. ถ้า Fv เป็น (B, out_c, 1, 1) —> upsample ให้เท่ากับ (H, W)\n",
    "        H, W = Fc_adj.shape[2], Fc_adj.shape[3]\n",
    "        if Fv_adj.shape[2:] != (H, W):\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=(H, W), mode='nearest')\n",
    "\n",
    "        # 3. concat along channel\n",
    "        F_concat = torch.cat([Fc_adj, Ff_adj, Fv_adj], dim=1)  # (B, 3*out_c, H, W)\n",
    "\n",
    "        # 4. Channel Attention (Squeeze-and-Excitation)\n",
    "        FGAP = F.adaptive_avg_pool2d(F_concat, (1, 1))\n",
    "        Fpw1 = F.relu(self.pw_conv1(FGAP))\n",
    "        Fpw2 = torch.sigmoid(self.pw_conv2(Fpw1))\n",
    "        F_weighted = F_concat * Fpw2\n",
    "        return F_weighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BivdGfIO4G73"
   },
   "source": [
    "### test FeatureFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1749879491310,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "E2bfU0jGZie0",
    "outputId": "76eabdbd-ef09-4445-dca0-5774117973c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused feature shape: torch.Size([2, 1536, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "# Suppose Fc is from CNN (B, 2048, 7, 7), Ff is from FPN of Faster R-CNN (B, 256, 7, 7)\n",
    "# Set out_c=512 (หรือเลือกเท่าที่ต้องการ)\n",
    "fusion = FeatureFusion(in_cnn=2048, in_frcnn=256, in_vit=768, out_c=512)\n",
    "Fc = torch.randn(2, 2048, 7, 7)   # ResNet\n",
    "Ff = torch.randn(2, 256, 7, 7)    # Faster R-CNN\n",
    "Fv = torch.randn(2, 768, 1, 1)    # ViT (B, 768, 1, 1)\n",
    "F_fused = fusion(Fc, Ff, Fv)\n",
    "print(\"Fused feature shape:\", F_fused.shape)  # (2, 1536, 7, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYw8MSEs4IuX"
   },
   "source": [
    "## FeatureFusionWithRecalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1749879491362,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "YIz-v4CwZjYh"
   },
   "outputs": [],
   "source": [
    "class FeatureFusionWithRecalibration(nn.Module):\n",
    "    def __init__(self, in_cnn, in_frcnn, out_c=512, reduction=16):\n",
    "        super().__init__()\n",
    "        self.adj_conv_cnn = nn.Conv2d(in_cnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_frcnn = nn.Conv2d(in_frcnn, out_c, kernel_size=1)\n",
    "\n",
    "        self.pw_conv1 = nn.Conv2d(2*out_c, 2*out_c // reduction, kernel_size=1)\n",
    "        self.pw_conv2 = nn.Conv2d(2*out_c // reduction, 2*out_c, kernel_size=1)\n",
    "\n",
    "    def forward(self, Fc, Ff):\n",
    "        # Adjust channels\n",
    "        Fc_adj = self.adj_conv_cnn(Fc)      # (B, C, H, W)\n",
    "        Ff_adj = self.adj_conv_frcnn(Ff)    # (B, C, H, W)\n",
    "\n",
    "        # Resize spatial dim if needed\n",
    "        H, W = Fc_adj.shape[2:]\n",
    "        if Ff_adj.shape[2:] != (H, W):\n",
    "            Ff_adj = F.interpolate(Ff_adj, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Concat\n",
    "        F_concat = torch.cat([Fc_adj, Ff_adj], dim=1)    # (B, 2C, H, W)\n",
    "\n",
    "        # Squeeze & Excitation (channel attention)\n",
    "        FGAP = F.adaptive_avg_pool2d(F_concat, (1, 1))   # (B, 2C, 1, 1)\n",
    "        Fpw1 = F.relu(self.pw_conv1(FGAP))               # (B, 2C//r, 1, 1)\n",
    "        Fpw2 = torch.sigmoid(self.pw_conv2(Fpw1))        # (B, 2C, 1, 1)\n",
    "        wc, wf = torch.chunk(Fpw2, 2, dim=1)             # (B, C, 1, 1) x2\n",
    "\n",
    "        # Recalibrate\n",
    "        Fc_recal = Fc_adj * wc\n",
    "        Ff_recal = Ff_adj * wf\n",
    "\n",
    "        # Fusion (คุณสามารถปรับสูตรตามต้องการ)\n",
    "        Ffinal_c = Fc_adj + Ff_recal\n",
    "        Ffinal_f = Ff_adj + Fc_recal\n",
    "\n",
    "        Ffinal = torch.cat([Ffinal_c, Ffinal_f], dim=1)  # (B, 2C, H, W)\n",
    "        return Ffinal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in9rL_xG4KXV"
   },
   "source": [
    "### test FeatureFusionWithRecalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749879491369,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "Z8q8ZXeZaXzm",
    "outputId": "23db084f-968f-48b9-f9ed-d47413d99feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1024, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "module = FeatureFusionWithRecalibration(in_cnn=2048, in_frcnn=256, out_c=512)\n",
    "Fc = torch.randn(2, 2048, 7, 7)\n",
    "Ff = torch.randn(2, 256, 7, 7)\n",
    "Ffinal = module(Fc, Ff)\n",
    "print(\"Output shape:\", Ffinal.shape)  # (2, 1536, 7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSWHEuWO4MaF"
   },
   "source": [
    "## CorrelationAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1749879491511,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "6mGRnWGSf1dc"
   },
   "outputs": [],
   "source": [
    "class CorrelationAwareAttention(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, embed_dim, num_sources=2):\n",
    "        super().__init__()\n",
    "        # Embedding layer for Ffinal (ลด channel)\n",
    "        self.embedding = nn.Conv2d(feat_dim, embed_dim, kernel_size=1)\n",
    "        # Attention transforms\n",
    "        self.W_e = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w_a = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.num_sources = num_sources\n",
    "\n",
    "    def forward(self, Ffinal, Fc_adj, Ff_adj, h_t):\n",
    "        # Ffinal: (B, 2C, H, W)\n",
    "        # Fc_adj, Ff_adj: (B, C, H, W)\n",
    "        # h_t: (B, hidden_dim)\n",
    "\n",
    "        # 1) Embedding multi-level feature\n",
    "        Eemb = self.embedding(Ffinal)  # (B, embed_dim, H, W)\n",
    "        B, E, H, W = Eemb.shape\n",
    "        N = H * W\n",
    "\n",
    "        # Resize spatial if needed\n",
    "        if Fc_adj.shape[2:] != Ff_adj.shape[2:]:\n",
    "            Ff_adj = F.interpolate(Ff_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Flatten spatial dims for attention computation\n",
    "        Eemb_flat = Eemb.view(B, E, N).transpose(1, 2)  # (B, N, E)\n",
    "\n",
    "        # 2) Correlation Matrix (แค่ CNN <-> FRCNN)\n",
    "        C = Fc_adj.shape[1]  # channel dim (should == out_c)\n",
    "        Ec = Fc_adj.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "        Ef = Ff_adj.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "\n",
    "        Rc = torch.bmm(Ec, Ef.transpose(1, 2))  # (B, N, N)\n",
    "        Rc_norm = torch.softmax(Rc, dim=-1)\n",
    "\n",
    "        # 3) Traditional Attention Score\n",
    "        Weei = self.W_e(Eemb_flat)                 # (B, N, hidden_dim)\n",
    "        Whht = self.W_h(h_t).unsqueeze(1)          # (B, 1, hidden_dim)\n",
    "        et = self.w_a(torch.tanh(Weei + Whht)).squeeze(-1)  # (B, N)\n",
    "\n",
    "        # 4) Adjust attention by correlation\n",
    "        e_corr_c = torch.bmm(Rc_norm, et.unsqueeze(2)).squeeze(-1)  # (B, N)\n",
    "        st = et + e_corr_c                                          # (B, N)\n",
    "        alpha = torch.softmax(st, dim=1)                            # (B, N)\n",
    "\n",
    "        # 5) Weighted sum for context vector\n",
    "        zt = torch.bmm(alpha.unsqueeze(1), Eemb_flat).squeeze(1)  # (B, embed_dim)\n",
    "\n",
    "        return zt, alpha  # (B, embed_dim), (B, N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8nyd3j8NCWg"
   },
   "source": [
    "### test CorrelationAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1749879491552,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "P-kAeqjX2bhN",
    "outputId": "49623a65-5bd0-4d10-f50a-0cb7b1457181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zt: torch.Size([2, 256]) alpha: torch.Size([2, 49])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 2, 512, 7, 7\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "module = CorrelationAwareAttention(feat_dim=3*C, hidden_dim=hidden_dim, embed_dim=embed_dim)\n",
    "Ffinal = torch.randn(B, 3*C, H, W)\n",
    "Fc_adj = torch.randn(B, C, H, W)\n",
    "Ff_adj = torch.randn(B, C, H, W)\n",
    "h_t = torch.randn(B, hidden_dim)\n",
    "zt, alpha = module(Ffinal, Fc_adj, Ff_adj, h_t)\n",
    "print(\"zt:\", zt.shape, \"alpha:\", alpha.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELR_TOL8ViRl"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1B_VYsD4PrN"
   },
   "source": [
    "## LSTMDecoderCorrAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1749879491594,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "6T7MqDIWaYM0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTMDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions, states):\n",
    "        # zt: (B, embed_dim)  context from attention\n",
    "        batch_size, seq_len = captions.size()\n",
    "        outputs = []\n",
    "        h, c = states\n",
    "        for t in range(seq_len):\n",
    "            xt = self.word_embedding(captions[:, t])   # (B, embed_dim)\n",
    "            lstm_input = torch.cat([xt, zt], dim=1)    # (B, 2*embed_dim)\n",
    "            h, c = self.lstm(lstm_input, (h, c))       # h, c: (B, hidden_dim)\n",
    "            out = self.fc(h)    # (B, vocab_size)\n",
    "            outputs.append(out.unsqueeze(1))           # (B, 1, vocab_size)\n",
    "        outputs = torch.cat(outputs, dim=1)            # (B, seq_len, vocab_size)\n",
    "        return outputs, (h, c)                         # <=== สำคัญ!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY-1d2QCHk5D"
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1749879491694,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "Bn2RAhK0HkW6"
   },
   "outputs": [],
   "source": [
    "class GRUDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_sources=1):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # ถ้า context มาจากหลาย source: zt size = num_sources * embed_dim\n",
    "        self.gru = nn.GRUCell(embed_dim + num_sources*embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions, h_t):\n",
    "        \"\"\"\n",
    "        zt: (B, num_sources*embed_dim)\n",
    "        captions: (B, seq_len)\n",
    "        h_t: (B, hidden_dim)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        B, seq_len = captions.shape\n",
    "        for t in range(seq_len):\n",
    "            xt = self.word_embedding(captions[:, t])         # (B, embed_dim)\n",
    "            gru_input = torch.cat([xt, zt], dim=1)           # (B, embed_dim + num_sources*embed_dim)\n",
    "            h_t = self.gru(gru_input, h_t)                   # (B, hidden_dim)\n",
    "            out = self.fc(h_t)                               # (B, vocab_size)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "        return torch.cat(outputs, dim=1), h_t  # (B, seq_len, vocab_size), (B, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1nFLYKoLFRM"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1749879491715,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "N4e5mLbEN_o-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class TransformerDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers=2, nhead=4, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # use [batch, seq, dim]\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions):\n",
    "        # รองรับ shape (seq_len,), (1, seq_len), (B, seq_len), (B, 1, seq_len)\n",
    "        captions = captions.long()\n",
    "        if captions.dim() == 1:\n",
    "            captions = captions.unsqueeze(0)  # (1, seq_len)\n",
    "        elif captions.dim() == 3:\n",
    "            # เช่น (B, 1, seq_len)\n",
    "            captions = captions.squeeze(1)\n",
    "        # ตอนนี้ captions เป็น (B, seq_len)\n",
    "        B, seq_len = captions.shape\n",
    "\n",
    "        cap_emb = self.word_embedding(captions)         # (B, seq_len, embed_dim)\n",
    "        cap_emb = self.pos_encoder(cap_emb)             # add pos encoding\n",
    "\n",
    "        # ทำให้ zt เป็น (B, 1, embed_dim)\n",
    "        if zt.dim() == 2:\n",
    "            zt = zt.unsqueeze(1)  # (B, 1, embed_dim)\n",
    "\n",
    "        out = self.transformer_decoder(\n",
    "            cap_emb,             # tgt (B, seq_len, embed_dim)\n",
    "            zt                   # memory (B, 1, embed_dim)\n",
    "        )\n",
    "        out = self.fc(out)  # (B, seq_len, vocab_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKICNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1749879491766,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "XJvWD30EC_fM"
   },
   "outputs": [],
   "source": [
    "class DKICNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_backbone=\"resnet50\", cnn_pretrained=False,\n",
    "        num_classes=4, out_c=512, reduction=16,\n",
    "        embed_dim=256, hidden_dim=512, vocab_size=1000,\n",
    "        severity_classes=4, cnn_pretrained_path=None,\n",
    "        frcnn_pretrained_path=None,\n",
    "        decoder_type=\"gru\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder_type = decoder_type\n",
    "        self.cnn_encoder = CNNEncoder(cnn_backbone, pretrained=cnn_pretrained, weight_path=cnn_pretrained_path)\n",
    "        self.frcnn_encoder = FasterRCNNBackbone(num_classes=num_classes, pretrained_path=frcnn_pretrained_path)\n",
    "\n",
    "        self.fusion = FeatureFusionWithRecalibration(\n",
    "            in_cnn=2048, in_frcnn=256, out_c=out_c, reduction=reduction  # <-- ไม่มี in_vit\n",
    "        )\n",
    "        self.attn = CorrelationAwareAttention(\n",
    "            feat_dim=2 * out_c, hidden_dim=hidden_dim, embed_dim=embed_dim  # <-- 2*out_c\n",
    "        )\n",
    "\n",
    "        if self.decoder_type == \"gru\":\n",
    "            self.decoder = GRUDecoderCorrAttn(embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size)\n",
    "        elif self.decoder_type == \"lstm\":\n",
    "            self.decoder = LSTMDecoderCorrAttn(embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size)\n",
    "        elif self.decoder_type == \"transformer\":\n",
    "            self.decoder = TransformerDecoderCorrAttn(\n",
    "                embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size,\n",
    "                nhead=2, num_layers=1, dropout=0.25\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"decoder_type '{decoder_type}' not supported\")\n",
    "\n",
    "        self.severity_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.Linear(2 * out_c, 128), nn.ReLU(), nn.Linear(128, severity_classes)\n",
    "        )\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.Linear(2 * out_c, 128), nn.ReLU(), nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, states=None):\n",
    "        Fc = self.cnn_encoder(images)\n",
    "        Ff = self.frcnn_encoder(images)\n",
    "        Ffinal = self.fusion(Fc, Ff)  # <-- ไม่มี Fv\n",
    "        Fc_adj = self.fusion.adj_conv_cnn(Fc)\n",
    "        Ff_adj = self.fusion.adj_conv_frcnn(Ff)\n",
    "        B = images.size(0)\n",
    "        hidden_dim = self.attn.W_h.in_features\n",
    "        h_t = torch.zeros(B, hidden_dim, device=images.device)\n",
    "        zt, alpha = self.attn(Ffinal, Fc_adj, Ff_adj, h_t)  # <-- ไม่มี Fv_adj\n",
    "        if self.decoder_type == \"gru\":\n",
    "            output, _ = self.decoder(zt, captions, h_t)\n",
    "        elif self.decoder_type == \"lstm\":\n",
    "            c_t = torch.zeros(B, hidden_dim, device=images.device)\n",
    "            output, _ = self.decoder(zt, captions, (h_t, c_t))\n",
    "        else:\n",
    "            output = self.decoder(zt, captions)\n",
    "        severity_logits = self.severity_head(Ffinal)\n",
    "        bbox_pred = self.bbox_head(Ffinal)\n",
    "        return output, alpha, severity_logits, bbox_pred\n",
    "\n",
    "    def top_k_temperature_sampling(self, logits, top_k=1, temperature=1.0):\n",
    "        logits = logits / temperature\n",
    "        values, indices = torch.topk(logits, top_k)\n",
    "        probs = torch.softmax(values, dim=-1)\n",
    "        idx = torch.multinomial(probs, 1)\n",
    "        next_token = indices[idx]\n",
    "        return next_token.item()\n",
    "\n",
    "    def generate(self, image, max_len=50, sos_token=2, eos_token=3, device='cuda', top_k=1, temperature=1.0, repetition_penalty=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "\n",
    "            Fc = self.cnn_encoder(image)\n",
    "            Ff = self.frcnn_encoder(image)\n",
    "            Ffinal = self.fusion(Fc, Ff)\n",
    "            Fc_adj = self.fusion.adj_conv_cnn(Fc)\n",
    "            Ff_adj = self.fusion.adj_conv_frcnn(Ff)\n",
    "            hidden_dim = self.attn.W_h.in_features\n",
    "            h_t = torch.zeros(1, hidden_dim, device=device)\n",
    "            zt, _ = self.attn(Ffinal, Fc_adj, Ff_adj, h_t)\n",
    "\n",
    "            severity_logits = self.severity_head(Ffinal)\n",
    "            bbox_pred = self.bbox_head(Ffinal)\n",
    "            severity_class = severity_logits.argmax(dim=1).item()\n",
    "            bbox_pred = bbox_pred.squeeze(0).cpu().numpy().tolist()\n",
    "\n",
    "            input_token = torch.tensor([sos_token], device=device).unsqueeze(0)\n",
    "            outputs = []\n",
    "\n",
    "            if self.decoder_type == \"gru\":\n",
    "                dec_state = h_t\n",
    "            elif self.decoder_type == \"lstm\":\n",
    "                c_t = torch.zeros(1, hidden_dim, device=device)\n",
    "                dec_state = (h_t, c_t)\n",
    "            elif self.decoder_type == \"transformer\":\n",
    "                memory = zt.unsqueeze(1)  # [1, 1, embed_dim]\n",
    "                tgt_tokens = [sos_token]\n",
    "            else:\n",
    "                raise NotImplementedError(\"Decoder type not supported.\")\n",
    "\n",
    "            for step in range(max_len):\n",
    "                if self.decoder_type in {\"gru\", \"lstm\"}:\n",
    "                    xt = self.decoder.word_embedding(input_token).squeeze(1)\n",
    "                    rnn_input = torch.cat([xt, zt], dim=1)\n",
    "                    if self.decoder_type == \"gru\":\n",
    "                        dec_state = self.decoder.gru(rnn_input, dec_state)\n",
    "                        logits = self.decoder.fc(dec_state)[0]\n",
    "                    else:\n",
    "                        h, c = dec_state\n",
    "                        h, c = self.decoder.lstm(rnn_input, (h, c))\n",
    "                        dec_state = (h, c)\n",
    "                        logits = self.decoder.fc(h)[0]\n",
    "                elif self.decoder_type == \"transformer\":\n",
    "                    tgt_seq = torch.tensor([tgt_tokens], device=device).long()\n",
    "                    logits = self.decoder(zt, tgt_seq)  # [1, seq_len, vocab_size]\n",
    "                    logits = logits[:, -1, :].squeeze(0)\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "\n",
    "                # repetition_penalty\n",
    "                if outputs:\n",
    "                    logits = logits.clone()\n",
    "                    for prev_token in set(outputs):\n",
    "                        logits[prev_token] /= repetition_penalty\n",
    "\n",
    "                next_token = self.top_k_temperature_sampling(logits, top_k=top_k, temperature=temperature)\n",
    "                if next_token == eos_token:\n",
    "                    break\n",
    "                outputs.append(next_token)\n",
    "                if self.decoder_type == \"transformer\":\n",
    "                    tgt_tokens.append(next_token)\n",
    "                else:\n",
    "                    input_token = torch.tensor([[next_token]], device=logits.device)\n",
    "\n",
    "            return outputs, severity_class, bbox_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oaAXNsJWJFT"
   },
   "source": [
    "### test DKCI with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6150,
     "status": "ok",
     "timestamp": 1749879497917,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "p4YrvC-s3hl4",
    "outputId": "ee6759a6-0e0e-4d61-c661-d872555ea39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN weights from /content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\n",
      "Loaded FasterRCNN weights from /content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\n",
      "DKICNet ready! Forward pass OK.\n",
      "Loaded CNN weights from /content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\n",
      "Loaded FasterRCNN weights from /content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\n",
      "DKICNet ready! Forward pass OK. Decoder type = lstm\n",
      "Loaded CNN weights from /content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\n",
      "Loaded FasterRCNN weights from /content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\n",
      "DKICNet ready! Forward pass OK. Decoder type = transformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 10000\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    decoder_type=\"gru\",\n",
    ")\n",
    "\n",
    "# Dummy input\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10))\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "# --- Assertion Check ---\n",
    "assert output.shape[0] == 2, \"Batch size ไม่ตรง\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK.\")\n",
    "\n",
    "\n",
    "\n",
    "#---- test LSTM decodeer ----#\n",
    "vocab_size = 10000\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    decoder_type=\"lstm\",  # <------ เปลี่ยนตรงนี้ ถ้าอยากลอง lstm\n",
    ")\n",
    "\n",
    "# Dummy input\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10))\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "\n",
    "# batch=2, seq_len=10\n",
    "assert output.shape == (2, 10, vocab_size), f\"Shape mismatch: {output.shape}\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK. Decoder type =\", model.decoder_type)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#---- test Transformer decoder ----#\n",
    "vocab_size = 10000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    decoder_type=\"transformer\",   # <------ ใช้ transformer\n",
    ")\n",
    "\n",
    "# Dummy input (long!)\n",
    "dummy_img = torch.randn(2, 3, 224, 224).to(device)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10), dtype=torch.long).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "# batch=2, seq_len=10\n",
    "assert output.shape == (2, 10, vocab_size), f\"Shape mismatch: {output.shape}\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK. Decoder type =\", model.decoder_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx11Sw7Y4UOh"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2836,
     "status": "ok",
     "timestamp": 1749879500761,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "733Yyiv0pTHE"
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "# 1. สร้าง vocab จาก caption ทั้งหมด\n",
    "def build_vocab(jsonl_path, min_freq=0):\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            if obj.get('caption'):\n",
    "                tokens = word_tokenize(obj['caption'], keep_whitespace=False)\n",
    "                counter.update(tokens)\n",
    "    # สร้าง vocab: padding=0, unk=1, sos=2, eos=3, (แล้วตามด้วย word)\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "\n",
    "    for w, c in counter.items():\n",
    "\n",
    "        if c >= min_freq and w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, vocab, max_len=2000, transform=None):\n",
    "        self.records = []\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                # ใช้ 'caption' หรือ 'caption_thai' ก็ได้\n",
    "                if obj.get('caption') or obj.get('caption_thai'):\n",
    "                    self.records.append(obj)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        tokens = word_tokenize(caption, keep_whitespace=False)\n",
    "        ids = [self.vocab.get('<sos>', 2)]\n",
    "        for w in tokens:\n",
    "            ids.append(self.vocab.get(w, self.vocab['<unk>']))\n",
    "        ids.append(self.vocab.get('<eos>', 3))\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab['<pad>']] * (self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        img = Image.open(rec['image']).convert('RGB')\n",
    "        img_t = self.transform(img)\n",
    "        caption_ids = self.encode_caption(rec['caption'])\n",
    "        severity = torch.tensor(rec.get('severity', 0), dtype=torch.long)\n",
    "        bndbox = rec.get(\"bndbox\", None)\n",
    "        if bndbox is not None:\n",
    "            bndbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "            bndbox = torch.tensor(bndbox, dtype=torch.float)\n",
    "        else:\n",
    "            bndbox = torch.zeros(4, dtype=torch.float)\n",
    "        return img_t, caption_ids, severity, bndbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11pj8ebG4WfN"
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1621,
     "status": "ok",
     "timestamp": 1749879502388,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "S_zNYESxt4dp"
   },
   "outputs": [],
   "source": [
    "# 3. สร้าง vocab & dataset & dataloader\n",
    "jsonl_path = \"./train/train.bbox224-2.jsonl\"\n",
    "vocab = build_vocab(jsonl_path)\n",
    "\n",
    "dataset = JSONLDataset(jsonl_path, vocab, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=12,pin_memory=True)\n",
    "\n",
    "# load val data set\n",
    "val_jsonl_path = \"./train/val.bbox224-2.jsonl\"\n",
    "\n",
    "val_dataset = JSONLDataset(val_jsonl_path, vocab, max_len=100)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=12,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYUcVYYJz3SF"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JS-HnMhT--O"
   },
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1749879502390,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "piANEZwST9cV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# NLP metric import...\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# ---- Bounding box metric ----\n",
    "def box_iou(box1, box2):\n",
    "    # box1, box2 : array หรือ tensor shape [4] = (x1, y1, x2, y2)\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def bbox_metrics(all_pred_bbox, all_gt_bbox):\n",
    "    ious = []\n",
    "    ap_05s = []\n",
    "    N = all_pred_bbox.size(0)\n",
    "    for i in range(N):\n",
    "        pred = all_pred_bbox[i].cpu().numpy()\n",
    "        gt = all_gt_bbox[i].cpu().numpy()\n",
    "        iou = box_iou(pred, gt)     # ของคุณเอง\n",
    "        ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "        ious.append(iou)\n",
    "        ap_05s.append(ap_05)\n",
    "    return np.mean(ious), np.mean(ap_05s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "    tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w == '<eos>' and i < min_length: continue\n",
    "        if w == '<eos>': break\n",
    "        if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "    return clean\n",
    "def compute_bleu(preds, refs, inv_vocab_dict):\n",
    "    scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = decode_caption(pred, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(ref, inv_vocab_dict)\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "        scores.append(score)\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "def compute_rouge(preds, refs, inv_vocab_dict):\n",
    "    r1, r2, rL = [], [], []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = \" \".join(decode_caption(pred, inv_vocab_dict))\n",
    "        ref_tokens  = \" \".join(decode_caption(ref, inv_vocab_dict))\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        scores = rouge.score(ref_tokens, pred_tokens)\n",
    "        r1.append(scores['rouge1'].fmeasure)\n",
    "        r2.append(scores['rouge2'].fmeasure)\n",
    "        rL.append(scores['rougeL'].fmeasure)\n",
    "    return (np.mean(r1) if r1 else 0, np.mean(r2) if r2 else 0, np.mean(rL) if rL else 0)\n",
    "\n",
    "def compute_meteor(preds, refs, inv_vocab_dict):\n",
    "    meteor_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = decode_caption(pred, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(ref, inv_vocab_dict)\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        meteor_scores.append(meteor_score([ref_tokens], pred_tokens))\n",
    "    return np.mean(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "# ---- Main: Validate Auto-Regressive ----\n",
    "def validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50):\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            for i in range(batch_size):\n",
    "                pred_ids, _, _ =  model.generate (\n",
    "              images[i], max_len=50, device=device, top_k=1, temperature=1,\n",
    "          )\n",
    "                ref_ids = captions[i].cpu().tolist()\n",
    "                all_preds.append(pred_ids)\n",
    "                all_refs.append(ref_ids)\n",
    "    return all_preds, all_refs\n",
    "\n",
    "def get_tf_prob(epoch):\n",
    "    if epoch >= ss_epochs:\n",
    "        return end_prob\n",
    "    return start_prob - (start_prob - end_prob) * (epoch / ss_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOKYEBpdIh0f"
   },
   "source": [
    "## Train with NLP every 10 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 335884,
     "status": "ok",
     "timestamp": 1749882134334,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "Nugl5XO1olA8",
    "outputId": "964e3752-8e73-4425-c7f6-1aeae30754de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model path: /content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/bestmodel_20250614_061634.pth\n",
      "Metrics log path: /content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/metrics_log_20250614_061634.csv\n",
      "Device: cuda\n",
      "vocab size: 265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN weights from /content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FasterRCNN weights from /content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\n",
      "\n",
      "[Epoch 1] Teacher forcing prob: 1.00\n",
      "Epoch 1 | train_loss: 11.0200 | loss_caption: 1.185 | loss_severity: 0.910 | loss_bbox: 0.009\n",
      "Epoch 1 | train_loss: 11.0200 | val_loss: 19.6206 | SeverityAcc: 0.7889 | IOU: 0.3680 | AP@0.5: 0.2389\n",
      "Saved best model at epoch 1: /content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/bestmodel_20250614_061634.pth\n",
      "\n",
      "[Epoch 2] Teacher forcing prob: 0.90\n",
      "Epoch 2 | train_loss: 5.1741 | loss_caption: 1.193 | loss_severity: 0.055 | loss_bbox: 0.016\n",
      "Epoch 2 | train_loss: 5.1741 | val_loss: 22.6883 | SeverityAcc: 0.8389 | IOU: 0.4341 | AP@0.5: 0.3556\n",
      "No improvement for 1 epochs.\n",
      "\n",
      "[Epoch 3] Teacher forcing prob: 0.80\n",
      "Epoch 3 | train_loss: 4.5256 | loss_caption: 1.508 | loss_severity: 0.011 | loss_bbox: 0.011\n",
      "Epoch 3 | train_loss: 4.5256 | val_loss: 23.6766 | SeverityAcc: 0.8222 | IOU: 0.4639 | AP@0.5: 0.4278\n",
      "No improvement for 2 epochs.\n",
      "\n",
      "[Epoch 4] Teacher forcing prob: 0.70\n",
      "Epoch 4 | train_loss: 4.3542 | loss_caption: 1.128 | loss_severity: 0.065 | loss_bbox: 0.007\n",
      "Epoch 4 | train_loss: 4.3542 | val_loss: 24.6032 | SeverityAcc: 0.8389 | IOU: 0.4535 | AP@0.5: 0.4278\n",
      "No improvement for 3 epochs.\n",
      "\n",
      "[Epoch 5] Teacher forcing prob: 0.60\n",
      "Epoch 5 | train_loss: 4.3347 | loss_caption: 1.213 | loss_severity: 0.017 | loss_bbox: 0.011\n",
      "Epoch 5 | train_loss: 4.3347 | val_loss: 25.3021 | SeverityAcc: 0.8389 | IOU: 0.4877 | AP@0.5: 0.4889\n",
      "No improvement for 4 epochs.\n",
      "\n",
      "[Epoch 6] Teacher forcing prob: 0.50\n",
      "Epoch 6 | train_loss: 4.5142 | loss_caption: 1.231 | loss_severity: 1.556 | loss_bbox: 0.009\n",
      "Epoch 6 | train_loss: 4.5142 | val_loss: 24.9633 | SeverityAcc: 0.8500 | IOU: 0.4097 | AP@0.5: 0.3111\n",
      "No improvement for 5 epochs.\n",
      "\n",
      "[Epoch 7] Teacher forcing prob: 0.40\n",
      "Epoch 7 | train_loss: 4.5887 | loss_caption: 1.162 | loss_severity: 0.363 | loss_bbox: 0.007\n",
      "Epoch 7 | train_loss: 4.5887 | val_loss: 25.2318 | SeverityAcc: 0.8278 | IOU: 0.4328 | AP@0.5: 0.3500\n",
      "No improvement for 6 epochs.\n",
      "\n",
      "[Epoch 8] Teacher forcing prob: 0.30\n",
      "Epoch 8 | train_loss: 4.9612 | loss_caption: 1.183 | loss_severity: 0.003 | loss_bbox: 0.007\n",
      "Epoch 8 | train_loss: 4.9612 | val_loss: 25.3600 | SeverityAcc: 0.8389 | IOU: 0.4557 | AP@0.5: 0.4389\n",
      "No improvement for 7 epochs.\n",
      "\n",
      "[Epoch 9] Teacher forcing prob: 0.20\n",
      "Epoch 9 | train_loss: 5.7070 | loss_caption: 1.971 | loss_severity: 0.272 | loss_bbox: 0.007\n",
      "Epoch 9 | train_loss: 5.7070 | val_loss: 24.4963 | SeverityAcc: 0.8278 | IOU: 0.4439 | AP@0.5: 0.4000\n",
      "No improvement for 8 epochs.\n",
      "\n",
      "[Epoch 10] Teacher forcing prob: 0.10\n",
      "Epoch 10 | train_loss: 6.5941 | loss_caption: 3.163 | loss_severity: 1.496 | loss_bbox: 0.012\n",
      "Epoch 10 | train_loss: 6.5941 | val_loss: 24.1108 | SeverityAcc: 0.8222 | IOU: 0.4807 | AP@0.5: 0.4889\n",
      "No improvement for 9 epochs.\n",
      "\n",
      "=== VALIDATION (Auto-regressive Generate) ===\n",
      "BLEU:   0.4807\n",
      "ROUGE-1 0.5657 | ROUGE-2 0.0278 | ROUGE-L 0.5657\n",
      "METEOR: 0.5605\n",
      "\n",
      "[Epoch 11] Teacher forcing prob: 0.00\n",
      "Epoch 11 | train_loss: 9.8523 | loss_caption: 2.256 | loss_severity: 0.699 | loss_bbox: 0.009\n",
      "Epoch 11 | train_loss: 9.8523 | val_loss: 21.7966 | SeverityAcc: 0.8222 | IOU: 0.4681 | AP@0.5: 0.4667\n",
      "No improvement for 10 epochs.\n",
      "\n",
      "[Epoch 12] Teacher forcing prob: 0.00\n",
      "Epoch 12 | train_loss: 8.4848 | loss_caption: 2.216 | loss_severity: 0.027 | loss_bbox: 0.005\n",
      "Epoch 12 | train_loss: 8.4848 | val_loss: 20.6229 | SeverityAcc: 0.8333 | IOU: 0.4698 | AP@0.5: 0.4278\n",
      "No improvement for 11 epochs.\n",
      "\n",
      "[Epoch 13] Teacher forcing prob: 0.00\n",
      "Epoch 13 | train_loss: 7.8185 | loss_caption: 1.593 | loss_severity: 0.039 | loss_bbox: 0.006\n",
      "Epoch 13 | train_loss: 7.8185 | val_loss: 20.9816 | SeverityAcc: 0.8444 | IOU: 0.4529 | AP@0.5: 0.4278\n",
      "No improvement for 12 epochs.\n",
      "\n",
      "[Epoch 14] Teacher forcing prob: 0.00\n",
      "Epoch 14 | train_loss: 7.4628 | loss_caption: 3.473 | loss_severity: 0.554 | loss_bbox: 0.004\n",
      "Epoch 14 | train_loss: 7.4628 | val_loss: 21.8717 | SeverityAcc: 0.8167 | IOU: 0.4776 | AP@0.5: 0.4944\n",
      "No improvement for 13 epochs.\n",
      "\n",
      "[Epoch 15] Teacher forcing prob: 0.00\n",
      "Epoch 15 | train_loss: 6.9066 | loss_caption: 2.259 | loss_severity: 0.038 | loss_bbox: 0.007\n",
      "Epoch 15 | train_loss: 6.9066 | val_loss: 19.9485 | SeverityAcc: 0.8278 | IOU: 0.4699 | AP@0.5: 0.4500\n",
      "No improvement for 14 epochs.\n",
      "\n",
      "[Epoch 16] Teacher forcing prob: 0.00\n",
      "Epoch 16 | train_loss: 6.8246 | loss_caption: 2.185 | loss_severity: 0.193 | loss_bbox: 0.008\n",
      "Epoch 16 | train_loss: 6.8246 | val_loss: 21.2036 | SeverityAcc: 0.8056 | IOU: 0.4826 | AP@0.5: 0.5000\n",
      "No improvement for 15 epochs.\n",
      "\n",
      "[Epoch 17] Teacher forcing prob: 0.00\n",
      "Epoch 17 | train_loss: 6.1324 | loss_caption: 1.027 | loss_severity: 0.003 | loss_bbox: 0.009\n",
      "Epoch 17 | train_loss: 6.1324 | val_loss: 21.5621 | SeverityAcc: 0.8333 | IOU: 0.4738 | AP@0.5: 0.4611\n",
      "No improvement for 16 epochs.\n",
      "\n",
      "[Epoch 18] Teacher forcing prob: 0.00\n",
      "Epoch 18 | train_loss: 6.5446 | loss_caption: 1.496 | loss_severity: 0.002 | loss_bbox: 0.009\n",
      "Epoch 18 | train_loss: 6.5446 | val_loss: 21.7733 | SeverityAcc: 0.8500 | IOU: 0.4583 | AP@0.5: 0.4333\n",
      "No improvement for 17 epochs.\n",
      "\n",
      "[Epoch 19] Teacher forcing prob: 0.00\n",
      "Epoch 19 | train_loss: 6.0337 | loss_caption: 2.462 | loss_severity: 0.015 | loss_bbox: 0.007\n",
      "Epoch 19 | train_loss: 6.0337 | val_loss: 22.1005 | SeverityAcc: 0.8444 | IOU: 0.4474 | AP@0.5: 0.4056\n",
      "No improvement for 18 epochs.\n",
      "\n",
      "[Epoch 20] Teacher forcing prob: 0.00\n",
      "Epoch 20 | train_loss: 5.6933 | loss_caption: 1.318 | loss_severity: 0.154 | loss_bbox: 0.008\n",
      "Epoch 20 | train_loss: 5.6933 | val_loss: 21.0235 | SeverityAcc: 0.8444 | IOU: 0.4734 | AP@0.5: 0.4556\n",
      "No improvement for 19 epochs.\n",
      "\n",
      "=== VALIDATION (Auto-regressive Generate) ===\n",
      "BLEU:   0.4883\n",
      "ROUGE-1 0.6287 | ROUGE-2 0.0222 | ROUGE-L 0.6287\n",
      "METEOR: 0.5494\n",
      "\n",
      "[Epoch 21] Teacher forcing prob: 0.00\n",
      "Epoch 21 | train_loss: 5.6104 | loss_caption: 1.977 | loss_severity: 0.997 | loss_bbox: 0.009\n",
      "Epoch 21 | train_loss: 5.6104 | val_loss: 21.8775 | SeverityAcc: 0.8278 | IOU: 0.4650 | AP@0.5: 0.4611\n",
      "No improvement for 20 epochs.\n",
      "\n",
      "[Epoch 22] Teacher forcing prob: 0.00\n",
      "Epoch 22 | train_loss: 5.4700 | loss_caption: 1.477 | loss_severity: 0.003 | loss_bbox: 0.006\n",
      "Epoch 22 | train_loss: 5.4700 | val_loss: 22.9953 | SeverityAcc: 0.8556 | IOU: 0.4865 | AP@0.5: 0.5333\n",
      "No improvement for 21 epochs.\n",
      "\n",
      "[Epoch 23] Teacher forcing prob: 0.00\n",
      "Epoch 23 | train_loss: 5.3716 | loss_caption: 1.913 | loss_severity: 0.009 | loss_bbox: 0.009\n",
      "Epoch 23 | train_loss: 5.3716 | val_loss: 22.1900 | SeverityAcc: 0.8389 | IOU: 0.4736 | AP@0.5: 0.4667\n",
      "No improvement for 22 epochs.\n",
      "\n",
      "[Epoch 24] Teacher forcing prob: 0.00\n",
      "Epoch 24 | train_loss: 5.1929 | loss_caption: 1.705 | loss_severity: 0.000 | loss_bbox: 0.006\n",
      "Epoch 24 | train_loss: 5.1929 | val_loss: 23.2924 | SeverityAcc: 0.8667 | IOU: 0.4299 | AP@0.5: 0.3611\n",
      "No improvement for 23 epochs.\n",
      "\n",
      "[Epoch 25] Teacher forcing prob: 0.00\n",
      "Epoch 25 | train_loss: 5.2984 | loss_caption: 2.921 | loss_severity: 0.001 | loss_bbox: 0.009\n",
      "Epoch 25 | train_loss: 5.2984 | val_loss: 20.7469 | SeverityAcc: 0.8611 | IOU: 0.4670 | AP@0.5: 0.4389\n",
      "No improvement for 24 epochs.\n",
      "\n",
      "[Epoch 26] Teacher forcing prob: 0.00\n",
      "Epoch 26 | train_loss: 4.9543 | loss_caption: 1.754 | loss_severity: 0.000 | loss_bbox: 0.011\n",
      "Epoch 26 | train_loss: 4.9543 | val_loss: 23.6807 | SeverityAcc: 0.8500 | IOU: 0.4792 | AP@0.5: 0.4722\n",
      "No improvement for 25 epochs.\n",
      "\n",
      "[Epoch 27] Teacher forcing prob: 0.00\n",
      "Epoch 27 | train_loss: 4.9087 | loss_caption: 1.048 | loss_severity: 0.000 | loss_bbox: 0.010\n",
      "Epoch 27 | train_loss: 4.9087 | val_loss: 22.5497 | SeverityAcc: 0.8722 | IOU: 0.4742 | AP@0.5: 0.4778\n",
      "No improvement for 26 epochs.\n",
      "\n",
      "[Epoch 28] Teacher forcing prob: 0.00\n",
      "Epoch 28 | train_loss: 4.8641 | loss_caption: 1.352 | loss_severity: 0.001 | loss_bbox: 0.008\n",
      "Epoch 28 | train_loss: 4.8641 | val_loss: 23.4499 | SeverityAcc: 0.8611 | IOU: 0.4227 | AP@0.5: 0.3444\n",
      "No improvement for 27 epochs.\n",
      "\n",
      "[Epoch 29] Teacher forcing prob: 0.00\n",
      "Epoch 29 | train_loss: 4.7060 | loss_caption: 1.096 | loss_severity: 0.000 | loss_bbox: 0.006\n",
      "Epoch 29 | train_loss: 4.7060 | val_loss: 23.9048 | SeverityAcc: 0.8444 | IOU: 0.4802 | AP@0.5: 0.5000\n",
      "No improvement for 28 epochs.\n",
      "\n",
      "[Epoch 30] Teacher forcing prob: 0.00\n",
      "Epoch 30 | train_loss: 4.7137 | loss_caption: 0.926 | loss_severity: 0.000 | loss_bbox: 0.005\n",
      "Epoch 30 | train_loss: 4.7137 | val_loss: 21.8858 | SeverityAcc: 0.8778 | IOU: 0.4668 | AP@0.5: 0.4389\n",
      "No improvement for 29 epochs.\n",
      "\n",
      "=== VALIDATION (Auto-regressive Generate) ===\n",
      "BLEU:   0.4574\n",
      "ROUGE-1 0.5537 | ROUGE-2 0.0389 | ROUGE-L 0.5537\n",
      "METEOR: 0.5216\n",
      "\n",
      "[Epoch 31] Teacher forcing prob: 0.00\n",
      "Epoch 31 | train_loss: 4.8786 | loss_caption: 3.855 | loss_severity: 0.008 | loss_bbox: 0.006\n",
      "Epoch 31 | train_loss: 4.8786 | val_loss: 25.5392 | SeverityAcc: 0.8389 | IOU: 0.4563 | AP@0.5: 0.4389\n",
      "No improvement for 30 epochs.\n",
      "Early stopping at epoch 31. Best val_loss=19.6206\n",
      "Training complete!\n",
      "\n",
      "=== FINAL NLP METRIC ON VAL SET ===\n",
      "BLEU:   0.8336\n",
      "ROUGE-1 0.7370 | ROUGE-2 0.0833 | ROUGE-L 0.7370\n",
      "METEOR: 0.9050\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random, numpy as np, csv, os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---- FIX RANDOM SEED ----\n",
    "seed = 314\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ---- CONFIG ----\n",
    "batch_size = 256         # ปรับลดเพื่อให้ train/stable ขึ้น\n",
    "num_workers = 12\n",
    "num_epochs = 300\n",
    "patience = 30\n",
    "pad_token = vocab['<pad>']\n",
    "dropout = 0.5\n",
    "encoder_lr = 0.0000241722049458957\n",
    "decoder_lr =  0.00176746711386226\n",
    "\n",
    "# ---- HYPERPARAM LOSS WEIGHT ----\n",
    "alpha = 3.84663031888853   # caption\n",
    "beta  = 1.36764237545158  # severity\n",
    "gamma = 0.625734096283357 # bbox\n",
    "\n",
    "# scheduled sampling\n",
    "start_prob = 1.0\n",
    "end_prob = 0.0\n",
    "ss_epochs = 10\n",
    "#--- decoder -----#\n",
    "decoder_type=\"lstm\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------  PARAMETER  -------#\n",
    "\n",
    "# label smoothing\n",
    "criterion_caption = nn.CrossEntropyLoss(ignore_index=pad_token, label_smoothing=0.1)\n",
    "criterion_severity = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/bestmodel_{now}.pth\"\n",
    "csv_path  = f\"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/metrics_log_{now}.csv\"\n",
    "param_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/params_{now}.txt\"\n",
    "\n",
    "print(f\"Save model path: {save_path}\")\n",
    "print(f\"Metrics log path: {csv_path}\")\n",
    "\n",
    "with open(param_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"dropout: {dropout}\\n\")\n",
    "    f.write(f\"encoder_lr: {encoder_lr}\\n\")\n",
    "    f.write(f\"decoder_lr: {decoder_lr}\\n\")\n",
    "    f.write(f\"patience: {patience}\\n\")\n",
    "    f.write(f\"batch_size: {batch_size}\\n\")\n",
    "    f.write(f\"num_workers: {num_workers}\\n\")\n",
    "    f.write(f\"seed: {seed}\\n\")\n",
    "    f.write(f\"timestamp: {now}\\n\")\n",
    "\n",
    "inv_vocab_dict = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "\n",
    "# ---- SETUP DEVICE ----\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "except ImportError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "\n",
    "model = DKICNet(\n",
    "    vocab_size=len(vocab),\n",
    "    severity_classes=4,\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    cnn_pretrained=False,\n",
    "    num_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    decoder_type=decoder_type\n",
    ").to(device)\n",
    "\n",
    "encoder_params = list(model.cnn_encoder.parameters())\n",
    "frcnn_params  = list(model.frcnn_encoder.parameters())\n",
    "other_params  = [p for n, p in model.named_parameters()\n",
    "                 if not (n.startswith('cnn_encoder') or n.startswith('frcnn_encoder')) and p.requires_grad]\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.cnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.frcnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.fusion.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.attn.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.decoder.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.severity_head.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.bbox_head.parameters(), 'lr': decoder_lr},\n",
    "])\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "acc_scores, iou_scores, ap05_scores = [], [], []\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'severity_acc', 'iou', 'ap@0.5'])\n",
    "\n",
    "# ---- TRAINING LOOP ----\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    tf_prob = get_tf_prob(epoch)\n",
    "    print(f\"\\n[Epoch {epoch+1}] Teacher forcing prob: {tf_prob:.2f}\")\n",
    "    all_sev_preds, all_sev_true = [], []\n",
    "    all_pred_bbox, all_gt_bbox = [], []\n",
    "    for images, captions, severity, bbox_targets, *_ in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        severity = severity.to(device)\n",
    "        bbox_targets = bbox_targets.to(device)\n",
    "        _, _, H, W = images.shape\n",
    "        bbox_targets_norm = bbox_targets.clone()\n",
    "        bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "        bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size, seq_len = captions.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, model.decoder.fc.out_features, device=device)\n",
    "        input_token = captions[:, 0]  # <sos>\n",
    "\n",
    "        # --- feature extraction (ไม่มี ViT) ---\n",
    "        Fc = model.cnn_encoder(images)           # (B, 2048, H, W)\n",
    "        Ff = model.frcnn_encoder(images)         # (B, 256,  H, W)\n",
    "        Ffinal = model.fusion(Fc, Ff)            # (B, 2*out_c, H, W)\n",
    "        Fc_adj = model.fusion.adj_conv_cnn(Fc)   # (B, out_c, H, W)\n",
    "        Ff_adj = model.fusion.adj_conv_frcnn(Ff) # (B, out_c, H, W)\n",
    "\n",
    "        # --- attention ---\n",
    "        zt, _ = model.attn(\n",
    "            Ffinal,\n",
    "            Fc_adj,\n",
    "            Ff_adj,\n",
    "            torch.zeros(batch_size, model.attn.W_h.in_features, device=device)\n",
    "        )\n",
    "\n",
    "        # ==== แยกกรณี decoder ====\n",
    "        if hasattr(model.decoder, \"gru\"):  # GRU\n",
    "            h_t = torch.zeros(batch_size, model.decoder.gru.hidden_size, device=device)\n",
    "            for t in range(1, seq_len):\n",
    "                xt = model.decoder.word_embedding(input_token)\n",
    "                gru_input = torch.cat([xt, zt], dim=1)\n",
    "                h_t = model.decoder.gru(gru_input, h_t)\n",
    "                logits = model.decoder.fc(h_t)\n",
    "                outputs[:, t] = logits\n",
    "                use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                next_token_tf = captions[:, t]\n",
    "                next_token_model = logits.argmax(-1)\n",
    "                input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "        elif hasattr(model.decoder, \"lstm\"):  # LSTM\n",
    "            h_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "            c_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "            for t in range(1, seq_len):\n",
    "                xt = model.decoder.word_embedding(input_token)\n",
    "                lstm_input = torch.cat([xt, zt], dim=1)\n",
    "                h_t, c_t = model.decoder.lstm(lstm_input, (h_t, c_t))\n",
    "                logits = model.decoder.fc(h_t)\n",
    "                outputs[:, t] = logits\n",
    "\n",
    "                # scheduled sampling / teacher forcing\n",
    "                use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                next_token_tf = captions[:, t]\n",
    "                next_token_model = logits.argmax(-1)\n",
    "                input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "\n",
    "        elif  hasattr(model.decoder, \"transformer_decoder\"):  # Transformer\n",
    "              tgt = captions[:, :-1]\n",
    "              seq_len = tgt.size(1)\n",
    "              tgt_embed = model.decoder.word_embedding(tgt)  # [B, seq_len-1, embed_dim]\n",
    "              tgt_embed = tgt_embed.transpose(0, 1)          # [seq_len-1, B, embed_dim]\n",
    "\n",
    "              # ให้ memory repeat สำหรับทุกตำแหน่งใน seq\n",
    "              memory = zt.unsqueeze(0).repeat(seq_len, 1, 1)  # [seq_len-1, B, embed_dim]\n",
    "\n",
    "              out = model.decoder.transformer_decoder(tgt_embed, memory)  # [seq_len-1, B, embed_dim]\n",
    "              out = out.transpose(0, 1)  # [B, seq_len-1, embed_dim]\n",
    "              logits = model.decoder.fc(out)  # [B, seq_len-1, vocab_size]\n",
    "              outputs[:, 1:] = logits\n",
    "        else:\n",
    "            raise NotImplementedError(\"รองรับแต่ GRU, LSTM, TransformerDecoderCorrAttn\")\n",
    "\n",
    "        # Severity/Bbox output\n",
    "        severity_logits = model.severity_head(Ffinal)\n",
    "        bbox_pred = model.bbox_head(Ffinal)\n",
    "        # Target reshape\n",
    "        outputs_ = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
    "        targets_ = captions[:, 1:].reshape(-1)\n",
    "\n",
    "        loss_caption = criterion_caption(outputs_, targets_)\n",
    "        loss_severity = criterion_severity(severity_logits, severity)\n",
    "        loss_bbox = criterion_bbox(bbox_pred, bbox_targets_norm)\n",
    "        loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # เก็บ metric\n",
    "        all_pred_bbox.append(bbox_pred.detach().cpu())\n",
    "        all_gt_bbox.append(bbox_targets_norm.detach().cpu())\n",
    "        all_sev_preds.extend(severity_logits.argmax(-1).cpu().tolist())\n",
    "        all_sev_true.extend(severity.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | train_loss: {avg_loss:.4f} | loss_caption: {loss_caption.item():.3f} | loss_severity: {loss_severity.item():.3f} | loss_bbox: {loss_bbox.item():.3f}\")\n",
    "    train_losses.append(avg_loss)\n",
    "    # Eval acc, IOU/AP@0.5\n",
    "    all_pred_bbox = torch.cat(all_pred_bbox, dim=0)\n",
    "    all_gt_bbox = torch.cat(all_gt_bbox, dim=0)\n",
    "    mean_iou, ap_05 = bbox_metrics(all_pred_bbox, all_gt_bbox)\n",
    "    acc = (np.array(all_sev_preds) == np.array(all_sev_true)).mean()\n",
    "    acc_scores.append(acc)\n",
    "    iou_scores.append(mean_iou)\n",
    "    ap05_scores.append(ap_05)\n",
    "    # VALIDATION LOSS ONLY\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        all_val_sev_preds, all_val_sev_true = [], []\n",
    "        all_val_pred_bbox, all_val_gt_bbox = [], []\n",
    "        for images, captions, severity, bbox_targets, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            severity = severity.to(device)\n",
    "            bbox_targets = bbox_targets.to(device)\n",
    "            _, _, H, W = images.shape\n",
    "            bbox_targets_norm = bbox_targets.clone()\n",
    "            bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "            bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "            outputs, _, severity_logits, bbox_preds = model(images, captions)\n",
    "            outputs_ = outputs.view(-1, outputs.size(-1))\n",
    "            targets_ = captions.view(-1)\n",
    "            loss_caption = criterion_caption(outputs_, targets_)\n",
    "            loss_severity = criterion_severity(severity_logits, severity)\n",
    "            loss_bbox = criterion_bbox(bbox_preds, bbox_targets_norm)\n",
    "            loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "            val_loss += loss.item()\n",
    "            all_val_sev_preds.extend(severity_logits.argmax(-1).cpu().tolist())\n",
    "            all_val_sev_true.extend(severity.cpu().tolist())\n",
    "            all_val_pred_bbox.append(bbox_preds.detach().cpu())\n",
    "            all_val_gt_bbox.append(bbox_targets_norm.detach().cpu())\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_iou, val_ap05 = bbox_metrics(torch.cat(all_val_pred_bbox,0), torch.cat(all_val_gt_bbox,0))\n",
    "        val_acc = (np.array(all_val_sev_preds) == np.array(all_val_sev_true)).mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss: {avg_loss:.4f} | val_loss: {avg_val_loss:.4f} | SeverityAcc: {val_acc:.4f} | IOU: {val_iou:.4f} | AP@0.5: {val_ap05:.4f}\")\n",
    "\n",
    "    # ---- SAVE MODEL + EARLY STOPPING ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1}: {save_path}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f'No improvement for {epochs_no_improve} epochs.')\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val_loss={best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # ---- LOG METRICS TO CSV ----\n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss, avg_val_loss, val_acc, val_iou, ap_05])\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or (epoch+1) == num_epochs:\n",
    "        print(\"\\n=== VALIDATION (Auto-regressive Generate) ===\")\n",
    "        all_preds, all_refs = validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50)\n",
    "        bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "        rouge1, rouge2, rougeL = compute_rouge(all_preds, all_refs, inv_vocab_dict)\n",
    "        meteor = compute_meteor(all_preds, all_refs, inv_vocab_dict)\n",
    "        print(f\"BLEU:   {bleu:.4f}\")\n",
    "        print(f\"ROUGE-1 {rouge1:.4f} | ROUGE-2 {rouge2:.4f} | ROUGE-L {rougeL:.4f}\")\n",
    "        print(f\"METEOR: {meteor:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"\\n=== FINAL NLP METRIC ON VAL SET ===\")\n",
    "model.eval()\n",
    "all_preds, all_refs = [], []\n",
    "with torch.no_grad():\n",
    "    for images, captions, *_ in val_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        outputs, _, _, _ = model(images, captions)\n",
    "        preds = outputs.argmax(-1).cpu().tolist()\n",
    "        refs = captions.cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_refs.extend(refs)\n",
    "bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "rouge1, rouge2, rougeL = compute_rouge(all_preds, all_refs, inv_vocab_dict)\n",
    "meteor = compute_meteor(all_preds, all_refs, inv_vocab_dict)\n",
    "print(f\"BLEU:   {bleu:.4f}\")\n",
    "print(f\"ROUGE-1 {rouge1:.4f} | ROUGE-2 {rouge2:.4f} | ROUGE-L {rougeL:.4f}\")\n",
    "print(f\"METEOR: {meteor:.4f}\")\n",
    "\n",
    "# ---- PLOT TRAINING LOSS----\n",
    "metrics = pd.read_csv(csv_path)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "plt.plot(acc_scores, label='Severity Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n",
    "plt.plot(iou_scores, label='Mean IOU')\n",
    "plt.plot(ap05_scores, label='AP@0.5')\n",
    "plt.xlabel('Epoch'); plt.ylabel('BBox Metric'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFF9ODrMZKsd"
   },
   "source": [
    " ## save latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1749874156234,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "ZmxiU9go_eg4"
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/bestmodel_{now}-latest.pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pseLYyqrVY_T"
   },
   "source": [
    "## optuna parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "s_ubyPT_VDcj"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "# ---- SETUP DEVICE ----\n",
    "\n",
    "\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "except ImportError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "def validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50):\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            for b in range(batch_size):\n",
    "                img = images[b]\n",
    "                ref = captions[b].cpu().tolist()\n",
    "                # ---- auto-regressive decode ----\n",
    "                pred_ids, _, _ = model.generate(img, max_len=max_len, device=device)\n",
    "                all_preds.append(pred_ids)\n",
    "                all_refs.append(ref)\n",
    "    return all_preds, all_refs\n",
    "\n",
    "def objective(trial):\n",
    "    # ---- Hyperparameter ----\n",
    "    encoder_lr = trial.suggest_loguniform(\"encoder_lr\", 1e-5, 5e-4)\n",
    "    decoder_lr = trial.suggest_loguniform(\"decoder_lr\", 5e-5, 2e-3)\n",
    "    alpha = trial.suggest_float(\"alpha\", 1.0, 6.0)   # caption loss weight\n",
    "    beta  = trial.suggest_float(\"beta\", 0.5, 2.0)    # severity loss weight\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.5, 2.0)   # bbox loss weight\n",
    "\n",
    "    # ===== Re-init Model & Optimizer =====\n",
    "    model = DKICNet(\n",
    "        vocab_size=len(vocab),\n",
    "        severity_classes=4,\n",
    "        cnn_backbone=\"resnet50\",\n",
    "        cnn_pretrained=False,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=4,\n",
    "        cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "        frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "        decoder_type=\"gru\"\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.cnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.frcnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.fusion.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.attn.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.decoder.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.severity_head.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.bbox_head.parameters(), 'lr': decoder_lr},\n",
    "    ])\n",
    "    # Loss\n",
    "    criterion_caption = nn.CrossEntropyLoss(ignore_index=pad_token, label_smoothing=0.1)\n",
    "    criterion_severity = nn.CrossEntropyLoss()\n",
    "    criterion_bbox = nn.SmoothL1Loss()\n",
    "\n",
    "    best_bleu = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # ==== Training Loop (ย่อ) ====\n",
    "    for epoch in range(8):   # สั้นๆ, 5-10 epoch ก็พอสำหรับ tuning\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "\n",
    "        for images, captions, severity, bbox_targets, *_ in train_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            severity = severity.to(device)\n",
    "            bbox_targets = bbox_targets.to(device)\n",
    "            _, _, H, W = images.shape\n",
    "            bbox_targets_norm = bbox_targets.clone()\n",
    "            bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "            bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size, seq_len = captions.size()\n",
    "            outputs = torch.zeros(batch_size, seq_len, model.decoder.fc.out_features, device=device)\n",
    "            input_token = captions[:, 0]\n",
    "\n",
    "            # ==== เพิ่ม Vit ====\n",
    "            Fc = model.cnn_encoder(images)\n",
    "            Ff = model.frcnn_encoder(images)\n",
    "\n",
    "            Ffinal = model.fusion(Fc, Ff)\n",
    "            Fc_adj = model.fusion.adj_conv_cnn(Fc)\n",
    "            Ff_adj = model.fusion.adj_conv_frcnn(Ff)\n",
    "\n",
    "\n",
    "            zt, _ = model.attn(Ffinal, Fc_adj, Ff_adj, torch.zeros(batch_size, model.attn.W_h.in_features, device=device))\n",
    "            # =====\n",
    "\n",
    "            if hasattr(model.decoder, \"gru\"):  # GRU\n",
    "                h_t = torch.zeros(batch_size, model.decoder.gru.hidden_size, device=device)\n",
    "                for t in range(1, seq_len):\n",
    "                    xt = model.decoder.word_embedding(input_token)\n",
    "                    gru_input = torch.cat([xt, zt], dim=1)\n",
    "                    h_t = model.decoder.gru(gru_input, h_t)\n",
    "                    logits = model.decoder.fc(h_t)\n",
    "                    outputs[:, t] = logits\n",
    "                    use_tf = (torch.rand(batch_size, device=device) < 1.0)\n",
    "                    next_token_tf = captions[:, t]\n",
    "                    next_token_model = logits.argmax(-1)\n",
    "                    input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "            elif hasattr(model.decoder, \"lstm\"):  # LSTM\n",
    "                h_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "                c_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "                for t in range(1, seq_len):\n",
    "                    xt = model.decoder.word_embedding(input_token)\n",
    "                    lstm_input = torch.cat([xt, zt], dim=1)\n",
    "                    h_t, c_t = model.decoder.lstm(lstm_input, (h_t, c_t))\n",
    "                    logits = model.decoder.fc(h_t)\n",
    "                    outputs[:, t] = logits\n",
    "\n",
    "                    # scheduled sampling / teacher forcing\n",
    "                    use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                    next_token_tf = captions[:, t]\n",
    "                    next_token_model = logits.argmax(-1)\n",
    "                    input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "\n",
    "            else:  # Transformer\n",
    "                tgt = captions[:, :-1]\n",
    "                seq_len = tgt.size(1)\n",
    "                tgt_embed = model.decoder.word_embedding(tgt).transpose(0, 1)\n",
    "                memory = zt.unsqueeze(0).repeat(seq_len, 1, 1)\n",
    "                out = model.decoder.transformer_decoder(tgt_embed, memory)\n",
    "                out = out.transpose(0, 1)\n",
    "                logits = model.decoder.fc(out)\n",
    "                outputs[:, 1:] = logits\n",
    "\n",
    "            severity_logits = model.severity_head(Ffinal)\n",
    "            bbox_pred = model.bbox_head(Ffinal)\n",
    "            outputs_ = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
    "            targets_ = captions[:, 1:].reshape(-1)\n",
    "\n",
    "            loss_caption = criterion_caption(outputs_, targets_)\n",
    "            loss_severity = criterion_severity(severity_logits, severity)\n",
    "            loss_bbox = criterion_bbox(bbox_pred, bbox_targets_norm)\n",
    "            loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        # ---- Validation NLP Metric ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds, all_refs = [], []\n",
    "            for images, captions, *_ in val_loader:\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                for b in range(batch_size):\n",
    "                      img = images[b]\n",
    "                      ref = captions[b].cpu().tolist()\n",
    "                      # === ใช้ generate ทีละ sample ===\n",
    "                      pred_ids, _, _ = model.generate(img, max_len=50, device=device)\n",
    "                      all_preds.append(pred_ids)\n",
    "                      all_refs.append(ref)\n",
    "\n",
    "            bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "            if bleu > best_bleu:\n",
    "                best_bleu = bleu\n",
    "                best_epoch = epoch\n",
    "\n",
    "\n",
    "    print(f\"Trial {trial.number} | best BLEU: {best_bleu:.4f} | alpha: {alpha:.2f} beta: {beta:.2f} gamma: {gamma:.2f} encoder_lr: {encoder_lr:.6f} decoder_lr: {decoder_lr:.6f}\")\n",
    "    return best_bleu\n",
    "\n",
    "# ---- Optuna Study ----\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)   # 30-50 รอบกำลังดี\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"BLEU: {trial.value:.4f}\")\n",
    "for k, v in trial.params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# ---- นำค่าที่ได้ไปใช้เทรนจริง ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaASnX75UNlE"
   },
   "source": [
    "## clear GPU memoryssssssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cS7QAWgQHNGO"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # 1. ลบตัวแปร model/optimizer/dataloader ที่ไม่ใช้แล้ว (optional)\n",
    "# del model\n",
    "# del optimizer\n",
    "# # del train_loader, val_loader, test_loader  # ถ้าจะลบออกหมด\n",
    "\n",
    "# # 2. เคลียร์ Python garbage\n",
    "# gc.collect()\n",
    "\n",
    "# # 3. เคลียร์ CUDA memory\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()  # สำหรับบางกรณีที่ memory fragment\n",
    "\n",
    "# print(\"CUDA memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1VTovM8FiRv"
   },
   "source": [
    "# Test auto regressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fiPuUKYL4P5"
   },
   "source": [
    "## test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1749879946638,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "6wIIdqIwwOwG"
   },
   "outputs": [],
   "source": [
    "# test_jsonl_path = '/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/test.filtered.jsonl'\n",
    "test_jsonl_path= '/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/test.bbox224-2.jsonl'\n",
    "test_dataset = JSONLDataset(test_jsonl_path, vocab, max_len=70)\n",
    "caption_ids = test_dataset[0][1]\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=70, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W_Eo-3GY5J2"
   },
   "source": [
    "### find latest best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1749876235466,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "O4xUsDbKWSVd",
    "outputId": "5e862280-91f2-44e0-c7f0-b409b8053b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Final_Deep_project/experiment-log/bestmodel_20250614_033332.pth\n"
     ]
    }
   ],
   "source": [
    "ls -t /content/drive/MyDrive/Final_Deep_project/experiment-log/*.pth | head -n 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM8-NnlkFnvh"
   },
   "source": [
    "## Test Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1749879949485,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "a9tPBD9iE_w8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "def get_last_conv_layer(encoder):\n",
    "    # สำหรับ CNNEncoder ที่ใช้ self.layer4 เป็น conv block สุดท้าย\n",
    "    return encoder.layer4\n",
    "\n",
    "def generate_gradcam(encoder, input_tensor, class_idx=None):\n",
    "    # encoder: CNNEncoder (เช่น model.cnn_encoder)\n",
    "    encoder.eval()\n",
    "    feature_maps = []\n",
    "    gradients = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        feature_maps.append(output)\n",
    "\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0])\n",
    "\n",
    "    last_conv = get_last_conv_layer(encoder)\n",
    "    f = last_conv.register_forward_hook(forward_hook)\n",
    "    b = last_conv.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = encoder(input_tensor)\n",
    "    if class_idx is None:\n",
    "        score = output.sum()\n",
    "    else:\n",
    "        score = output[:, class_idx].sum()\n",
    "    encoder.zero_grad()\n",
    "    score.backward(retain_graph=True)\n",
    "\n",
    "    grads_val = gradients[0].detach().cpu().numpy()[0]\n",
    "    fmap = feature_maps[0].detach().cpu().numpy()[0]\n",
    "    weights = np.mean(grads_val, axis=(1, 2))\n",
    "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * fmap[i, :, :]\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "    return cam\n",
    "\n",
    "def visualize_result(image_path, pred_caption, gt_caption, sev_pred, sev_gt, cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.5):\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    # ======= วาดกรอบ bounding box =======\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):\n",
    "            if box_scores is not None and box_scores[i] < box_threshold:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_str = \"\"\n",
    "            if box_labels is not None:\n",
    "                label_str += f\"{box_labels[i]}\"\n",
    "            if box_scores is not None:\n",
    "                label_str += f\" ({box_scores[i]:.2f})\"\n",
    "            if label_str:\n",
    "                ax.text(x1, y1, label_str, color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    # ======= Grad-CAM =======\n",
    "    if cam is not None:\n",
    "        plt.subplot(1,2,2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam*255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_last_selfattention(vit_model, img_tensor):\n",
    "    # Forward pass เพื่อให้มี attn map\n",
    "    outputs = vit_model.vit.forward_features(img_tensor)\n",
    "    # ไปหยิบ attn จาก block สุดท้าย\n",
    "    attn = vit_model.vit.blocks[-1].attn.get_attn()\n",
    "    return attn  # shape: [B, num_heads, num_tokens, num_tokens]\n",
    "\n",
    "def show_vit_attention_map(img, vit_model, img_tensor, patch_size=16, head=0):\n",
    "    \"\"\"\n",
    "    img: original PIL image\n",
    "    vit_model: ViTEncoder (ที่ใช้ timm)\n",
    "    img_tensor: [1,3,H,W] preprocessed for vit\n",
    "    patch_size: (default 16)\n",
    "    head: index of attention head (default 0)\n",
    "    \"\"\"\n",
    "    # 1. get attention weights [B, num_heads, num_tokens, num_tokens]\n",
    "    attn = get_last_selfattention(vit_model, img_tensor)[0]  # [num_heads, num_tokens, num_tokens]\n",
    "    nh = attn.shape[0]\n",
    "    # เราใช้เฉพาะ cls token (index 0) => attn[:, 0, 1:] (skip cls)\n",
    "    attn_map = attn[head, 0, 1:].reshape(int(img_tensor.shape[-1] / patch_size), -1)  # [n_patches, n_patches]\n",
    "    attn_map = attn_map.detach().cpu().numpy()\n",
    "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-6)\n",
    "\n",
    "    # Resize map to image size\n",
    "    attn_map = np.kron(attn_map, np.ones((patch_size, patch_size)))  # expand\n",
    "    attn_map = np.clip(attn_map, 0, 1)\n",
    "    attn_map = Image.fromarray(np.uint8(attn_map*255)).resize(img.size, resample=Image.BILINEAR)\n",
    "    attn_map = np.asarray(attn_map) / 255\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(attn_map, cmap='jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'ViT Attention Head {head}')\n",
    "    plt.show()\n",
    "    return attn_map\n",
    "\n",
    "def plot_vit_attention(model, img_tensor, image_display=None, patch_size=16):\n",
    "    vit = model.vit_encoder.vit  # timm model\n",
    "    vit.eval()\n",
    "    attns = []\n",
    "\n",
    "    # --- timm ViT ไม่มี default output attn, ต้อง hook ที่ attn module ---\n",
    "    def hook(module, input, output):\n",
    "        # output = (x) = (batch, num_patches+1, dim)\n",
    "        # module = Attention\n",
    "        attn_weights = module.get_attn()\n",
    "        attns.append(attn_weights.detach().cpu())\n",
    "    # -- ค้นหา attn module บล็อคสุดท้าย --\n",
    "    attn_mod = vit.blocks[-1].attn\n",
    "    # -- เพิ่ม get_attn() ให้ attn_mod --\n",
    "    def get_attn(self):\n",
    "        return self.attn_weights\n",
    "    import types\n",
    "    attn_mod.get_attn = types.MethodType(get_attn, attn_mod)\n",
    "    # -- hook ก่อน forward --\n",
    "    handle = attn_mod.register_forward_hook(hook)\n",
    "\n",
    "    # -- patch: บันทึก attn_weights ใน self.attn_weights ก่อน forward --\n",
    "    def forward_with_save_attn(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        self.attn_weights = attn  # <-- save\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    attn_mod.forward = types.MethodType(forward_with_save_attn, attn_mod)\n",
    "\n",
    "    _ = vit(img_tensor.to(next(vit.parameters()).device))\n",
    "    handle.remove()\n",
    "    attn_map = attns[0]  # (B, heads, tokens, tokens)\n",
    "    attn_map = attn_map.mean(1).squeeze(0)  # (tokens, tokens)\n",
    "    cls_attn = attn_map[0, 1:]              # (num_patches,)\n",
    "    num_patches = int(np.sqrt(cls_attn.shape[0]))\n",
    "    attn_2d = cls_attn.reshape(num_patches, num_patches).numpy()\n",
    "    attn_2d = attn_2d / attn_2d.max()\n",
    "\n",
    "    # --- plot ---\n",
    "    if image_display is None:\n",
    "        img = tensor_to_pil(img_tensor)\n",
    "    elif isinstance(image_display, str):\n",
    "        img = Image.open(image_display).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image_display)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(img)\n",
    "    attn_2d_resized = np.array(Image.fromarray(np.uint8(attn_2d*255)).resize(img.size, Image.BILINEAR)) / 255.\n",
    "    plt.imshow(attn_2d_resized, alpha=0.4, cmap='jet')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"ViT Attention Map\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_bbox_gt_pred(img, gt_box, pred_box, pred_score=None, threshold=0.16):\n",
    "    # --- Convert ภาพ & แสดงขนาดภาพ ---\n",
    "    if isinstance(img, str):\n",
    "        img = Image.open(img).convert('RGB')\n",
    "    elif torch.is_tensor(img):\n",
    "        img = tensor_to_pil(img)\n",
    "    elif isinstance(img, np.ndarray):\n",
    "        img = Image.fromarray(img)\n",
    "    # Print ขนาดภาพ (PIL: size = (W, H))\n",
    "    print(\"== plot_bbox_gt_pred ==\")\n",
    "    print(f\"Image size (PIL): {img.size}\")        # (width, height)\n",
    "    if hasattr(img, 'shape'):\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # -- Debug print พิกัด --\n",
    "    print(f\"GT box: {gt_box}\")\n",
    "    print(f\"Pred box: {pred_box}\")\n",
    "    if pred_score is not None:\n",
    "        print(f\"Pred score: {pred_score:.3f}\")\n",
    "\n",
    "    # -- GT Box: วาดแค่เมื่อพิกัด valid --\n",
    "    gt_valid = not (gt_box is None or np.allclose(gt_box, 0) or np.isnan(gt_box).any())\n",
    "    if gt_valid:\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        print(f\"GT box drawn at: ({gt_x1:.1f},{gt_y1:.1f},{gt_x2:.1f},{gt_y2:.1f})\")\n",
    "        rect_gt = plt.Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,\n",
    "                                linewidth=2, edgecolor='blue', facecolor='none', label='GT')\n",
    "        ax.add_patch(rect_gt)\n",
    "    else:\n",
    "        print(\"** WARNING: GT box is missing/invalid (จะไม่วาดกรอบน้ำเงิน) **\")\n",
    "\n",
    "    # -- Pred Box --\n",
    "    pred_valid = not (pred_box is None or np.allclose(pred_box, 0) or np.isnan(pred_box).any())\n",
    "    if pred_valid and ((pred_score is None) or (pred_score > threshold)):\n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        print(f\"Pred box drawn at: ({pred_x1:.1f},{pred_y1:.1f},{pred_x2:.1f},{pred_y2:.1f})\")\n",
    "        rect_pred = plt.Rectangle((pred_x1, pred_y1), pred_x2-pred_x1, pred_y2-pred_y1,\n",
    "                                  linewidth=2, edgecolor='red', facecolor='none', label='Pred')\n",
    "        ax.add_patch(rect_pred)\n",
    "    elif not pred_valid:\n",
    "        print(\"** WARNING: Pred box is missing/invalid (จะไม่วาดกรอบแดง) **\")\n",
    "\n",
    "    handles = []\n",
    "    if gt_valid: handles.append(plt.Line2D([0], [0], color='blue', lw=2, label='GT'))\n",
    "    if pred_valid: handles.append(plt.Line2D([0], [0], color='red', lw=2, label=f'Pred (score>{threshold:.2f})'))\n",
    "    if handles:\n",
    "        ax.legend(handles=handles)\n",
    "    plt.title(\"GT (Blue) / Pred (Red)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "# def decode_caption(cap_ids, inv_vocab_dict, min_length=10):\n",
    "#     cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "#     tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "#     clean = []\n",
    "#     for i, w in enumerate(tokens):\n",
    "#         if w == '<eos>' and i < min_length: continue\n",
    "#         if w == '<eos>' or w == '<pad>': break\n",
    "#         if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "#     return clean\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def compute_metrics(pred_tokens, ref_tokens):\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "    meteor = meteor_score([ref_tokens], pred_tokens)\n",
    "    rscore = rouge.score(\" \".join(ref_tokens), \" \".join(pred_tokens))\n",
    "    rouge1 = rscore['rouge1'].fmeasure\n",
    "    rouge2 = rscore['rouge2'].fmeasure\n",
    "    rougel = rscore['rougeL'].fmeasure\n",
    "    return bleu, rouge1, rouge2, rougel, meteor\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "    union = boxAArea + boxBArea - interArea + 1e-6\n",
    "    return interArea / union if union > 0 else 0\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n",
    "\n",
    "def visualize_result(image, pred_caption, gt_caption, sev_pred, sev_gt,\n",
    "                     cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.16):\n",
    "    import matplotlib.patches as patches\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image)\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    # ไม่วาดกรอบสีเขียว, จะวาดแค่ตอน plot bbox GT vs pred เท่านั้น\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    if cam is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam * 255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_bbox_gt_pred(img, gt_box, pred_box, pred_score=None, threshold=0.16):\n",
    "    # --- Convert ภาพ & แสดงขนาดภาพ ---\n",
    "    if isinstance(img, str):\n",
    "        img = Image.open(img).convert('RGB')\n",
    "    elif torch.is_tensor(img):\n",
    "        img = tensor_to_pil(img)\n",
    "    elif isinstance(img, np.ndarray):\n",
    "        img = Image.fromarray(img)\n",
    "    # Print ขนาดภาพ (PIL: size = (W, H))\n",
    "    print(\"== plot_bbox_gt_pred ==\")\n",
    "    print(f\"Image size (PIL): {img.size}\")        # (width, height)\n",
    "    if hasattr(img, 'shape'):\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # -- Debug print พิกัด --\n",
    "    print(f\"GT box: {gt_box}\")\n",
    "    print(f\"Pred box: {pred_box}\")\n",
    "    if pred_score is not None:\n",
    "        print(f\"Pred score: {pred_score:.3f}\")\n",
    "\n",
    "    # -- GT Box: วาดแค่เมื่อพิกัด valid --\n",
    "    gt_valid = not (gt_box is None or np.allclose(gt_box, 0) or np.isnan(gt_box).any())\n",
    "    if gt_valid:\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        print(f\"GT box drawn at: ({gt_x1:.1f},{gt_y1:.1f},{gt_x2:.1f},{gt_y2:.1f})\")\n",
    "        rect_gt = plt.Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,\n",
    "                                linewidth=2, edgecolor='blue', facecolor='none', label='GT')\n",
    "        ax.add_patch(rect_gt)\n",
    "    else:\n",
    "        print(\"** WARNING: GT box is missing/invalid (จะไม่วาดกรอบน้ำเงิน) **\")\n",
    "\n",
    "    # -- Pred Box --\n",
    "    pred_valid = not (pred_box is None or np.allclose(pred_box, 0) or np.isnan(pred_box).any())\n",
    "    if pred_valid and ((pred_score is None) or (pred_score > threshold)):\n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        print(f\"Pred box drawn at: ({pred_x1:.1f},{pred_y1:.1f},{pred_x2:.1f},{pred_y2:.1f})\")\n",
    "        rect_pred = plt.Rectangle((pred_x1, pred_y1), pred_x2-pred_x1, pred_y2-pred_y1,\n",
    "                                  linewidth=2, edgecolor='red', facecolor='none', label='Pred')\n",
    "        ax.add_patch(rect_pred)\n",
    "    elif not pred_valid:\n",
    "        print(\"** WARNING: Pred box is missing/invalid (จะไม่วาดกรอบแดง) **\")\n",
    "\n",
    "    handles = []\n",
    "    if gt_valid: handles.append(plt.Line2D([0], [0], color='blue', lw=2, label='GT'))\n",
    "    if pred_valid: handles.append(plt.Line2D([0], [0], color='red', lw=2, label=f'Pred (score>{threshold:.2f})'))\n",
    "    if handles:\n",
    "        ax.legend(handles=handles)\n",
    "    plt.title(\"GT (Blue) / Pred (Red)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "# def decode_caption(cap_ids, inv_vocab_dict, min_length=10):\n",
    "#     cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "#     tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "#     clean = []\n",
    "#     for i, w in enumerate(tokens):\n",
    "#         if w == '<eos>' and i < min_length: continue\n",
    "#         if w == '<eos>' or w == '<pad>': break\n",
    "#         if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "#     return clean\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def compute_metrics(pred_tokens, ref_tokens):\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "    meteor = meteor_score([ref_tokens], pred_tokens)\n",
    "    rscore = rouge.score(\" \".join(ref_tokens), \" \".join(pred_tokens))\n",
    "    rouge1 = rscore['rouge1'].fmeasure\n",
    "    rouge2 = rscore['rouge2'].fmeasure\n",
    "    rougel = rscore['rougeL'].fmeasure\n",
    "    return bleu, rouge1, rouge2, rougel, meteor\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "    union = boxAArea + boxBArea - interArea + 1e-6\n",
    "    return interArea / union if union > 0 else 0\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n",
    "\n",
    "def visualize_result(image, pred_caption, gt_caption, sev_pred, sev_gt,\n",
    "                     cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.16):\n",
    "    import matplotlib.patches as patches\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image)\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    # ไม่วาดกรอบสีเขียว, จะวาดแค่ตอน plot bbox GT vs pred เท่านั้น\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    if cam is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam * 255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.detach().cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va4JL2igHcGV"
   },
   "source": [
    "## Optimize decoder parameter **optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2018,
     "status": "error",
     "timestamp": 1749871267605,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "v7ihsTGxHkjU",
    "outputId": "6e704b76-83ef-452c-8bfa-3cf9293176c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:21:01,978] A new study created in memory with name: no-name-191f23a3-320a-4997-ae27-994f29bfd6bc\n",
      "[W 2025-06-14 03:21:03,526] Trial 0 failed with parameters: {'top_k': 10, 'temperature': 1.3602192661049943, 'repetition_penalty': 2.6851275841918483} because of the following error: RuntimeError('Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"<ipython-input-53-932621575>\", line 25, in objective\n",
      "    generated_ids, severity_pred, bbox_pred = model.generate(\n",
      "                                              ^^^^^^^^^^^^^^^\n",
      "  File \"<ipython-input-22-2337632866>\", line 80, in generate\n",
      "    Fc = self.cnn_encoder(image)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<ipython-input-10-1284818446>\", line 30, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "[W 2025-06-14 03:21:03,528] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-932621575>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# n_trials กำหนดจำนวนรอบ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best params:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-932621575>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 generated_ids, severity_pred, bbox_pred = model.generate(\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mimg_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2337632866>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, image, max_len, sos_token, eos_token, device, top_k, temperature, repetition_penalty)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mFc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mFf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrcnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mFfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1284818446>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # กำหนดช่วง parameter ที่จะค้นหา\n",
    "    top_k = trial.suggest_int(\"top_k\", 1, 10)\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.5, 2.0)\n",
    "    repetition_penalty = trial.suggest_float(\"repetition_penalty\", 1.0, 3.0)\n",
    "\n",
    "    # Reset metric\n",
    "    bleu_scores, rouge1s, rouge2s, rougels, meteors, accs, iou_scores, ap05_scores = [], [], [], [], [], [], [], []\n",
    "    all_sev_preds, all_sev_true = [], []\n",
    "\n",
    "    # ---- TEST LOOP ----\n",
    "    for batch in test_loader:\n",
    "        images, captions, severity, bbox_targets = batch[:4]\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        severity = severity.to(device)\n",
    "        bbox_targets = bbox_targets.to(device)\n",
    "        _, _, H, W = images.shape\n",
    "\n",
    "        for b in range(images.size(0)):\n",
    "            img_tensor = images[b].unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                generated_ids, severity_pred, bbox_pred = model.generate(\n",
    "                    img_tensor[0], max_len=50, device=device,\n",
    "                    top_k=top_k, temperature=temperature, repetition_penalty=repetition_penalty\n",
    "                )\n",
    "\n",
    "            pred_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "            ref_tokens  = decode_caption(captions[b].cpu().tolist(), inv_vocab_dict)\n",
    "            sev_pred = int(severity_pred)\n",
    "            sev_gt = int(severity[b].item())\n",
    "\n",
    "            gt_box = bbox_targets[b].detach().cpu().numpy()\n",
    "            pred_box = np.array(bbox_pred)\n",
    "            if np.max(gt_box) <= 1.2:\n",
    "                gt_box = [gt_box[0]*W, gt_box[1]*H, gt_box[2]*W, gt_box[3]*H]\n",
    "            if np.max(pred_box) <= 1.2:\n",
    "                pred_box = [pred_box[0]*W, pred_box[1]*H, pred_box[2]*W, pred_box[3]*H]\n",
    "            iou = box_iou(pred_box, gt_box)\n",
    "            ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "\n",
    "            bleu, r1, r2, rl, meteor = compute_metrics(pred_tokens, ref_tokens)\n",
    "            acc = 1 if sev_pred == sev_gt else 0\n",
    "\n",
    "            bleu_scores.append(bleu)\n",
    "            rouge1s.append(r1)\n",
    "            rouge2s.append(r2)\n",
    "            rougels.append(rl)\n",
    "            meteors.append(meteor)\n",
    "            accs.append(acc)\n",
    "            iou_scores.append(iou)\n",
    "            ap05_scores.append(ap_05)\n",
    "            all_sev_preds.append(sev_pred)\n",
    "            all_sev_true.append(sev_gt)\n",
    "\n",
    "    # ---- Objective: จะเลือกวิธีรวม metric (เช่นเฉลี่ย หรือ กำหนด weight เอง)\n",
    "    metric_score = (\n",
    "        np.mean(bleu_scores) +\n",
    "        np.mean(rouge1s) +\n",
    "        np.mean(rouge2s) +\n",
    "        np.mean(rougels) +\n",
    "        np.mean(meteors)\n",
    "    ) / 5  # เฉลี่ยทุก metric\n",
    "\n",
    "    # หรือถ้าจะ maximize BLEU อย่างเดียว:\n",
    "    # metric_score = np.mean(bleu_scores)\n",
    "\n",
    "    return metric_score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)  # n_trials กำหนดจำนวนรอบ\n",
    "\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "print(\"Best score:\", study.best_trial.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_Qh195cNjB9"
   },
   "source": [
    "## test auto regressive loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1749882170783,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "jwKzwkiCkUqj"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- อัปเดตเส้นทางของ weights ---\n",
    "cnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\"\n",
    "frcnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\"\n",
    "main_model_path = \"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/GRU/bestmodel_20250614_061634.pth\"  # main DKICNet weight\n",
    "decoder_type=\"lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "19wWiPcPj7bdraaTOK-pCycivs7Xv2T2f"
    },
    "executionInfo": {
     "elapsed": 29998,
     "status": "ok",
     "timestamp": 1749882205713,
     "user": {
      "displayName": "ธนาคาร จักรธํารงค์",
      "userId": "08426797893416796095"
     },
     "user_tz": -420
    },
    "id": "_IQ0Jar4XQ7D",
    "outputId": "30197cd6-60ad-4618-b510-cd614b122be1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "# ---- METRIC LOG ----\n",
    "\n",
    "bleu_scores, rouge1s, rouge2s, rougels, meteors, accs = [], [], [], [], [], []\n",
    "iou_scores, ap05_scores = [], []\n",
    "all_sev_preds, all_sev_true = [], []\n",
    "show = 0\n",
    "\n",
    "# ---- ฟอนต์ไทย ----\n",
    "fm.fontManager.addfont('thsarabunnew-webfont.ttf')\n",
    "plt.rcParams['font.family'] = 'TH Sarabun New'\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment_log_novit_GM/Transformer/test_metrics_log_{now}.csv\"\n",
    "print(\"Log test metric to:\", csv_path)\n",
    "\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'image_path', 'bleu', 'rouge1', 'rouge2', 'rougeL', 'meteor',\n",
    "        'severity_acc', 'iou', 'ap@0.5', 'caption_pred', 'caption_gt', 'sev_pred', 'sev_gt'\n",
    "    ])\n",
    "\n",
    "\n",
    "inv_vocab_dict = {idx: word for word, idx in vocab.items()}\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "\n",
    "# --- สร้างโมเดล DKICNet ---\n",
    "vocab_size = 265  # ต้องตรงกับตอนเทรน\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    decoder_type=\"lstm\"  #<------ แก้ decoder ให้ตรง\n",
    ")\n",
    "\n",
    "# ✅ โหลด weights ของ DKICNet\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"✅ Loaded model weights from 'model_state_dict' key.\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"✅ Loaded model weights directly.\")\n",
    "\n",
    "# --- เตรียมอุปกรณ์ & eval mode ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ---- metrics, buffer ----\n",
    "bleu_scores, rouge1s, rouge2s, rougels, meteors, accs = [], [], [], [], [], []\n",
    "iou_scores, ap05_scores = [], []\n",
    "all_sev_preds, all_sev_true = [], []\n",
    "show = 0\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "for batch in test_loader:\n",
    "    images, captions, severity, bbox_targets = batch[:4]\n",
    "    image_paths = batch[4] if len(batch) > 4 else None\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    severity = severity.to(device)\n",
    "    bbox_targets = bbox_targets.to(device)\n",
    "    _, _, H, W = images.shape\n",
    "\n",
    "    for b in range(images.size(0)):\n",
    "        # ------- รองรับ ViT/Multimodal ได้หมด --------\n",
    "        img_tensor = images[b].unsqueeze(0)  # [1, 3, H, W] ส่งเข้า model.generate ทีละภาพ\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids, severity_pred, bbox_pred = model.generate(\n",
    "                img_tensor, max_len=50, device=device, top_k=5, temperature=0.8, repetition_penalty=1.6\n",
    "            )\n",
    "\n",
    "        pred_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(captions[b].cpu().tolist(), inv_vocab_dict)\n",
    "        pred_str = \"\".join(pred_tokens)\n",
    "        gt_str = \"\".join(ref_tokens)\n",
    "        sev_pred = int(severity_pred)\n",
    "        sev_gt = int(severity[b].item())\n",
    "\n",
    "        img_display = image_paths[b] if image_paths is not None else images[b]\n",
    "\n",
    "        gt_box = bbox_targets[b].detach().cpu().numpy()\n",
    "        pred_box = np.array(bbox_pred)\n",
    "\n",
    "        if np.max(gt_box) <= 1.2:\n",
    "            gt_box = [gt_box[0]*W, gt_box[1]*H, gt_box[2]*W, gt_box[3]*H]\n",
    "        if np.max(pred_box) <= 1.2:\n",
    "            pred_box = [pred_box[0]*W, pred_box[1]*H, pred_box[2]*W, pred_box[3]*H]\n",
    "\n",
    "        iou = box_iou(pred_box, gt_box)\n",
    "        ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "\n",
    "        bleu, r1, r2, rl, meteor = compute_metrics(pred_tokens, ref_tokens)\n",
    "        acc = 1 if sev_pred == sev_gt else 0\n",
    "\n",
    "        all_sev_preds.append(sev_pred)\n",
    "        all_sev_true.append(sev_gt)\n",
    "        iou_scores.append(iou)\n",
    "        ap05_scores.append(ap_05)\n",
    "\n",
    "        with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([img_display, bleu, r1, r2, rl, meteor, acc, iou, ap_05, pred_str, gt_str, sev_pred, sev_gt])\n",
    "\n",
    "        bleu_scores.append(bleu)\n",
    "        rouge1s.append(r1)\n",
    "        rouge2s.append(r2)\n",
    "        rougels.append(rl)\n",
    "        meteors.append(meteor)\n",
    "        accs.append(acc)\n",
    "\n",
    "\n",
    "        # ---- GT vs Pred plot (แสดง 20 รูปแรก) ----\n",
    "        if show < 20:\n",
    "            plot_bbox_gt_pred(img_display, gt_box, pred_box, pred_score=None, threshold=0.16)\n",
    "            # GradCAM (optional)\n",
    "            img_t = images[b].unsqueeze(0)\n",
    "            cam = generate_gradcam(model.cnn_encoder, img_t)\n",
    "            visualize_result(\n",
    "                img_display, pred_str, gt_str, sev_pred, sev_gt,\n",
    "                cam=cam,\n",
    "                boxes=None,\n",
    "                box_labels=None,\n",
    "                box_scores=None,\n",
    "                box_threshold=0.16\n",
    "            )\n",
    "            # try:\n",
    "            #     #plot_vit_attention(model, img_t, image_display=img_display)\n",
    "\n",
    "            #     # if torch.is_tensor(img_t):\n",
    "            #     #     img = tensor_to_pil(img_t)\n",
    "            #     #     show_vit_attention_map(\n",
    "            #     #         img=img,                    # PIL\n",
    "            #     #         vit_model=model.vit_encoder,\n",
    "            #     #         img_tensor=img_tensor,\n",
    "            #     #         patch_size=16,              # หรือ 14, 32 แล้วแต่ vit\n",
    "            #     #         head=0                      # หรือเปลี่ยนเป็น 1,2,... ดูแต่ละ head ได้\n",
    "            #     #     )\n",
    "            # except Exception as e:\n",
    "            #     print(\"ViT attention plot error:\", e)\n",
    "            show += 1\n",
    "\n",
    "\n",
    "# ---- SUMMARY REPORT ----\n",
    "def report_metric(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    print(f\"{name:10s} | Mean: {arr.mean():.4f} | Std: {arr.std():.4f}\")\n",
    "\n",
    "print(\"\\n====== SUMMARY TEST RESULT ======\")\n",
    "report_metric(\"BLEU\", bleu_scores)\n",
    "report_metric(\"ROUGE-1\", rouge1s)\n",
    "report_metric(\"ROUGE-2\", rouge2s)\n",
    "report_metric(\"ROUGE-L\", rougels)\n",
    "report_metric(\"METEOR\", meteors)\n",
    "report_metric(\"SeverityAcc\", accs)\n",
    "report_metric(\"IOU\", iou_scores)\n",
    "report_metric(\"AP@0.5\", ap05_scores)\n",
    "\n",
    "# ----- F1, Precision, Recall, Classification Report -----\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_sev_true, all_sev_preds, average=None, zero_division=0)\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_sev_true, all_sev_preds, average='macro', zero_division=0)\n",
    "print(\"\\n--- Severity Classification Metrics (per class) ---\")\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i} | Precision: {precision[i]:.4f} | Recall: {recall[i]:.4f} | F1: {f1[i]:.4f} | Support: {support[i]}\")\n",
    "print(\"\\nMacro  | Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}\".format(precision_macro, recall_macro, f1_macro))\n",
    "print(\"\\nClassification report\\n\", classification_report(all_sev_true, all_sev_preds, digits=4))\n",
    "\n",
    "# ---- CONFUSION MATRIX ----\n",
    "cm = confusion_matrix(all_sev_true, all_sev_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Severity Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ---- HISTOGRAM ----\n",
    "plt.figure()\n",
    "plt.hist(bleu_scores, bins=20, alpha=0.7, label='BLEU')\n",
    "plt.hist(rouge1s, bins=20, alpha=0.7, label='ROUGE-1')\n",
    "plt.hist(rouge2s, bins=20, alpha=0.7, label='ROUGE-2')\n",
    "plt.hist(rougels, bins=20, alpha=0.7, label='ROUGE-L')\n",
    "plt.hist(meteors, bins=20, alpha=0.7, label='METEOR')\n",
    "# plt.hist(iou_scores, bins=20, alpha=0.7, label='IOU')\n",
    "# plt.hist(ap05_scores, bins=2, alpha=0.7, label='AP@0.5')\n",
    "plt.legend()\n",
    "plt.title(\"Test Set Metrics Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYeoO0odFyb_"
   },
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHEYwrJga7LP"
   },
   "source": [
    "### Load pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yJsS_Nja_k_"
   },
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6in3mNFpGjAv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred, threshold=0.1):\n",
    "    # แปลง tensor → PIL\n",
    "    if image_tensor.dim() == 4:\n",
    "        image_tensor = image_tensor[0]\n",
    "    img_pil = to_pil_image(image_tensor.cpu())\n",
    "\n",
    "    # สร้าง plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "    # วาด bounding box (ถ้าพิกัดไม่เป็นศูนย์หมด)\n",
    "    if bbox_pred and not all(v == 0 for v in bbox_pred):\n",
    "        x1, y1, x2, y2 = bbox_pred\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none', label='Pred Box')\n",
    "        ax.add_patch(rect)\n",
    "        plt.legend()\n",
    "\n",
    "    # ใส่ข้อความ caption + severity บนภาพ\n",
    "    ax.text(5, 20, f\"Caption: {caption_str}\", fontsize=12, color='white', backgroundcolor='black')\n",
    "    ax.text(5, 45, f\"Severity: {severity_cls}\", fontsize=12, color='white', backgroundcolor='black')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Model Inference Result\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJvzDX2LbC34"
   },
   "source": [
    "### plot test unseen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "anhyVHcuBOPd"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# --- สร้างโมเดล DKICNet ---\n",
    "vocab_size = 265  # ต้องตรงกับตอนเทรน\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    decoder_type=decoder_type\n",
    ")\n",
    "\n",
    "# ✅ โหลด weights ของ DKICNet\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"✅ Loaded model weights from 'model_state_dict' key.\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"✅ Loaded model weights directly.\")\n",
    "\n",
    "# --- เตรียมอุปกรณ์ & eval mode ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- เตรียมภาพสำหรับ inference ---\n",
    "# image_path = \"/content/drive/MyDrive/Final_Deep_project/Traffy/Traffy_cracked_image/backup/2025-733RNV.jpg\"\n",
    "\n",
    "\n",
    "image_path = \"/content/drive/MyDrive/Final_Deep_project/Traffy/Traffy_cracked_image/selected/2025-ZWW3XT.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image).to(device)\n",
    "\n",
    "# --- Run inference ---\n",
    "generated_ids, severity_cls, bbox_pred =  model.generate(\n",
    "    img_tensor[0], max_len=50, device=device, top_k=1, temperature=1\n",
    ")\n",
    "# --- Decode คำบรรยาย ---\n",
    "caption_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "caption_str = \"\".join(caption_tokens)\n",
    "\n",
    "# --- แปลง bbox เป็นพิกัดภาพ (ถ้า normalized) ---\n",
    "_, H, W = image_tensor.shape\n",
    "if max(bbox_pred) <= 1.2:\n",
    "    bbox_pred = [bbox_pred[0]*W, bbox_pred[1]*H, bbox_pred[2]*W, bbox_pred[3]*H]\n",
    "\n",
    "# --- แสดงผลลัพธ์ ---\n",
    "print(\"== DEBUG ==\")\n",
    "print(\"Raw IDs               :\", generated_ids)\n",
    "print(\"Decoded tokens        :\", caption_tokens)\n",
    "print(\"Predicted Caption     :\", caption_str)\n",
    "print(\"Predicted Severity    :\", severity_cls)\n",
    "print(\"Predicted BoundingBox :\", bbox_pred)\n",
    "\n",
    "# --- Plot ผลลัพธ์ ---\n",
    "plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpBYfxWPw-0p"
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oNwGo9LxHO7E"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "torch.manual_seed(314)\n",
    "\n",
    "# --- Import module/model ที่คุณสร้างเอง ตรงนี้ต้อง import ให้ถูก ---\n",
    "# from yourmodule import DKICNet, decode_caption, inv_vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "# === โหลด vocab dict ===\n",
    "# inv_vocab_dict = torch.load(\"/content/drive/MyDrive/Final_Deep_project/vocab_inv_dict.pth\") # ถ้ามีไฟล์นี้\n",
    "\n",
    "# === สร้างและโหลดโมเดล ===\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    decoder_type=decoder_type\n",
    ")\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint.get(\"model_state_dict\", checkpoint))\n",
    "print(\"✅ Loaded model weights.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Transform ภาพ ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === ฟังก์ชันวาดผลลัพธ์ ===\n",
    "def plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred):\n",
    "    img_pil = to_pil_image(image_tensor.cpu())\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "    if bbox_pred and not all(v == 0 for v in bbox_pred):\n",
    "        x1, y1, x2, y2 = bbox_pred\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.text(5, 20, f\"Caption: {caption_str}\", fontsize=10, color='white', backgroundcolor='black')\n",
    "    ax.text(5, 45, f\"Severity: {severity_cls}\", fontsize=10, color='white', backgroundcolor='black')\n",
    "    ax.axis('off')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # -- fixed: ใช้ buffer_rgba แทน tostring_rgb\n",
    "    img_np = np.array(fig.canvas.buffer_rgba())[..., :3]\n",
    "    plt.close(fig)\n",
    "    return img_np\n",
    "\n",
    "# === ฟังก์ชันหลักสำหรับ inference (เพิ่ม top_k, temperature) ===\n",
    "def gradio_inference(image, top_k, temperature, repetitive_penalty):\n",
    "    try:\n",
    "        # 1. Transform\n",
    "        image_tensor = transform(image).to(device)\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "        # 2. Run inference (ส่ง top_k, temperature)\n",
    "        generated_ids, severity_cls, bbox_pred = model.generate(\n",
    "            image_tensor[0], max_len=50, device=device, top_k=int(top_k), temperature=float(temperature), repetition_penalty=float(repetitive_penalty)\n",
    "        )\n",
    "\n",
    "        caption_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "        caption_str = \"\".join(caption_tokens)\n",
    "\n",
    "        _, H, W = image_tensor.shape[-3:]\n",
    "        if max(bbox_pred) <= 1.2:\n",
    "            bbox_pred = [bbox_pred[0]*W, bbox_pred[1]*H, bbox_pred[2]*W, bbox_pred[3]*H]\n",
    "\n",
    "        img_np = plot_inference_result(image_tensor[0], caption_str, severity_cls, bbox_pred)\n",
    "        return img_np, caption_str, str(severity_cls), str(bbox_pred)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        err = f\"[ERROR]\\n{traceback.format_exc()}\"\n",
    "        print(err)\n",
    "        return np.zeros((224,224,3),dtype=np.uint8), err, \"ERROR\", \"ERROR\"\n",
    "\n",
    "# === Gradio interface (เพิ่ม slider สำหรับ top_k, temperature) ===\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_inference,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"📷 Upload bridge image\"),\n",
    "        gr.Slider(1, 20, value=1, step=1, label=\"top_k (sampling)\"),\n",
    "        gr.Slider(0.1, 2.0, value=1.0, step=0.05, label=\"temperature (sampling)\"),\n",
    "        gr.Slider(1,3, value=1.0, step=0.05, label=\"repetitive penalty\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(type=\"numpy\", label=\"📌 Prediction Result\"),\n",
    "        gr.Textbox(label=\"📝 Caption\"),\n",
    "        gr.Textbox(label=\"🔥 Severity Class\"),\n",
    "        gr.Textbox(label=\"🧭 Bounding Box\")\n",
    "    ],\n",
    "    title=\"Crack Damage Caption Generator\",\n",
    "    description=\"อัปโหลด  → ระบบจะอธิบายความเสียหาย พร้อมพยากรณ์ความรุนแรง และกรอบตำแหน่ง\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMau4tfdk1Mv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [
    {
     "file_id": "1zl6PampOyPkSjf1S_yPcLr0cq-jW4WDv",
     "timestamp": 1749787017207
    },
    {
     "file_id": "1zh7Zu9uwwRP-T01zgnUFavhqdlBXvX8L",
     "timestamp": 1749699607378
    },
    {
     "file_id": "11N0iPsQsBvNXqloZ0GGHKQkecePy9_AU",
     "timestamp": 1749692538802
    },
    {
     "file_id": "1ikpa4adMRiggogf28UPumCzcfwW2_0vg",
     "timestamp": 1749648377203
    },
    {
     "file_id": "1Ni1kpMwOyPzEVGuXEqvkaPSxouee5J0L",
     "timestamp": 1749645974509
    },
    {
     "file_id": "19vbINb0bKgb_rvYBfL7U62YcZOgLpfLg",
     "timestamp": 1749522228176
    },
    {
     "file_id": "1NcuU4ag0XBS0UrZlFN_nSq7OYuMMzIS_",
     "timestamp": 1749512402692
    },
    {
     "file_id": "1460tRfu5PAgva40B_zz5kuO63t0qlWBV",
     "timestamp": 1749476801985
    },
    {
     "file_id": "1WRfPmo51tI8I286iC8-4vsXqG9gjfvE5",
     "timestamp": 1749453036817
    },
    {
     "file_id": "1OjTurtXyu_J0x5zhW-qOhwn41GVknOUm",
     "timestamp": 1749432614736
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
