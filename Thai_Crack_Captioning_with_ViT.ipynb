{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6BEFxhkWfKm"
   },
   "source": [
    "## Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27975,
     "status": "ok",
     "timestamp": 1749910356248,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "CTPHueo8VLu7",
    "outputId": "042c449c-1355-4ca1-dbfd-acac6dedd01a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi5GTL7V34Vp"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12724,
     "status": "ok",
     "timestamp": 1749910396422,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "pbm2DCED33su",
    "outputId": "2d6abe94-34bd-493e-b5fb-32c86a61c07e"
   },
   "outputs": [],
   "source": [
    "!pip install pythainlp   rouge-score nltk deepcut optuna gradio -q\n",
    "!wget -q https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
    "!ls\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge2lPFmJ37hB"
   },
   "source": [
    "## CNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nwZHBkBWhYN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, backbone_name=\"resnet50\", pretrained=True, weight_path=None, map_location=None):\n",
    "        super().__init__()\n",
    "        backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "        # Initial layers: Conv1, BN, ReLU, MaxPool\n",
    "        self.conv1 = backbone.conv1\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.relu = backbone.relu\n",
    "        self.maxpool = backbone.maxpool\n",
    "        self.layer1 = backbone.layer1\n",
    "        self.layer2 = backbone.layer2\n",
    "        self.layer3 = backbone.layer3\n",
    "        self.layer4 = backbone.layer4\n",
    "\n",
    "        # Load weights from custom path\n",
    "        if weight_path is not None:\n",
    "            state = torch.load(weight_path, map_location=map_location)\n",
    "            # ถ้า save state dict แบบ model.state_dict()\n",
    "            if 'state_dict' in state:\n",
    "                # รองรับ checkpoint บางแบบ เช่น {'state_dict': ...}\n",
    "                state = state['state_dict']\n",
    "            self.load_state_dict(state, strict=False)  # strict=False ในกรณีที่ layer ไม่ตรงเป๊ะ\n",
    "            print(f\"Loaded CNN weights from {weight_path}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWd3vZ3Y390e"
   },
   "source": [
    "### test CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7404,
     "status": "ok",
     "timestamp": 1749822411637,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "6j1JPjxPX00G",
    "outputId": "e7c92663-b13b-4c6e-94b2-e13e55d08234"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ใส่ path ไฟล์ weights (หรือ None ถ้าไม่โหลด custom weight)\n",
    "weight_path = \"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\"  # เปลี่ยนเป็น path จริง ถ้ามี\n",
    "\n",
    "# สร้าง encoder (โหลด weights ถ้าระบุ)\n",
    "encoder = CNNEncoder(\"resnet50\", pretrained=False, weight_path=weight_path, map_location='cpu')\n",
    "\n",
    "# ทดสอบ forward\n",
    "dummy_input = torch.randn(2, 3, 224, 224)  # batch_size=2, RGB, 224x224\n",
    "Fc = encoder(dummy_input)\n",
    "# assert ตรวจสอบ shape\n",
    "expected_shape = (2, 2048, 7, 7)  # สำหรับ resnet50\n",
    "assert Fc.shape == expected_shape, f\"Expected {expected_shape}, got {Fc.shape}\"\n",
    "\n",
    "print(\"Test passed: Feature shape is correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzpkWTMQ4BsZ"
   },
   "source": [
    "## FasterRCNNBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb6JPPFLX1WA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "class FasterRCNNBackbone(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_path=None, out_size=14, map_location='cpu'):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        if pretrained_path is not None:\n",
    "            state = torch.load(pretrained_path, map_location=map_location)\n",
    "            try:\n",
    "                self.model.load_state_dict(state)\n",
    "                print(f\"Loaded FasterRCNN weights from {pretrained_path}\")   # <-- เพิ่มตรงนี้\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Cannot load state dict: {e}\")\n",
    "        self.backbone = self.model.backbone\n",
    "        self.out_channels = self.backbone.out_channels\n",
    "        self.out_size = out_size\n",
    "        self.feature_adj = nn.Sequential(\n",
    "            nn.Conv2d(self.out_channels, 256, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, boxes=None, detect=False):\n",
    "        if detect:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                x = [img for img in x]\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            features = self.backbone(x)['0']  # (B, 256, H, W)\n",
    "            B = features.shape[0]\n",
    "            device = features.device\n",
    "            if boxes is None:\n",
    "                H, W = features.shape[2], features.shape[3]\n",
    "                boxes = [torch.tensor([[0, 0, W-1, H-1]], dtype=torch.float, device=device) for _ in range(B)]\n",
    "                boxes = torch.cat([\n",
    "                    torch.cat([torch.full((1, 1), i, device=device), b], dim=1)\n",
    "                    for i, b in enumerate(boxes)\n",
    "                ], dim=0)\n",
    "            roi_features = roi_align(features, boxes, output_size=(self.out_size, self.out_size))\n",
    "            roi_features = self.feature_adj(roi_features)\n",
    "            return roi_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaHVHHa34DdP"
   },
   "source": [
    "### test FasterRCNNBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11469,
     "status": "ok",
     "timestamp": 1749822423120,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "a_PYL9H0Yy5C",
    "outputId": "e49ba634-2949-4216-916f-d744a3bb781a"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "num_classes = 4  # background + bridge_damage (แก้ตาม dataset)\n",
    "pretrained_path = \"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\"\n",
    "faster_rcnn_encoder = FasterRCNNBackbone(num_classes, pretrained_path=pretrained_path, map_location='cpu')\n",
    "dummy_input = torch.randn(2, 3, 224, 224)\n",
    "Ff = faster_rcnn_encoder(dummy_input)\n",
    "# สมมติขนาด output ที่คาดหวัง เช่น (2, 256, 56, 56) ขึ้นกับ backbone\n",
    "assert Ff.shape[0] == 2 and Ff.shape[1] == 256, f\"Unexpected shape: {Ff.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5Wa5xei0Q4R"
   },
   "source": [
    "## vision transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uP06Xjs70l50"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, vit_model=\"vit_base_patch16_224\", pretrained=True, weight_path=None):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(vit_model, pretrained=pretrained, num_classes=0)  # remove classifier\n",
    "        if weight_path is not None:\n",
    "            state_dict = torch.load(weight_path, map_location='cpu')\n",
    "            # ถ้าตัว weight เป็น dict ที่มี 'model' ต้องแกะก่อน (เช่น {'model': state_dict})\n",
    "            if 'model' in state_dict:\n",
    "                state_dict = state_dict['model']\n",
    "            self.vit.load_state_dict(state_dict, strict=False)  # strict=False กัน layer name ไม่ตรงบางตัว\n",
    "            print(f\"Loaded ViT weights from {weight_path}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W] --> output: [B, 768]\n",
    "        feat = self.vit(x)  # vit-base returns (B, 768)\n",
    "        return feat.unsqueeze(-1).unsqueeze(-1)  # [B, 768, 1, 1] for compatibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkcQ1YXW03I0"
   },
   "source": [
    "### test vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15764,
     "status": "ok",
     "timestamp": 1749822442349,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "kPXTz-sY02EU",
    "outputId": "b664a941-9e41-47e7-e629-2166468c04fc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# สร้าง instance ของ ViTEncoder\n",
    "vit_encoder = ViTEncoder(\n",
    "    vit_model=\"vit_base_patch16_224\",\n",
    "    pretrained=False,  # ถ้าอยากใช้แค่ weights ที่ finetune, preload ไม่ต้องก็ได้\n",
    "    weight_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "# สร้าง dummy input: batch size = 2, 3 channel, 224x224\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# ทดสอบ forward pass\n",
    "with torch.no_grad():\n",
    "    out = vit_encoder(dummy_img)\n",
    "\n",
    "# ตรวจสอบ shape ของ output\n",
    "print(\"Output shape:\", out.shape)  # ต้องได้ [2, 768, 1, 1]\n",
    "\n",
    "assert out.shape == (2, 768, 1, 1), f\"Output shape ผิด: {out.shape}\"\n",
    "print(\"ViTEncoder test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvwfGW9d4Fel"
   },
   "source": [
    "## FeatureFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AM961IDZDVC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureFusion(nn.Module):\n",
    "    def __init__(self, in_cnn, in_frcnn, in_vit, out_c=512, reduction=16):\n",
    "        \"\"\"\n",
    "        in_cnn: channels of CNN feature map (Fc)\n",
    "        in_frcnn: channels of Faster R-CNN feature map (Ff)\n",
    "        in_vit: channels of ViT feature map (Fv)\n",
    "        out_c: common channel dimension after adjustment\n",
    "        reduction: for bottleneck in attention mechanism\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.adj_conv_cnn = nn.Conv2d(in_cnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_frcnn = nn.Conv2d(in_frcnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_vit = nn.Conv2d(in_vit, out_c, kernel_size=1)\n",
    "\n",
    "        self.pw_conv1 = nn.Conv2d(3*out_c, 3*out_c // reduction, kernel_size=1)\n",
    "        self.pw_conv2 = nn.Conv2d(3*out_c // reduction, 3*out_c, kernel_size=1)\n",
    "\n",
    "    def forward(self, Fc, Ff, Fv):\n",
    "        \"\"\"\n",
    "        Fc: (B, in_cnn, H, W)  -- CNN feature\n",
    "        Ff: (B, in_frcnn, H, W) -- Faster R-CNN feature\n",
    "        Fv: (B, in_vit, 1, 1)   -- ViT feature (หรือ (B, in_vit, H, W) ก็ได้)\n",
    "        Returns: (B, 3*out_c, H, W) fused features (weighted)\n",
    "        \"\"\"\n",
    "        # 1. ปรับขนาด channel ให้เท่ากัน\n",
    "        Fc_adj = self.adj_conv_cnn(Fc)     # (B, out_c, H, W)\n",
    "        Ff_adj = self.adj_conv_frcnn(Ff)   # (B, out_c, H, W)\n",
    "        Fv_adj = self.adj_conv_vit(Fv)     # (B, out_c, 1, 1) (อาจจะเป็น (B, out_c, H, W) ก็ได้)\n",
    "\n",
    "        # 2. ถ้า Fv เป็น (B, out_c, 1, 1) —> upsample ให้เท่ากับ (H, W)\n",
    "        H, W = Fc_adj.shape[2], Fc_adj.shape[3]\n",
    "        if Fv_adj.shape[2:] != (H, W):\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=(H, W), mode='nearest')\n",
    "\n",
    "        # 3. concat along channel\n",
    "        F_concat = torch.cat([Fc_adj, Ff_adj, Fv_adj], dim=1)  # (B, 3*out_c, H, W)\n",
    "\n",
    "        # 4. Channel Attention (Squeeze-and-Excitation)\n",
    "        FGAP = F.adaptive_avg_pool2d(F_concat, (1, 1))\n",
    "        Fpw1 = F.relu(self.pw_conv1(FGAP))\n",
    "        Fpw2 = torch.sigmoid(self.pw_conv2(Fpw1))\n",
    "        F_weighted = F_concat * Fpw2\n",
    "        return F_weighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BivdGfIO4G73"
   },
   "source": [
    "### test FeatureFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1749822442418,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "E2bfU0jGZie0",
    "outputId": "f57f362b-93f3-4982-e27f-adfd9cbc5cce"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# Suppose Fc is from CNN (B, 2048, 7, 7), Ff is from FPN of Faster R-CNN (B, 256, 7, 7)\n",
    "# Set out_c=512 (หรือเลือกเท่าที่ต้องการ)\n",
    "fusion = FeatureFusion(in_cnn=2048, in_frcnn=256, in_vit=768, out_c=512)\n",
    "Fc = torch.randn(2, 2048, 7, 7)   # ResNet\n",
    "Ff = torch.randn(2, 256, 7, 7)    # Faster R-CNN\n",
    "Fv = torch.randn(2, 768, 1, 1)    # ViT (B, 768, 1, 1)\n",
    "F_fused = fusion(Fc, Ff, Fv)\n",
    "print(\"Fused feature shape:\", F_fused.shape)  # (2, 1536, 7, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYw8MSEs4IuX"
   },
   "source": [
    "## FeatureFusionWithRecalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIz-v4CwZjYh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureFusionWithRecalibration(nn.Module):\n",
    "    def __init__(self, in_cnn, in_frcnn, in_vit, out_c=512, reduction=16):\n",
    "        super().__init__()\n",
    "        self.adj_conv_cnn = nn.Conv2d(in_cnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_frcnn = nn.Conv2d(in_frcnn, out_c, kernel_size=1)\n",
    "        self.adj_conv_vit = nn.Conv2d(in_vit, out_c, kernel_size=1)\n",
    "\n",
    "        self.pw_conv1 = nn.Conv2d(3*out_c, 3*out_c // reduction, kernel_size=1)\n",
    "        self.pw_conv2 = nn.Conv2d(3*out_c // reduction, 3*out_c, kernel_size=1)\n",
    "\n",
    "    def forward(self, Fc, Ff, Fv):\n",
    "        # Adjust\n",
    "        Fc_adj = self.adj_conv_cnn(Fc)       # (B, C, H, W)\n",
    "        Ff_adj = self.adj_conv_frcnn(Ff)     # (B, C, h, w)\n",
    "        Fv_adj = self.adj_conv_vit(Fv)       # (B, C, 1, 1)\n",
    "\n",
    "        # Resize if needed\n",
    "        H, W = Fc_adj.shape[2:]\n",
    "        if Ff_adj.shape[2:] != (H, W):\n",
    "            Ff_adj = F.interpolate(Ff_adj, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        if Fv_adj.shape[2:] != (H, W):\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=(H, W), mode='nearest')\n",
    "\n",
    "        # Concat\n",
    "        F_concat = torch.cat([Fc_adj, Ff_adj, Fv_adj], dim=1)  # (B, 3C, H, W)\n",
    "\n",
    "        # Squeeze & Excitation (attention)\n",
    "        FGAP = F.adaptive_avg_pool2d(F_concat, (1, 1))         # (B, 3C, 1, 1)\n",
    "        Fpw1 = F.relu(self.pw_conv1(FGAP))                     # (B, 3C//r, 1, 1)\n",
    "        Fpw2 = torch.sigmoid(self.pw_conv2(Fpw1))              # (B, 3C, 1, 1)\n",
    "        wc, wf, wv = torch.chunk(Fpw2, 3, dim=1)               # (B, C, 1, 1) x 3\n",
    "\n",
    "        # Recalibrate\n",
    "        Fc_recal = Fc_adj * wc\n",
    "        Ff_recal = Ff_adj * wf\n",
    "        Fv_recal = Fv_adj * wv\n",
    "\n",
    "        # Fusion แบบ simple (สามารถปรับสูตรตามต้องการ)\n",
    "        Ffinal_c = Fc_adj + Ff_recal + Fv_recal\n",
    "        Ffinal_f = Ff_adj + Fc_recal + Fv_recal\n",
    "        Ffinal_v = Fv_adj + Fc_recal + Ff_recal\n",
    "\n",
    "        Ffinal = torch.cat([Ffinal_c, Ffinal_f, Ffinal_v], dim=1)  # (B, 3C, H, W)\n",
    "        return Ffinal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in9rL_xG4KXV"
   },
   "source": [
    "### test FeatureFusionWithRecalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749822442485,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "Z8q8ZXeZaXzm",
    "outputId": "7f226d92-d80d-4487-ca01-d964b080e483"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "module = FeatureFusionWithRecalibration(in_cnn=2048, in_frcnn=256, in_vit=768, out_c=512)\n",
    "Fc = torch.randn(2, 2048, 7, 7)\n",
    "Ff = torch.randn(2, 256, 7, 7)\n",
    "Fv = torch.randn(2, 768, 1, 1)\n",
    "Ffinal = module(Fc, Ff, Fv)\n",
    "print(\"Output shape:\", Ffinal.shape)  # (2, 1536, 7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSWHEuWO4MaF"
   },
   "source": [
    "## CorrelationAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mGRnWGSf1dc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CorrelationAwareAttention(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, embed_dim, num_sources=3):\n",
    "        super().__init__()\n",
    "        # Embedding layer for Ffinal (ลด channel)\n",
    "        self.embedding = nn.Conv2d(feat_dim, embed_dim, kernel_size=1)\n",
    "        # Attention transforms\n",
    "        self.W_e = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w_a = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.num_sources = num_sources\n",
    "\n",
    "    def forward(self, Ffinal, Fc_adj, Ff_adj, Fv_adj, h_t):\n",
    "        # Ffinal: (B, 3C, H, W)\n",
    "        # Fc_adj, Ff_adj, Fv_adj: (B, C, H, W)\n",
    "        # h_t: (B, hidden_dim)\n",
    "\n",
    "        # 1) Embedding multi-level feature\n",
    "        Eemb = self.embedding(Ffinal)  # (B, embed_dim, H, W)\n",
    "        B, E, H, W = Eemb.shape\n",
    "        N = H * W\n",
    "\n",
    "        # Resize Ff_adj, Fv_adj ให้ spatial เท่ากับ Fc_adj ก่อน .view\n",
    "        if Fc_adj.shape[2:] != Ff_adj.shape[2:]:\n",
    "            Ff_adj = F.interpolate(Ff_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "        if Fc_adj.shape[2:] != Fv_adj.shape[2:]:\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Flatten spatial dims for attention computation\n",
    "        Eemb_flat = Eemb.view(B, E, N).transpose(1, 2)  # (B, N, E)\n",
    "\n",
    "        # 2) Correlation Matrices\n",
    "        C = Fc_adj.shape[1]  # channel dim (should == out_c จาก fusion)\n",
    "        Ec = Fc_adj.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "        Ef = Ff_adj.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "        Ev = Fv_adj.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "\n",
    "        # คำนวณ correlation แบบคู่ (ถ้าอยาก customize เพิ่มเติม แจ้งได้)\n",
    "        Rc = torch.bmm(Ec, Ef.transpose(1, 2))  # (B, N, N)\n",
    "        Rv = torch.bmm(Ec, Ev.transpose(1, 2))  # (B, N, N)\n",
    "        # Normalize\n",
    "        Rc_norm = torch.softmax(Rc, dim=-1)\n",
    "        Rv_norm = torch.softmax(Rv, dim=-1)\n",
    "\n",
    "        # 3) Traditional Attention Score\n",
    "        Weei = self.W_e(Eemb_flat)                 # (B, N, hidden_dim)\n",
    "        Whht = self.W_h(h_t).unsqueeze(1)          # (B, 1, hidden_dim)\n",
    "        et = self.w_a(torch.tanh(Weei + Whht)).squeeze(-1)  # (B, N)\n",
    "\n",
    "        # 4) Adjust attention by correlation\n",
    "        e_corr_c = torch.bmm(Rc_norm, et.unsqueeze(2)).squeeze(-1)  # (B, N)\n",
    "        e_corr_v = torch.bmm(Rv_norm, et.unsqueeze(2)).squeeze(-1)  # (B, N)\n",
    "        st = et + e_corr_c + e_corr_v                               # (B, N)\n",
    "        alpha = torch.softmax(st, dim=1)                            # (B, N)\n",
    "\n",
    "        # 5) Weighted sum for context vector\n",
    "        zt = torch.bmm(alpha.unsqueeze(1), Eemb_flat).squeeze(1)  # (B, embed_dim)\n",
    "\n",
    "        return zt, alpha  # (B, embed_dim), (B, N)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8nyd3j8NCWg"
   },
   "source": [
    "### test CorrelationAwareAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1749822442552,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "P-kAeqjX2bhN",
    "outputId": "d8397936-0fe8-4d5c-ff10-82dc3cf46677"
   },
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 512, 7, 7\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "module = CorrelationAwareAttention(feat_dim=3*C, hidden_dim=hidden_dim, embed_dim=embed_dim)\n",
    "Ffinal = torch.randn(B, 3*C, H, W)\n",
    "Fc_adj = torch.randn(B, C, H, W)\n",
    "Ff_adj = torch.randn(B, C, H, W)\n",
    "Fv_adj = torch.randn(B, C, H, W)\n",
    "h_t = torch.randn(B, hidden_dim)\n",
    "zt, alpha = module(Ffinal, Fc_adj, Ff_adj, Fv_adj, h_t)\n",
    "print(\"zt:\", zt.shape, \"alpha:\", alpha.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELR_TOL8ViRl"
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1B_VYsD4PrN"
   },
   "source": [
    "## LSTMDecoderCorrAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T7MqDIWaYM0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTMDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions, states):\n",
    "        # zt: (B, embed_dim)  context from attention\n",
    "        batch_size, seq_len = captions.size()\n",
    "        outputs = []\n",
    "        h, c = states\n",
    "        for t in range(seq_len):\n",
    "            xt = self.word_embedding(captions[:, t])   # (B, embed_dim)\n",
    "            lstm_input = torch.cat([xt, zt], dim=1)    # (B, 2*embed_dim)\n",
    "            h, c = self.lstm(lstm_input, (h, c))       # h, c: (B, hidden_dim)\n",
    "            out = self.fc(h)    # (B, vocab_size)\n",
    "            outputs.append(out.unsqueeze(1))           # (B, 1, vocab_size)\n",
    "        outputs = torch.cat(outputs, dim=1)            # (B, seq_len, vocab_size)\n",
    "        return outputs, (h, c)                         # <=== สำคัญ!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY-1d2QCHk5D"
   },
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn2RAhK0HkW6"
   },
   "outputs": [],
   "source": [
    "class GRUDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_sources=1):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # ถ้า context มาจากหลาย source: zt size = num_sources * embed_dim\n",
    "        self.gru = nn.GRUCell(embed_dim + num_sources*embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions, h_t):\n",
    "        \"\"\"\n",
    "        zt: (B, num_sources*embed_dim)\n",
    "        captions: (B, seq_len)\n",
    "        h_t: (B, hidden_dim)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        B, seq_len = captions.shape\n",
    "        for t in range(seq_len):\n",
    "            xt = self.word_embedding(captions[:, t])         # (B, embed_dim)\n",
    "            gru_input = torch.cat([xt, zt], dim=1)           # (B, embed_dim + num_sources*embed_dim)\n",
    "            h_t = self.gru(gru_input, h_t)                   # (B, hidden_dim)\n",
    "            out = self.fc(h_t)                               # (B, vocab_size)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "        return torch.cat(outputs, dim=1), h_t  # (B, seq_len, vocab_size), (B, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1nFLYKoLFRM"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4e5mLbEN_o-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class TransformerDecoderCorrAttn(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers=2, nhead=4, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # use [batch, seq, dim]\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, zt, captions):\n",
    "        # รองรับ shape (seq_len,), (1, seq_len), (B, seq_len), (B, 1, seq_len)\n",
    "        captions = captions.long()\n",
    "        if captions.dim() == 1:\n",
    "            captions = captions.unsqueeze(0)  # (1, seq_len)\n",
    "        elif captions.dim() == 3:\n",
    "            # เช่น (B, 1, seq_len)\n",
    "            captions = captions.squeeze(1)\n",
    "        # ตอนนี้ captions เป็น (B, seq_len)\n",
    "        B, seq_len = captions.shape\n",
    "\n",
    "        cap_emb = self.word_embedding(captions)         # (B, seq_len, embed_dim)\n",
    "        cap_emb = self.pos_encoder(cap_emb)             # add pos encoding\n",
    "\n",
    "        # ทำให้ zt เป็น (B, 1, embed_dim)\n",
    "        if zt.dim() == 2:\n",
    "            zt = zt.unsqueeze(1)  # (B, 1, embed_dim)\n",
    "\n",
    "        out = self.transformer_decoder(\n",
    "            cap_emb,             # tgt (B, seq_len, embed_dim)\n",
    "            zt                   # memory (B, 1, embed_dim)\n",
    "        )\n",
    "        out = self.fc(out)  # (B, seq_len, vocab_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8ktBxJC4RO7"
   },
   "source": [
    "#DKICNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJvWD30EC_fM"
   },
   "outputs": [],
   "source": [
    "class DKICNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_backbone=\"resnet50\", cnn_pretrained=False,\n",
    "        num_classes=4, out_c=512, reduction=16,\n",
    "        embed_dim=256, hidden_dim=512, vocab_size=1000,\n",
    "        severity_classes=4, cnn_pretrained_path=None,\n",
    "        frcnn_pretrained_path=None, vit_model=\"vit_base_patch16_224\",\n",
    "        vit_pretrained_path=None,\n",
    "        decoder_type=\"gru\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder_type=decoder_type\n",
    "        self.cnn_encoder = CNNEncoder(cnn_backbone, pretrained=cnn_pretrained, weight_path=cnn_pretrained_path)\n",
    "        self.frcnn_encoder = FasterRCNNBackbone(num_classes=num_classes, pretrained_path=frcnn_pretrained_path)\n",
    "        self.vit_encoder = ViTEncoder(vit_model=vit_model, pretrained=True, weight_path=vit_pretrained_path)\n",
    "\n",
    "        self.fusion = FeatureFusionWithRecalibration(\n",
    "            in_cnn=2048, in_frcnn=256, in_vit=768, out_c=out_c, reduction=reduction\n",
    "        )\n",
    "        self.attn = CorrelationAwareAttention(\n",
    "            feat_dim=3 * out_c, hidden_dim=hidden_dim, embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        if self.decoder_type == \"gru\":\n",
    "            self.decoder = GRUDecoderCorrAttn(embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size)\n",
    "        elif self.decoder_type == \"lstm\":\n",
    "            self.decoder = LSTMDecoderCorrAttn(embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size)\n",
    "        elif self.decoder_type == \"transformer\":\n",
    "            self.decoder = TransformerDecoderCorrAttn(\n",
    "                embed_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=vocab_size,\n",
    "                nhead=2, num_layers=1, dropout=0.25\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"decoder_type '{decoder_type}' not supported\")\n",
    "\n",
    "        self.severity_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.Linear(3 * out_c, 128), nn.ReLU(), nn.Linear(128, severity_classes)\n",
    "        )\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.Linear(3 * out_c, 128), nn.ReLU(), nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, states=None):\n",
    "        Fc = self.cnn_encoder(images)\n",
    "        Ff = self.frcnn_encoder(images)\n",
    "        Fv = self.vit_encoder(images)\n",
    "        Ffinal = self.fusion(Fc, Ff, Fv)\n",
    "        Fc_adj = self.fusion.adj_conv_cnn(Fc)\n",
    "        Ff_adj = self.fusion.adj_conv_frcnn(Ff)\n",
    "        Fv_adj = self.fusion.adj_conv_vit(Fv)\n",
    "        Fv_adj = F.interpolate(Fv_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "        B = images.size(0)\n",
    "        hidden_dim = self.attn.W_h.in_features\n",
    "        h_t = torch.zeros(B, hidden_dim, device=images.device)\n",
    "        zt, alpha = self.attn(Ffinal, Fc_adj, Ff_adj, Fv_adj, h_t)\n",
    "        if self.decoder_type == \"gru\":\n",
    "            output, _ = self.decoder(zt, captions, h_t)\n",
    "        elif self.decoder_type == \"lstm\":\n",
    "            c_t = torch.zeros(B, hidden_dim, device=images.device)\n",
    "            output, _ = self.decoder(zt, captions, (h_t, c_t))\n",
    "        else:\n",
    "            output = self.decoder(zt, captions)\n",
    "        severity_logits = self.severity_head(Ffinal)\n",
    "        bbox_pred = self.bbox_head(Ffinal)\n",
    "        return output, alpha, severity_logits, bbox_pred\n",
    "\n",
    "    def top_k_temperature_sampling(self, logits, top_k=1, temperature=1.0):\n",
    "        logits = logits / temperature\n",
    "        values, indices = torch.topk(logits, top_k)\n",
    "        probs = torch.softmax(values, dim=-1)\n",
    "        idx = torch.multinomial(probs, 1)\n",
    "        next_token = indices[idx]\n",
    "        return next_token.item()\n",
    "\n",
    "    def generate(self, image, max_len=50, sos_token=2, eos_token=3, device='cuda', top_k=1, temperature=1.0, repetition_penalty=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "\n",
    "            # ==== Feature Extraction ====\n",
    "            Fc = self.cnn_encoder(image)\n",
    "            Ff = self.frcnn_encoder(image)\n",
    "            Fv = self.vit_encoder(image)\n",
    "            Ffinal = self.fusion(Fc, Ff, Fv)\n",
    "            Fc_adj = self.fusion.adj_conv_cnn(Fc)\n",
    "            Ff_adj = self.fusion.adj_conv_frcnn(Ff)\n",
    "            Fv_adj = self.fusion.adj_conv_vit(Fv)\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "            hidden_dim = self.attn.W_h.in_features\n",
    "            h_t = torch.zeros(1, hidden_dim, device=device)\n",
    "            zt, _ = self.attn(Ffinal, Fc_adj, Ff_adj, Fv_adj, h_t)\n",
    "\n",
    "            # ==== Heads ====\n",
    "            severity_logits = self.severity_head(Ffinal)\n",
    "            bbox_pred = self.bbox_head(Ffinal)\n",
    "            severity_class = severity_logits.argmax(dim=1).item()\n",
    "            bbox_pred = bbox_pred.squeeze(0).cpu().numpy().tolist()\n",
    "\n",
    "            # ==== Decoding ====\n",
    "            input_token = torch.tensor([sos_token], device=device).unsqueeze(0)  # [1,1]\n",
    "            outputs = []\n",
    "\n",
    "            if self.decoder_type == \"gru\":\n",
    "                dec_state = h_t\n",
    "            elif self.decoder_type == \"lstm\":\n",
    "                c_t = torch.zeros(1, hidden_dim, device=device)\n",
    "                dec_state = (h_t, c_t)\n",
    "            elif self.decoder_type == \"transformer\":\n",
    "                # เตรียม memory หรือ context feature\n",
    "                memory = zt.unsqueeze(1)  # [1, 1, embed_dim] หรือปรับให้ compatible กับ decoder\n",
    "                tgt_tokens = [sos_token]  # Start sequence\n",
    "            else:\n",
    "                raise NotImplementedError(\"Decoder type not supported.\")\n",
    "\n",
    "            for step in range(max_len):\n",
    "                if self.decoder_type in {\"gru\", \"lstm\"}:\n",
    "                    xt = self.decoder.word_embedding(input_token).squeeze(1)\n",
    "                    rnn_input = torch.cat([xt, zt], dim=1)\n",
    "                    if self.decoder_type == \"gru\":\n",
    "                        dec_state = self.decoder.gru(rnn_input, dec_state)\n",
    "                        logits = self.decoder.fc(dec_state)[0]  # [vocab_size]\n",
    "                    else:  # lstm\n",
    "                        h, c = dec_state\n",
    "                        h, c = self.decoder.lstm(rnn_input, (h, c))\n",
    "                        dec_state = (h, c)\n",
    "                        logits = self.decoder.fc(h)[0]\n",
    "                elif self.decoder_type == \"transformer\":\n",
    "                     tgt_seq = torch.tensor([tgt_tokens], device=device).long()  # <--- สำคัญ!\n",
    "                     logits = self.decoder(zt, tgt_seq)  # [1, seq_len, vocab_size]\n",
    "                     logits = logits[:, -1, :].squeeze(0)\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "\n",
    "                # repetition_penalty\n",
    "                if outputs:\n",
    "                    logits = logits.clone()\n",
    "                    for prev_token in set(outputs):\n",
    "                        logits[prev_token] /= repetition_penalty\n",
    "\n",
    "                next_token = self.top_k_temperature_sampling(logits, top_k=top_k, temperature=temperature)\n",
    "                if next_token == eos_token:\n",
    "                    break\n",
    "                outputs.append(next_token)\n",
    "                if self.decoder_type == \"transformer\":\n",
    "                    tgt_tokens.append(next_token)\n",
    "                else:\n",
    "                    input_token = torch.tensor([[next_token]], device=logits.device)\n",
    "\n",
    "            return outputs, severity_class, bbox_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oaAXNsJWJFT"
   },
   "source": [
    "### test DKCI with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486,
     "referenced_widgets": [
      "05c13bdf08da49f899626a01728af6b7",
      "2bfa45677a2e49218c91fa489de992f6",
      "1536f3bb3731431dbfed2e7fa6dde909",
      "c0930805c0b946a49b5b1384a625b7ea",
      "fc0ab0f6101047568d78ce6f4821ab52",
      "bace1c226e254003b0f174622d89d960",
      "88440904f9e2484eb79345e7f6cf7a1e",
      "acc83590cbdf48d987152ab1c4047b2e",
      "753f9978265b45339ec483b2e7b21cbb",
      "9d4b02b3542942c0a918dffb64c29424",
      "3e81a396d86943a59a0f366357597e20"
     ]
    },
    "executionInfo": {
     "elapsed": 14961,
     "status": "ok",
     "timestamp": 1749822457595,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "p4YrvC-s3hl4",
    "outputId": "2891446b-6755-4b42-dd82-57a614c2a57c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 10000\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\",\n",
    "    decoder_type=\"gru\",\n",
    ")\n",
    "\n",
    "# Dummy input\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10))\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "# --- Assertion Check ---\n",
    "assert output.shape[0] == 2, \"Batch size ไม่ตรง\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK.\")\n",
    "\n",
    "\n",
    "\n",
    "#---- test LSTM decodeer ----#\n",
    "vocab_size = 10000\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\",\n",
    "    decoder_type=\"lstm\",  # <------ เปลี่ยนตรงนี้ ถ้าอยากลอง lstm\n",
    ")\n",
    "\n",
    "# Dummy input\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10))\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "\n",
    "# batch=2, seq_len=10\n",
    "assert output.shape == (2, 10, vocab_size), f\"Shape mismatch: {output.shape}\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK. Decoder type =\", model.decoder_type)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "#---- test Transformer decoder ----#\n",
    "vocab_size = 10000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,  # ต้องตรงกับตอน train frcnn\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\",\n",
    "    decoder_type=\"transformer\",   # <------ ใช้ transformer\n",
    ")\n",
    "\n",
    "# Dummy input (long!)\n",
    "dummy_img = torch.randn(2, 3, 224, 224).to(device)\n",
    "dummy_cap = torch.randint(0, vocab_size, (2, 10), dtype=torch.long).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Forward\n",
    "output, alpha, severity_logits, bbox_pred = model(dummy_img, dummy_cap)\n",
    "\n",
    "# batch=2, seq_len=10\n",
    "assert output.shape == (2, 10, vocab_size), f\"Shape mismatch: {output.shape}\"\n",
    "assert output.shape[1] == 10, \"Sequence length ไม่ตรง\"\n",
    "assert output.shape[2] == vocab_size, \"vocab size ไม่ตรง\"\n",
    "assert severity_logits.shape == (2, 4), \"Severity logits shape ผิด\"\n",
    "assert bbox_pred.shape == (2, 4), \"bbox_pred shape ผิด\"\n",
    "\n",
    "print(\"DKICNet ready! Forward pass OK. Decoder type =\", model.decoder_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx11Sw7Y4UOh"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "733Yyiv0pTHE"
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "# 1. สร้าง vocab จาก caption ทั้งหมด\n",
    "def build_vocab(jsonl_path, min_freq=0):\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            if obj.get('caption'):\n",
    "                tokens = word_tokenize(obj['caption'], keep_whitespace=False)\n",
    "                counter.update(tokens)\n",
    "    # สร้าง vocab: padding=0, unk=1, sos=2, eos=3, (แล้วตามด้วย word)\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}\n",
    "\n",
    "    for w, c in counter.items():\n",
    "\n",
    "        if c >= min_freq and w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class JSONLDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, vocab, max_len=2000, transform=None):\n",
    "        self.records = []\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                # ใช้ 'caption' หรือ 'caption_thai' ก็ได้\n",
    "                if obj.get('caption') or obj.get('caption_thai'):\n",
    "                    self.records.append(obj)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        tokens = word_tokenize(caption, keep_whitespace=False)\n",
    "        ids = [self.vocab.get('<sos>', 2)]\n",
    "        for w in tokens:\n",
    "            ids.append(self.vocab.get(w, self.vocab['<unk>']))\n",
    "        ids.append(self.vocab.get('<eos>', 3))\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab['<pad>']] * (self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        img = Image.open(rec['image']).convert('RGB')\n",
    "        img_t = self.transform(img)\n",
    "        caption_ids = self.encode_caption(rec['caption'])\n",
    "        severity = torch.tensor(rec.get('severity', 0), dtype=torch.long)\n",
    "        bndbox = rec.get(\"bndbox\", None)\n",
    "        if bndbox is not None:\n",
    "            bndbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "            bndbox = torch.tensor(bndbox, dtype=torch.float)\n",
    "        else:\n",
    "            bndbox = torch.zeros(4, dtype=torch.float)\n",
    "        return img_t, caption_ids, severity, bndbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11pj8ebG4WfN"
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_zNYESxt4dp"
   },
   "outputs": [],
   "source": [
    "# 3. สร้าง vocab & dataset & dataloader\n",
    "jsonl_path = \"/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/train.bbox224-2.jsonl\"\n",
    "vocab = build_vocab(jsonl_path)\n",
    "\n",
    "dataset = JSONLDataset(jsonl_path, vocab, max_len=100)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=12,pin_memory=True)\n",
    "\n",
    "# load val data set\n",
    "val_jsonl_path = \"/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/val.bbox224-2.jsonl\"\n",
    "\n",
    "val_dataset = JSONLDataset(val_jsonl_path, vocab, max_len=100)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=12,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYUcVYYJz3SF"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JS-HnMhT--O"
   },
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piANEZwST9cV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# NLP metric import...\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# ---- Bounding box metric ----\n",
    "def box_iou(box1, box2):\n",
    "    # box1, box2 : array หรือ tensor shape [4] = (x1, y1, x2, y2)\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def bbox_metrics(all_pred_bbox, all_gt_bbox):\n",
    "    ious = []\n",
    "    ap_05s = []\n",
    "    N = all_pred_bbox.size(0)\n",
    "    for i in range(N):\n",
    "        pred = all_pred_bbox[i].cpu().numpy()\n",
    "        gt = all_gt_bbox[i].cpu().numpy()\n",
    "        iou = box_iou(pred, gt)     # ของคุณเอง\n",
    "        ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "        ious.append(iou)\n",
    "        ap_05s.append(ap_05)\n",
    "    return np.mean(ious), np.mean(ap_05s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "    tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w == '<eos>' and i < min_length: continue\n",
    "        if w == '<eos>': break\n",
    "        if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "    return clean\n",
    "def compute_bleu(preds, refs, inv_vocab_dict):\n",
    "    scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = decode_caption(pred, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(ref, inv_vocab_dict)\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "        scores.append(score)\n",
    "    return np.mean(scores) if scores else 0\n",
    "\n",
    "def compute_rouge(preds, refs, inv_vocab_dict):\n",
    "    r1, r2, rL = [], [], []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = \" \".join(decode_caption(pred, inv_vocab_dict))\n",
    "        ref_tokens  = \" \".join(decode_caption(ref, inv_vocab_dict))\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        scores = rouge.score(ref_tokens, pred_tokens)\n",
    "        r1.append(scores['rouge1'].fmeasure)\n",
    "        r2.append(scores['rouge2'].fmeasure)\n",
    "        rL.append(scores['rougeL'].fmeasure)\n",
    "    return (np.mean(r1) if r1 else 0, np.mean(r2) if r2 else 0, np.mean(rL) if rL else 0)\n",
    "\n",
    "def compute_meteor(preds, refs, inv_vocab_dict):\n",
    "    meteor_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = decode_caption(pred, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(ref, inv_vocab_dict)\n",
    "        if len(pred_tokens) == 0 or len(ref_tokens) == 0: continue\n",
    "        meteor_scores.append(meteor_score([ref_tokens], pred_tokens))\n",
    "    return np.mean(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "# ---- Main: Validate Auto-Regressive ----\n",
    "def validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50):\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            for i in range(batch_size):\n",
    "                pred_ids, _, _ =  model.generate (\n",
    "              images[i], max_len=50, device=device, top_k=1, temperature=1,\n",
    "          )\n",
    "                ref_ids = captions[i].cpu().tolist()\n",
    "                all_preds.append(pred_ids)\n",
    "                all_refs.append(ref_ids)\n",
    "    return all_preds, all_refs\n",
    "\n",
    "def get_tf_prob(epoch):\n",
    "    if epoch >= ss_epochs:\n",
    "        return end_prob\n",
    "    return start_prob - (start_prob - end_prob) * (epoch / ss_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOKYEBpdIh0f"
   },
   "source": [
    "## Train with NLP every 10 loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 6862,
     "status": "error",
     "timestamp": 1749823637485,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "Nugl5XO1olA8",
    "outputId": "a168c7d4-d806-4da7-ca28-aa83451b145f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random, numpy as np, csv, os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ---- FIX RANDOM SEED ----\n",
    "seed = 314\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ---- CONFIG ----\n",
    "batch_size = 256         # ปรับลดเพื่อให้ train/stable ขึ้น\n",
    "num_workers = 12\n",
    "num_epochs = 300\n",
    "patience = 30\n",
    "pad_token = vocab['<pad>']\n",
    "dropout = 0.5\n",
    "encoder_lr = 0.000128\n",
    "decoder_lr =  0.001905\n",
    "\n",
    "# ---- HYPERPARAM LOSS WEIGHT ----\n",
    "alpha = 2.42   # caption\n",
    "beta  = 0.98  # severity\n",
    "gamma = 0.51  # bbox\n",
    "\n",
    "# scheduled sampling\n",
    "start_prob = 1.0\n",
    "end_prob = 0.0\n",
    "ss_epochs = 10\n",
    "#--- decoder -----#\n",
    "decoder_type=\"lstm\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------  PARAMETER  -------#\n",
    "\n",
    "# label smoothing\n",
    "criterion_caption = nn.CrossEntropyLoss(ignore_index=pad_token, label_smoothing=0.1)\n",
    "criterion_severity = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment-log/bestmodel_{now}.pth\"\n",
    "csv_path  = f\"/content/drive/MyDrive/Final_Deep_project/experiment-log/metrics_log_{now}.csv\"\n",
    "param_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment-log/params_{now}.txt\"\n",
    "\n",
    "print(f\"Save model path: {save_path}\")\n",
    "print(f\"Metrics log path: {csv_path}\")\n",
    "\n",
    "with open(param_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"dropout: {dropout}\\n\")\n",
    "    f.write(f\"encoder_lr: {encoder_lr}\\n\")\n",
    "    f.write(f\"decoder_lr: {decoder_lr}\\n\")\n",
    "    f.write(f\"patience: {patience}\\n\")\n",
    "    f.write(f\"batch_size: {batch_size}\\n\")\n",
    "    f.write(f\"num_workers: {num_workers}\\n\")\n",
    "    f.write(f\"seed: {seed}\\n\")\n",
    "    f.write(f\"timestamp: {now}\\n\")\n",
    "\n",
    "inv_vocab_dict = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "\n",
    "# ---- SETUP DEVICE ----\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "except ImportError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "\n",
    "model = DKICNet(\n",
    "    vocab_size=len(vocab),\n",
    "    severity_classes=4,\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    cnn_pretrained=False,\n",
    "    num_classes=4,\n",
    "    cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "    frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "    vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\",\n",
    "    decoder_type=decoder_type\n",
    ").to(device)\n",
    "\n",
    "encoder_params = list(model.cnn_encoder.parameters())\n",
    "frcnn_params  = list(model.frcnn_encoder.parameters())\n",
    "other_params  = [p for n, p in model.named_parameters()\n",
    "                 if not (n.startswith('cnn_encoder') or n.startswith('frcnn_encoder')) and p.requires_grad]\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.cnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.frcnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.fusion.parameters(), 'lr': encoder_lr},\n",
    "    {'params': model.attn.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.decoder.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.severity_head.parameters(), 'lr': decoder_lr},\n",
    "    {'params': model.bbox_head.parameters(), 'lr': decoder_lr},\n",
    "])\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "acc_scores, iou_scores, ap05_scores = [], [], []\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'severity_acc', 'iou', 'ap@0.5'])\n",
    "\n",
    "# ---- TRAINING LOOP ----\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    tf_prob = get_tf_prob(epoch)\n",
    "    print(f\"\\n[Epoch {epoch+1}] Teacher forcing prob: {tf_prob:.2f}\")\n",
    "    all_sev_preds, all_sev_true = [], []\n",
    "    all_pred_bbox, all_gt_bbox = [], []\n",
    "    for images, captions, severity, bbox_targets, *_ in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        severity = severity.to(device)\n",
    "        bbox_targets = bbox_targets.to(device)\n",
    "        _, _, H, W = images.shape\n",
    "        bbox_targets_norm = bbox_targets.clone()\n",
    "        bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "        bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size, seq_len = captions.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, model.decoder.fc.out_features, device=device)\n",
    "        input_token = captions[:, 0]  # <sos>\n",
    "\n",
    "        # --- feature extraction ---\n",
    "        Fc = model.cnn_encoder(images)           # (B, 2048, H, W)\n",
    "        Ff = model.frcnn_encoder(images)         # (B, 256,  H, W)\n",
    "        Fv = model.vit_encoder(images)           # (B, 768, 1, 1)\n",
    "        Ffinal = model.fusion(Fc, Ff, Fv)        # (B, 3*out_c, H, W)\n",
    "        Fc_adj = model.fusion.adj_conv_cnn(Fc)   # (B, out_c, H, W)\n",
    "        Ff_adj = model.fusion.adj_conv_frcnn(Ff) # (B, out_c, H, W)\n",
    "        Fv_adj = model.fusion.adj_conv_vit(Fv)   # (B, out_c, 1, 1)\n",
    "        # ขยาย spatial vit ให้เท่า Fc_adj\n",
    "        Fv_adj = F.interpolate(Fv_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # --- attention ---\n",
    "        zt, _ = model.attn(\n",
    "            Ffinal,\n",
    "            Fc_adj,\n",
    "            Ff_adj,\n",
    "            Fv_adj,\n",
    "            torch.zeros(batch_size, model.attn.W_h.in_features, device=device)\n",
    "        )\n",
    "\n",
    "        print(f\"@@@@ {hasattr(model.decoder)}\")\n",
    "        print(f'$$$$$$$ {hasattr(model.decoder, \"transformer\")}')\n",
    "\n",
    "\n",
    "            # ==== แยกกรณี decoder ====\n",
    "        if hasattr(model.decoder, \"gru\"):  # GRU\n",
    "            h_t = torch.zeros(batch_size, model.decoder.gru.hidden_size, device=device)\n",
    "            for t in range(1, seq_len):\n",
    "                xt = model.decoder.word_embedding(input_token)\n",
    "                gru_input = torch.cat([xt, zt], dim=1)\n",
    "                h_t = model.decoder.gru(gru_input, h_t)\n",
    "                logits = model.decoder.fc(h_t)\n",
    "                outputs[:, t] = logits\n",
    "                use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                next_token_tf = captions[:, t]\n",
    "                next_token_model = logits.argmax(-1)\n",
    "                input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "        elif hasattr(model.decoder, \"lstm\"):  # LSTM\n",
    "            h_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "            c_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "            for t in range(1, seq_len):\n",
    "                xt = model.decoder.word_embedding(input_token)\n",
    "                lstm_input = torch.cat([xt, zt], dim=1)\n",
    "                h_t, c_t = model.decoder.lstm(lstm_input, (h_t, c_t))\n",
    "                logits = model.decoder.fc(h_t)\n",
    "                outputs[:, t] = logits\n",
    "\n",
    "                # scheduled sampling / teacher forcing\n",
    "                use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                next_token_tf = captions[:, t]\n",
    "                next_token_model = logits.argmax(-1)\n",
    "                input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "\n",
    "        elif  hasattr(model.decoder, \"transformer\"):  # Transformer\n",
    "              tgt = captions[:, :-1]\n",
    "              seq_len = tgt.size(1)\n",
    "              tgt_embed = model.decoder.word_embedding(tgt)  # [B, seq_len-1, embed_dim]\n",
    "              tgt_embed = tgt_embed.transpose(0, 1)          # [seq_len-1, B, embed_dim]\n",
    "\n",
    "              # ให้ memory repeat สำหรับทุกตำแหน่งใน seq\n",
    "              memory = zt.unsqueeze(0).repeat(seq_len, 1, 1)  # [seq_len-1, B, embed_dim]\n",
    "\n",
    "              out = model.decoder.transformer_decoder(tgt_embed, memory)  # [seq_len-1, B, embed_dim]\n",
    "              out = out.transpose(0, 1)  # [B, seq_len-1, embed_dim]\n",
    "              logits = model.decoder.fc(out)  # [B, seq_len-1, vocab_size]\n",
    "              outputs[:, 1:] = logits\n",
    "        else:\n",
    "            raise NotImplementedError(\"รองรับแต่ GRU หรือ TransformerDecoderCorrAttn\")\n",
    "\n",
    "        # Severity/Bbox output\n",
    "        severity_logits = model.severity_head(Ffinal)\n",
    "        bbox_pred = model.bbox_head(Ffinal)\n",
    "        # Target reshape\n",
    "        outputs_ = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
    "        targets_ = captions[:, 1:].reshape(-1)\n",
    "\n",
    "        loss_caption = criterion_caption(outputs_, targets_)\n",
    "        loss_severity = criterion_severity(severity_logits, severity)\n",
    "        loss_bbox = criterion_bbox(bbox_pred, bbox_targets_norm)\n",
    "        loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # เก็บ metric\n",
    "        all_pred_bbox.append(bbox_pred.detach().cpu())\n",
    "        all_gt_bbox.append(bbox_targets_norm.detach().cpu())\n",
    "        all_sev_preds.extend(severity_logits.argmax(-1).cpu().tolist())\n",
    "        all_sev_true.extend(severity.cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | train_loss: {avg_loss:.4f} | loss_caption: {loss_caption.item():.3f} | loss_severity: {loss_severity.item():.3f} | loss_bbox: {loss_bbox.item():.3f}\")\n",
    "    train_losses.append(avg_loss)\n",
    "    # Eval acc, IOU/AP@0.5\n",
    "    all_pred_bbox = torch.cat(all_pred_bbox, dim=0)\n",
    "    all_gt_bbox = torch.cat(all_gt_bbox, dim=0)\n",
    "    mean_iou, ap_05 = bbox_metrics(all_pred_bbox, all_gt_bbox)\n",
    "    acc = (np.array(all_sev_preds) == np.array(all_sev_true)).mean()\n",
    "    acc_scores.append(acc)\n",
    "    iou_scores.append(mean_iou)\n",
    "    ap05_scores.append(ap_05)\n",
    "    # VALIDATION LOSS ONLY\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        all_val_sev_preds, all_val_sev_true = [], []\n",
    "        all_val_pred_bbox, all_val_gt_bbox = [], []\n",
    "        for images, captions, severity, bbox_targets, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            severity = severity.to(device)\n",
    "            bbox_targets = bbox_targets.to(device)\n",
    "            _, _, H, W = images.shape\n",
    "            bbox_targets_norm = bbox_targets.clone()\n",
    "            bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "            bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "            outputs, _, severity_logits, bbox_preds = model(images, captions)\n",
    "            outputs_ = outputs.view(-1, outputs.size(-1))\n",
    "            targets_ = captions.view(-1)\n",
    "            loss_caption = criterion_caption(outputs_, targets_)\n",
    "            loss_severity = criterion_severity(severity_logits, severity)\n",
    "            loss_bbox = criterion_bbox(bbox_preds, bbox_targets_norm)\n",
    "            loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "            val_loss += loss.item()\n",
    "            all_val_sev_preds.extend(severity_logits.argmax(-1).cpu().tolist())\n",
    "            all_val_sev_true.extend(severity.cpu().tolist())\n",
    "            all_val_pred_bbox.append(bbox_preds.detach().cpu())\n",
    "            all_val_gt_bbox.append(bbox_targets_norm.detach().cpu())\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_iou, val_ap05 = bbox_metrics(torch.cat(all_val_pred_bbox,0), torch.cat(all_val_gt_bbox,0))\n",
    "        val_acc = (np.array(all_val_sev_preds) == np.array(all_val_sev_true)).mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss: {avg_loss:.4f} | val_loss: {avg_val_loss:.4f} | SeverityAcc: {val_acc:.4f} | IOU: {val_iou:.4f} | AP@0.5: {val_ap05:.4f}\")\n",
    "\n",
    "    # ---- SAVE MODEL + EARLY STOPPING ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Saved best model at epoch {epoch+1}: {save_path}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f'No improvement for {epochs_no_improve} epochs.')\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val_loss={best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # ---- LOG METRICS TO CSV ----\n",
    "    with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch+1, avg_loss, avg_val_loss, val_acc, val_iou, ap_05])\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or (epoch+1) == num_epochs:\n",
    "        print(\"\\n=== VALIDATION (Auto-regressive Generate) ===\")\n",
    "        all_preds, all_refs = validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50)\n",
    "        bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "        rouge1, rouge2, rougeL = compute_rouge(all_preds, all_refs, inv_vocab_dict)\n",
    "        meteor = compute_meteor(all_preds, all_refs, inv_vocab_dict)\n",
    "        print(f\"BLEU:   {bleu:.4f}\")\n",
    "        print(f\"ROUGE-1 {rouge1:.4f} | ROUGE-2 {rouge2:.4f} | ROUGE-L {rougeL:.4f}\")\n",
    "        print(f\"METEOR: {meteor:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"\\n=== FINAL NLP METRIC ON VAL SET ===\")\n",
    "model.eval()\n",
    "all_preds, all_refs = [], []\n",
    "with torch.no_grad():\n",
    "    for images, captions, *_ in val_loader:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        outputs, _, _, _ = model(images, captions)\n",
    "        preds = outputs.argmax(-1).cpu().tolist()\n",
    "        refs = captions.cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_refs.extend(refs)\n",
    "bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "rouge1, rouge2, rougeL = compute_rouge(all_preds, all_refs, inv_vocab_dict)\n",
    "meteor = compute_meteor(all_preds, all_refs, inv_vocab_dict)\n",
    "print(f\"BLEU:   {bleu:.4f}\")\n",
    "print(f\"ROUGE-1 {rouge1:.4f} | ROUGE-2 {rouge2:.4f} | ROUGE-L {rougeL:.4f}\")\n",
    "print(f\"METEOR: {meteor:.4f}\")\n",
    "\n",
    "# ---- PLOT TRAINING LOSS----\n",
    "metrics = pd.read_csv(csv_path)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.show()\n",
    "plt.plot(acc_scores, label='Severity Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.show()\n",
    "plt.plot(iou_scores, label='Mean IOU')\n",
    "plt.plot(ap05_scores, label='AP@0.5')\n",
    "plt.xlabel('Epoch'); plt.ylabel('BBox Metric'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFF9ODrMZKsd"
   },
   "source": [
    " ## save latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmxiU9go_eg4"
   },
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment-log/bestmodel_{now}-latest.pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pseLYyqrVY_T"
   },
   "source": [
    "## optuna parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10361504,
     "status": "ok",
     "timestamp": 1749810210672,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "s_ubyPT_VDcj",
    "outputId": "0cbc5180-8ab9-4ef2-f177-031cab276267"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "# ---- SETUP DEVICE ----\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "except ImportError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "def validate_auto_regressive(model, val_loader, inv_vocab_dict, device, max_len=50):\n",
    "    model.eval()\n",
    "    all_preds, all_refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, captions, *_ in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            for b in range(batch_size):\n",
    "                img = images[b]\n",
    "                ref = captions[b].cpu().tolist()\n",
    "                # ---- auto-regressive decode ----\n",
    "                pred_ids, _, _ = model.generate(img, max_len=max_len, device=device)\n",
    "                all_preds.append(pred_ids)\n",
    "                all_refs.append(ref)\n",
    "    return all_preds, all_refs\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # ---- Hyperparameter ----\n",
    "    encoder_lr = trial.suggest_loguniform(\"encoder_lr\", 1e-5, 5e-4)\n",
    "    decoder_lr = trial.suggest_loguniform(\"decoder_lr\", 5e-5, 2e-3)\n",
    "    alpha = trial.suggest_float(\"alpha\", 1.0, 6.0)   # caption loss weight\n",
    "    beta  = trial.suggest_float(\"beta\", 0.5, 2.0)    # severity loss weight\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.5, 2.0)   # bbox loss weight\n",
    "\n",
    "    # ===== Re-init Model & Optimizer =====\n",
    "    model = DKICNet(\n",
    "        vocab_size=len(vocab),\n",
    "        severity_classes=4,\n",
    "        cnn_backbone=\"resnet50\",\n",
    "        cnn_pretrained=False,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=4,\n",
    "        cnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\",\n",
    "        frcnn_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\",\n",
    "        vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\",\n",
    "        decoder_type=\"transformer\"\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.cnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.frcnn_encoder.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.fusion.parameters(), 'lr': encoder_lr},\n",
    "        {'params': model.attn.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.decoder.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.severity_head.parameters(), 'lr': decoder_lr},\n",
    "        {'params': model.bbox_head.parameters(), 'lr': decoder_lr},\n",
    "    ])\n",
    "    # Loss\n",
    "    criterion_caption = nn.CrossEntropyLoss(ignore_index=pad_token, label_smoothing=0.1)\n",
    "    criterion_severity = nn.CrossEntropyLoss()\n",
    "    criterion_bbox = nn.SmoothL1Loss()\n",
    "\n",
    "    best_bleu = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # ==== Training Loop (ย่อ) ====\n",
    "    for epoch in range(8):   # สั้นๆ, 5-10 epoch ก็พอสำหรับ tuning\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "\n",
    "        for images, captions, severity, bbox_targets, *_ in train_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            severity = severity.to(device)\n",
    "            bbox_targets = bbox_targets.to(device)\n",
    "            _, _, H, W = images.shape\n",
    "            bbox_targets_norm = bbox_targets.clone()\n",
    "            bbox_targets_norm[:, [0, 2]] = bbox_targets[:, [0, 2]] / W\n",
    "            bbox_targets_norm[:, [1, 3]] = bbox_targets[:, [1, 3]] / H\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size, seq_len = captions.size()\n",
    "            outputs = torch.zeros(batch_size, seq_len, model.decoder.fc.out_features, device=device)\n",
    "            input_token = captions[:, 0]\n",
    "\n",
    "            # ==== เพิ่ม Vit ====\n",
    "            Fc = model.cnn_encoder(images)\n",
    "            Ff = model.frcnn_encoder(images)\n",
    "            Fv = model.vit_encoder(images)\n",
    "            Ffinal = model.fusion(Fc, Ff, Fv)\n",
    "            Fc_adj = model.fusion.adj_conv_cnn(Fc)\n",
    "            Ff_adj = model.fusion.adj_conv_frcnn(Ff)\n",
    "            Fv_adj = model.fusion.adj_conv_vit(Fv)\n",
    "            Fv_adj = F.interpolate(Fv_adj, size=Fc_adj.shape[2:], mode='bilinear', align_corners=False)\n",
    "            zt, _ = model.attn(Ffinal, Fc_adj, Ff_adj, Fv_adj, torch.zeros(batch_size, model.attn.W_h.in_features, device=device))\n",
    "            # =====\n",
    "\n",
    "                # ---- Flexible Decoder ----\n",
    "            if model.decoder_type == \"gru\":\n",
    "                h_t = torch.zeros(batch_size, model.decoder.gru.hidden_size, device=device)\n",
    "                for t in range(1, seq_len):\n",
    "                    xt = model.decoder.word_embedding(input_token)\n",
    "                    gru_input = torch.cat([xt, zt], dim=1)\n",
    "                    h_t = model.decoder.gru(gru_input, h_t)\n",
    "                    logits = model.decoder.fc(h_t)\n",
    "                    outputs[:, t] = logits\n",
    "                    # Scheduled sampling\n",
    "                    use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                    next_token_tf = captions[:, t]\n",
    "                    next_token_model = logits.argmax(-1)\n",
    "                    input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "\n",
    "            elif model.decoder_type == \"lstm\":\n",
    "                h_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "                c_t = torch.zeros(batch_size, model.decoder.lstm.hidden_size, device=device)\n",
    "                for t in range(1, seq_len):\n",
    "                    xt = model.decoder.word_embedding(input_token)\n",
    "                    lstm_input = torch.cat([xt, zt], dim=1)\n",
    "                    h_t, c_t = model.decoder.lstm(lstm_input, (h_t, c_t))\n",
    "                    logits = model.decoder.fc(h_t)\n",
    "                    outputs[:, t] = logits\n",
    "                    use_tf = (torch.rand(batch_size, device=device) < tf_prob)\n",
    "                    next_token_tf = captions[:, t]\n",
    "                    next_token_model = logits.argmax(-1)\n",
    "                    input_token = torch.where(use_tf, next_token_tf, next_token_model)\n",
    "\n",
    "            elif hasattr(model.decoder, \"transformer_decoder\"):  # Transformer\n",
    "                tgt = captions[:, :-1]      # teacher-forcing\n",
    "                tgt_embed = model.decoder.word_embedding(tgt)   # [B, L-1, emb]\n",
    "                tgt_embed = model.decoder.pos_encoder(tgt_embed)\n",
    "                # memory shape [B, 1, emb] → repeat to [B, L-1, emb]\n",
    "                memory = zt.unsqueeze(1).repeat(1, tgt_embed.size(1), 1)\n",
    "                out = model.decoder.transformer_decoder(\n",
    "                    tgt_embed, memory\n",
    "                )  # [B, L-1, emb]\n",
    "                logits = model.decoder.fc(out)  # [B, L-1, vocab]\n",
    "                outputs[:, 1:] = logits  # shift 1 step\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(\"รองรับแต่ GRU / LSTM / TransformerDecoderCorrAttn\")\n",
    "\n",
    "\n",
    "            severity_logits = model.severity_head(Ffinal)\n",
    "            bbox_pred = model.bbox_head(Ffinal)\n",
    "            outputs_ = outputs[:, 1:].reshape(-1, outputs.size(-1))\n",
    "            targets_ = captions[:, 1:].reshape(-1)\n",
    "\n",
    "            loss_caption = criterion_caption(outputs_, targets_)\n",
    "            loss_severity = criterion_severity(severity_logits, severity)\n",
    "            loss_bbox = criterion_bbox(bbox_pred, bbox_targets_norm)\n",
    "            loss = alpha * loss_caption + beta * loss_severity + gamma * loss_bbox\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        # ---- Validation NLP Metric ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds, all_refs = [], []\n",
    "            for images, captions, *_ in val_loader:\n",
    "                images = images.to(device)\n",
    "                captions = captions.to(device)\n",
    "                batch_size = images.size(0)\n",
    "                for b in range(batch_size):\n",
    "                    img = images[b]\n",
    "                    ref = captions[b].cpu().tolist()\n",
    "                    # === ใช้ generate ทีละ sample ===\n",
    "                    pred_ids, _, _ = model.generate(img, max_len=50, device=device)\n",
    "                    all_preds.append(pred_ids)\n",
    "                    all_refs.append(ref)\n",
    "\n",
    "            bleu = compute_bleu(all_preds, all_refs, inv_vocab_dict)\n",
    "            if bleu > best_bleu:\n",
    "                best_bleu = bleu\n",
    "                best_epoch = epoch\n",
    "\n",
    "\n",
    "    print(f\"Trial {trial.number} | best BLEU: {best_bleu:.4f} | alpha: {alpha:.2f} beta: {beta:.2f} gamma: {gamma:.2f} encoder_lr: {encoder_lr:.6f} decoder_lr: {decoder_lr:.6f}\")\n",
    "    return best_bleu\n",
    "\n",
    "# ---- Optuna Study ----\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)   # 30-50 รอบกำลังดี\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"BLEU: {trial.value:.4f}\")\n",
    "for k, v in trial.params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# ---- นำค่าที่ได้ไปใช้เทรนจริง ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaASnX75UNlE"
   },
   "source": [
    "## clear GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS7QAWgQHNGO"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # 1. ลบตัวแปร model/optimizer/dataloader ที่ไม่ใช้แล้ว (optional)\n",
    "# del model\n",
    "# del optimizer\n",
    "# # del train_loader, val_loader, test_loader  # ถ้าจะลบออกหมด\n",
    "\n",
    "# # 2. เคลียร์ Python garbage\n",
    "# gc.collect()\n",
    "\n",
    "# # 3. เคลียร์ CUDA memory\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()  # สำหรับบางกรณีที่ memory fragment\n",
    "\n",
    "# print(\"CUDA memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1VTovM8FiRv"
   },
   "source": [
    "# Test auto regressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fiPuUKYL4P5"
   },
   "source": [
    "## test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wIIdqIwwOwG"
   },
   "outputs": [],
   "source": [
    "# test_jsonl_path = '/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/test.filtered.jsonl'\n",
    "test_jsonl_path= '/content/drive/MyDrive/Final_Deep_project/final_data/dkic-net/test.bbox224-2.jsonl'\n",
    "test_dataset = JSONLDataset(test_jsonl_path, vocab, max_len=70)\n",
    "caption_ids = test_dataset[0][1]\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=70, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W_Eo-3GY5J2"
   },
   "source": [
    "### find latest best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1749782847266,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "O4xUsDbKWSVd",
    "outputId": "549c8d94-5bae-4105-8a8b-55e720806bce"
   },
   "outputs": [],
   "source": [
    "ls -t /content/drive/MyDrive/Final_Deep_project/experiment-log/*.pth | head -n 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM8-NnlkFnvh"
   },
   "source": [
    "## Test Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9tPBD9iE_w8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "def get_last_conv_layer(encoder):\n",
    "    # สำหรับ CNNEncoder ที่ใช้ self.layer4 เป็น conv block สุดท้าย\n",
    "    return encoder.layer4\n",
    "\n",
    "def generate_gradcam(encoder, input_tensor, class_idx=None):\n",
    "    # encoder: CNNEncoder (เช่น model.cnn_encoder)\n",
    "    encoder.eval()\n",
    "    feature_maps = []\n",
    "    gradients = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        feature_maps.append(output)\n",
    "\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0])\n",
    "\n",
    "    last_conv = get_last_conv_layer(encoder)\n",
    "    f = last_conv.register_forward_hook(forward_hook)\n",
    "    b = last_conv.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = encoder(input_tensor)\n",
    "    if class_idx is None:\n",
    "        score = output.sum()\n",
    "    else:\n",
    "        score = output[:, class_idx].sum()\n",
    "    encoder.zero_grad()\n",
    "    score.backward(retain_graph=True)\n",
    "\n",
    "    grads_val = gradients[0].detach().cpu().numpy()[0]\n",
    "    fmap = feature_maps[0].detach().cpu().numpy()[0]\n",
    "    weights = np.mean(grads_val, axis=(1, 2))\n",
    "    cam = np.zeros(fmap.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * fmap[i, :, :]\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "    return cam\n",
    "\n",
    "def visualize_result(image_path, pred_caption, gt_caption, sev_pred, sev_gt, cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.5):\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    # ======= วาดกรอบ bounding box =======\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):\n",
    "            if box_scores is not None and box_scores[i] < box_threshold:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_str = \"\"\n",
    "            if box_labels is not None:\n",
    "                label_str += f\"{box_labels[i]}\"\n",
    "            if box_scores is not None:\n",
    "                label_str += f\" ({box_scores[i]:.2f})\"\n",
    "            if label_str:\n",
    "                ax.text(x1, y1, label_str, color='white', fontsize=10, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    # ======= Grad-CAM =======\n",
    "    if cam is not None:\n",
    "        plt.subplot(1,2,2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam*255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_last_selfattention(vit_model, img_tensor):\n",
    "    # Forward pass เพื่อให้มี attn map\n",
    "    outputs = vit_model.vit.forward_features(img_tensor)\n",
    "    # ไปหยิบ attn จาก block สุดท้าย\n",
    "    attn = vit_model.vit.blocks[-1].attn.get_attn()\n",
    "    return attn  # shape: [B, num_heads, num_tokens, num_tokens]\n",
    "\n",
    "def show_vit_attention_map(img, vit_model, img_tensor, patch_size=16, head=0):\n",
    "    \"\"\"\n",
    "    img: original PIL image\n",
    "    vit_model: ViTEncoder (ที่ใช้ timm)\n",
    "    img_tensor: [1,3,H,W] preprocessed for vit\n",
    "    patch_size: (default 16)\n",
    "    head: index of attention head (default 0)\n",
    "    \"\"\"\n",
    "    # 1. get attention weights [B, num_heads, num_tokens, num_tokens]\n",
    "    attn = get_last_selfattention(vit_model, img_tensor)[0]  # [num_heads, num_tokens, num_tokens]\n",
    "    nh = attn.shape[0]\n",
    "    # เราใช้เฉพาะ cls token (index 0) => attn[:, 0, 1:] (skip cls)\n",
    "    attn_map = attn[head, 0, 1:].reshape(int(img_tensor.shape[-1] / patch_size), -1)  # [n_patches, n_patches]\n",
    "    attn_map = attn_map.detach().cpu().numpy()\n",
    "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min() + 1e-6)\n",
    "\n",
    "    # Resize map to image size\n",
    "    attn_map = np.kron(attn_map, np.ones((patch_size, patch_size)))  # expand\n",
    "    attn_map = np.clip(attn_map, 0, 1)\n",
    "    attn_map = Image.fromarray(np.uint8(attn_map*255)).resize(img.size, resample=Image.BILINEAR)\n",
    "    attn_map = np.asarray(attn_map) / 255\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(attn_map, cmap='jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'ViT Attention Head {head}')\n",
    "    plt.show()\n",
    "    return attn_map\n",
    "\n",
    "def plot_vit_attention(model, img_tensor, image_display=None, patch_size=16):\n",
    "    vit = model.vit_encoder.vit  # timm model\n",
    "    vit.eval()\n",
    "    attns = []\n",
    "\n",
    "    # --- timm ViT ไม่มี default output attn, ต้อง hook ที่ attn module ---\n",
    "    def hook(module, input, output):\n",
    "        # output = (x) = (batch, num_patches+1, dim)\n",
    "        # module = Attention\n",
    "        attn_weights = module.get_attn()\n",
    "        attns.append(attn_weights.detach().cpu())\n",
    "    # -- ค้นหา attn module บล็อคสุดท้าย --\n",
    "    attn_mod = vit.blocks[-1].attn\n",
    "    # -- เพิ่ม get_attn() ให้ attn_mod --\n",
    "    def get_attn(self):\n",
    "        return self.attn_weights\n",
    "    import types\n",
    "    attn_mod.get_attn = types.MethodType(get_attn, attn_mod)\n",
    "    # -- hook ก่อน forward --\n",
    "    handle = attn_mod.register_forward_hook(hook)\n",
    "\n",
    "    # -- patch: บันทึก attn_weights ใน self.attn_weights ก่อน forward --\n",
    "    def forward_with_save_attn(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        self.attn_weights = attn  # <-- save\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    attn_mod.forward = types.MethodType(forward_with_save_attn, attn_mod)\n",
    "\n",
    "    _ = vit(img_tensor.to(next(vit.parameters()).device))\n",
    "    handle.remove()\n",
    "    attn_map = attns[0]  # (B, heads, tokens, tokens)\n",
    "    attn_map = attn_map.mean(1).squeeze(0)  # (tokens, tokens)\n",
    "    cls_attn = attn_map[0, 1:]              # (num_patches,)\n",
    "    num_patches = int(np.sqrt(cls_attn.shape[0]))\n",
    "    attn_2d = cls_attn.reshape(num_patches, num_patches).numpy()\n",
    "    attn_2d = attn_2d / attn_2d.max()\n",
    "\n",
    "    # --- plot ---\n",
    "    if image_display is None:\n",
    "        img = tensor_to_pil(img_tensor)\n",
    "    elif isinstance(image_display, str):\n",
    "        img = Image.open(image_display).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image_display)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(img)\n",
    "    attn_2d_resized = np.array(Image.fromarray(np.uint8(attn_2d*255)).resize(img.size, Image.BILINEAR)) / 255.\n",
    "    plt.imshow(attn_2d_resized, alpha=0.4, cmap='jet')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"ViT Attention Map\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_bbox_gt_pred(img, gt_box, pred_box, pred_score=None, threshold=0.16):\n",
    "    # --- Convert ภาพ & แสดงขนาดภาพ ---\n",
    "    if isinstance(img, str):\n",
    "        img = Image.open(img).convert('RGB')\n",
    "    elif torch.is_tensor(img):\n",
    "        img = tensor_to_pil(img)\n",
    "    elif isinstance(img, np.ndarray):\n",
    "        img = Image.fromarray(img)\n",
    "    # Print ขนาดภาพ (PIL: size = (W, H))\n",
    "    print(\"== plot_bbox_gt_pred ==\")\n",
    "    print(f\"Image size (PIL): {img.size}\")        # (width, height)\n",
    "    if hasattr(img, 'shape'):\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # -- Debug print พิกัด --\n",
    "    print(f\"GT box: {gt_box}\")\n",
    "    print(f\"Pred box: {pred_box}\")\n",
    "    if pred_score is not None:\n",
    "        print(f\"Pred score: {pred_score:.3f}\")\n",
    "\n",
    "    # -- GT Box: วาดแค่เมื่อพิกัด valid --\n",
    "    gt_valid = not (gt_box is None or np.allclose(gt_box, 0) or np.isnan(gt_box).any())\n",
    "    if gt_valid:\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        print(f\"GT box drawn at: ({gt_x1:.1f},{gt_y1:.1f},{gt_x2:.1f},{gt_y2:.1f})\")\n",
    "        rect_gt = plt.Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,\n",
    "                                linewidth=2, edgecolor='blue', facecolor='none', label='GT')\n",
    "        ax.add_patch(rect_gt)\n",
    "    else:\n",
    "        print(\"** WARNING: GT box is missing/invalid (จะไม่วาดกรอบน้ำเงิน) **\")\n",
    "\n",
    "    # -- Pred Box --\n",
    "    pred_valid = not (pred_box is None or np.allclose(pred_box, 0) or np.isnan(pred_box).any())\n",
    "    if pred_valid and ((pred_score is None) or (pred_score > threshold)):\n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        print(f\"Pred box drawn at: ({pred_x1:.1f},{pred_y1:.1f},{pred_x2:.1f},{pred_y2:.1f})\")\n",
    "        rect_pred = plt.Rectangle((pred_x1, pred_y1), pred_x2-pred_x1, pred_y2-pred_y1,\n",
    "                                  linewidth=2, edgecolor='red', facecolor='none', label='Pred')\n",
    "        ax.add_patch(rect_pred)\n",
    "    elif not pred_valid:\n",
    "        print(\"** WARNING: Pred box is missing/invalid (จะไม่วาดกรอบแดง) **\")\n",
    "\n",
    "    handles = []\n",
    "    if gt_valid: handles.append(plt.Line2D([0], [0], color='blue', lw=2, label='GT'))\n",
    "    if pred_valid: handles.append(plt.Line2D([0], [0], color='red', lw=2, label=f'Pred (score>{threshold:.2f})'))\n",
    "    if handles:\n",
    "        ax.legend(handles=handles)\n",
    "    plt.title(\"GT (Blue) / Pred (Red)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "# def decode_caption(cap_ids, inv_vocab_dict, min_length=10):\n",
    "#     cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "#     tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "#     clean = []\n",
    "#     for i, w in enumerate(tokens):\n",
    "#         if w == '<eos>' and i < min_length: continue\n",
    "#         if w == '<eos>' or w == '<pad>': break\n",
    "#         if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "#     return clean\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def compute_metrics(pred_tokens, ref_tokens):\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "    meteor = meteor_score([ref_tokens], pred_tokens)\n",
    "    rscore = rouge.score(\" \".join(ref_tokens), \" \".join(pred_tokens))\n",
    "    rouge1 = rscore['rouge1'].fmeasure\n",
    "    rouge2 = rscore['rouge2'].fmeasure\n",
    "    rougel = rscore['rougeL'].fmeasure\n",
    "    return bleu, rouge1, rouge2, rougel, meteor\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "    union = boxAArea + boxBArea - interArea + 1e-6\n",
    "    return interArea / union if union > 0 else 0\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n",
    "\n",
    "def visualize_result(image, pred_caption, gt_caption, sev_pred, sev_gt,\n",
    "                     cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.16):\n",
    "    import matplotlib.patches as patches\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image)\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    # ไม่วาดกรอบสีเขียว, จะวาดแค่ตอน plot bbox GT vs pred เท่านั้น\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    if cam is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam * 255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_bbox_gt_pred(img, gt_box, pred_box, pred_score=None, threshold=0.16):\n",
    "    # --- Convert ภาพ & แสดงขนาดภาพ ---\n",
    "    if isinstance(img, str):\n",
    "        img = Image.open(img).convert('RGB')\n",
    "    elif torch.is_tensor(img):\n",
    "        img = tensor_to_pil(img)\n",
    "    elif isinstance(img, np.ndarray):\n",
    "        img = Image.fromarray(img)\n",
    "    # Print ขนาดภาพ (PIL: size = (W, H))\n",
    "    print(\"== plot_bbox_gt_pred ==\")\n",
    "    print(f\"Image size (PIL): {img.size}\")        # (width, height)\n",
    "    if hasattr(img, 'shape'):\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # -- Debug print พิกัด --\n",
    "    print(f\"GT box: {gt_box}\")\n",
    "    print(f\"Pred box: {pred_box}\")\n",
    "    if pred_score is not None:\n",
    "        print(f\"Pred score: {pred_score:.3f}\")\n",
    "\n",
    "    # -- GT Box: วาดแค่เมื่อพิกัด valid --\n",
    "    gt_valid = not (gt_box is None or np.allclose(gt_box, 0) or np.isnan(gt_box).any())\n",
    "    if gt_valid:\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        print(f\"GT box drawn at: ({gt_x1:.1f},{gt_y1:.1f},{gt_x2:.1f},{gt_y2:.1f})\")\n",
    "        rect_gt = plt.Rectangle((gt_x1, gt_y1), gt_x2-gt_x1, gt_y2-gt_y1,\n",
    "                                linewidth=2, edgecolor='blue', facecolor='none', label='GT')\n",
    "        ax.add_patch(rect_gt)\n",
    "    else:\n",
    "        print(\"** WARNING: GT box is missing/invalid (จะไม่วาดกรอบน้ำเงิน) **\")\n",
    "\n",
    "    # -- Pred Box --\n",
    "    pred_valid = not (pred_box is None or np.allclose(pred_box, 0) or np.isnan(pred_box).any())\n",
    "    if pred_valid and ((pred_score is None) or (pred_score > threshold)):\n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        print(f\"Pred box drawn at: ({pred_x1:.1f},{pred_y1:.1f},{pred_x2:.1f},{pred_y2:.1f})\")\n",
    "        rect_pred = plt.Rectangle((pred_x1, pred_y1), pred_x2-pred_x1, pred_y2-pred_y1,\n",
    "                                  linewidth=2, edgecolor='red', facecolor='none', label='Pred')\n",
    "        ax.add_patch(rect_pred)\n",
    "    elif not pred_valid:\n",
    "        print(\"** WARNING: Pred box is missing/invalid (จะไม่วาดกรอบแดง) **\")\n",
    "\n",
    "    handles = []\n",
    "    if gt_valid: handles.append(plt.Line2D([0], [0], color='blue', lw=2, label='GT'))\n",
    "    if pred_valid: handles.append(plt.Line2D([0], [0], color='red', lw=2, label=f'Pred (score>{threshold:.2f})'))\n",
    "    if handles:\n",
    "        ax.legend(handles=handles)\n",
    "    plt.title(\"GT (Blue) / Pred (Red)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- NLP METRIC ON VALIDATION SET (after train) ----\n",
    "# def decode_caption(cap_ids, inv_vocab_dict, min_length=10):\n",
    "#     cap_str = \"\".join([inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids])\n",
    "#     tokens = word_tokenize(cap_str, engine=\"newmm\")   # ลองเปลี่ยนเป็น deepcut หรือ attacut\n",
    "#     clean = []\n",
    "#     for i, w in enumerate(tokens):\n",
    "#         if w == '<eos>' and i < min_length: continue\n",
    "#         if w == '<eos>' or w == '<pad>': break\n",
    "#         if w not in {'<pad>', '<sos>'}: clean.append(w)\n",
    "#     return clean\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def compute_metrics(pred_tokens, ref_tokens):\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=SmoothingFunction().method1, weights=(0.5, 0.5, 0, 0))\n",
    "    meteor = meteor_score([ref_tokens], pred_tokens)\n",
    "    rscore = rouge.score(\" \".join(ref_tokens), \" \".join(pred_tokens))\n",
    "    rouge1 = rscore['rouge1'].fmeasure\n",
    "    rouge2 = rscore['rouge2'].fmeasure\n",
    "    rougel = rscore['rougeL'].fmeasure\n",
    "    return bleu, rouge1, rouge2, rougel, meteor\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = max(0, box1[2] - box1[0]) * max(0, box1[3] - box1[1])\n",
    "    boxBArea = max(0, box2[2] - box2[0]) * max(0, box2[3] - box2[1])\n",
    "    union = boxAArea + boxBArea - interArea + 1e-6\n",
    "    return interArea / union if union > 0 else 0\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n",
    "\n",
    "def visualize_result(image, pred_caption, gt_caption, sev_pred, sev_gt,\n",
    "                     cam=None, boxes=None, box_labels=None, box_scores=None, box_threshold=0.16):\n",
    "    import matplotlib.patches as patches\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert(\"RGB\")\n",
    "    else:\n",
    "        img = tensor_to_pil(image)\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    # ไม่วาดกรอบสีเขียว, จะวาดแค่ตอน plot bbox GT vs pred เท่านั้น\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Pred: {pred_caption}\\nGT: {gt_caption}\\nSeverity(P/G): {sev_pred}/{sev_gt}\")\n",
    "\n",
    "    if cam is not None:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cam_img = Image.fromarray(np.uint8(cam * 255)).resize(img.size, Image.BILINEAR)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(cam_img, alpha=0.5, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def tensor_to_pil(img_tensor):\n",
    "    if img_tensor.ndim == 4:\n",
    "        img_tensor = img_tensor[0]\n",
    "    img_tensor = img_tensor.detach().cpu()\n",
    "    if img_tensor.max() <= 1:\n",
    "        img_tensor = img_tensor * 255\n",
    "    img_tensor = img_tensor.to(torch.uint8)\n",
    "    return T.ToPILImage()(img_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va4JL2igHcGV"
   },
   "source": [
    "## Optimize decoder parameter **optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7ihsTGxHkjU"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # กำหนดช่วง parameter ที่จะค้นหา\n",
    "    top_k = trial.suggest_int(\"top_k\", 1, 10)\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.5, 2.0)\n",
    "    repetition_penalty = trial.suggest_float(\"repetition_penalty\", 1.0, 3.0)\n",
    "\n",
    "    # Reset metric\n",
    "    bleu_scores, rouge1s, rouge2s, rougels, meteors, accs, iou_scores, ap05_scores = [], [], [], [], [], [], [], []\n",
    "    all_sev_preds, all_sev_true = [], []\n",
    "\n",
    "    # ---- TEST LOOP ----\n",
    "    for batch in test_loader:\n",
    "        images, captions, severity, bbox_targets = batch[:4]\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        severity = severity.to(device)\n",
    "        bbox_targets = bbox_targets.to(device)\n",
    "        _, _, H, W = images.shape\n",
    "\n",
    "        for b in range(images.size(0)):\n",
    "            img_tensor = images[b].unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                generated_ids, severity_pred, bbox_pred = model.generate(\n",
    "                    img_tensor[0], max_len=50, device=device,\n",
    "                    top_k=top_k, temperature=temperature, repetition_penalty=repetition_penalty\n",
    "                )\n",
    "\n",
    "            pred_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "            ref_tokens  = decode_caption(captions[b].cpu().tolist(), inv_vocab_dict)\n",
    "            sev_pred = int(severity_pred)\n",
    "            sev_gt = int(severity[b].item())\n",
    "\n",
    "            gt_box = bbox_targets[b].detach().cpu().numpy()\n",
    "            pred_box = np.array(bbox_pred)\n",
    "            if np.max(gt_box) <= 1.2:\n",
    "                gt_box = [gt_box[0]*W, gt_box[1]*H, gt_box[2]*W, gt_box[3]*H]\n",
    "            if np.max(pred_box) <= 1.2:\n",
    "                pred_box = [pred_box[0]*W, pred_box[1]*H, pred_box[2]*W, pred_box[3]*H]\n",
    "            iou = box_iou(pred_box, gt_box)\n",
    "            ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "\n",
    "            bleu, r1, r2, rl, meteor = compute_metrics(pred_tokens, ref_tokens)\n",
    "            acc = 1 if sev_pred == sev_gt else 0\n",
    "\n",
    "            bleu_scores.append(bleu)\n",
    "            rouge1s.append(r1)\n",
    "            rouge2s.append(r2)\n",
    "            rougels.append(rl)\n",
    "            meteors.append(meteor)\n",
    "            accs.append(acc)\n",
    "            iou_scores.append(iou)\n",
    "            ap05_scores.append(ap_05)\n",
    "            all_sev_preds.append(sev_pred)\n",
    "            all_sev_true.append(sev_gt)\n",
    "\n",
    "    # ---- Objective: จะเลือกวิธีรวม metric (เช่นเฉลี่ย หรือ กำหนด weight เอง)\n",
    "    metric_score = (\n",
    "        np.mean(bleu_scores) +\n",
    "        np.mean(rouge1s) +\n",
    "        np.mean(rouge2s) +\n",
    "        np.mean(rougels) +\n",
    "        np.mean(meteors)\n",
    "    ) / 5  # เฉลี่ยทุก metric\n",
    "\n",
    "    # หรือถ้าจะ maximize BLEU อย่างเดียว:\n",
    "    # metric_score = np.mean(bleu_scores)\n",
    "\n",
    "    return metric_score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)  # n_trials กำหนดจำนวนรอบ\n",
    "\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "print(\"Best score:\", study.best_trial.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_Qh195cNjB9"
   },
   "source": [
    "## test auto regressive loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "17fEGmUL3TpZwpB35mnR21QpsaFQ7Jgb8"
    },
    "executionInfo": {
     "elapsed": 117788,
     "status": "ok",
     "timestamp": 1749782995971,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "_IQ0Jar4XQ7D",
    "outputId": "3614813e-36de-4348-ed10-434242a77c6e"
   },
   "outputs": [],
   "source": [
    "import csv, numpy as np, matplotlib.pyplot as plt, matplotlib.font_manager as fm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "# ---- METRIC LOG ----\n",
    "\n",
    "bleu_scores, rouge1s, rouge2s, rougels, meteors, accs = [], [], [], [], [], []\n",
    "iou_scores, ap05_scores = [], []\n",
    "all_sev_preds, all_sev_true = [], []\n",
    "show = 0\n",
    "\n",
    "# ---- ฟอนต์ไทย ----\n",
    "fm.fontManager.addfont('thsarabunnew-webfont.ttf')\n",
    "plt.rcParams['font.family'] = 'TH Sarabun New'\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = f\"/content/drive/MyDrive/Final_Deep_project/experiment-log/test_metrics_log_{now}.csv\"\n",
    "print(\"Log test metric to:\", csv_path)\n",
    "\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'image_path', 'bleu', 'rouge1', 'rouge2', 'rougeL', 'meteor',\n",
    "        'severity_acc', 'iou', 'ap@0.5', 'caption_pred', 'caption_gt', 'sev_pred', 'sev_gt'\n",
    "    ])\n",
    "\n",
    "\n",
    "inv_vocab_dict = {idx: word for word, idx in vocab.items()}\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "# --- อัปเดตเส้นทางของ weights ---\n",
    "cnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\"\n",
    "frcnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\"\n",
    "vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\"\n",
    "main_model_path = \"/content/drive/MyDrive/Final_Deep_project/experiment-log/bestmodel_20250613_023021.pth\"  # main DKICNet weight\n",
    "\n",
    "# --- สร้างโมเดล DKICNet ---\n",
    "vocab_size = 265  # ต้องตรงกับตอนเทรน\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    vit_pretrained_path=vit_pretrained_path,\n",
    "    decoder_type=\"lstm\"  #<------ แก้ decoder ให้ตรง\n",
    ")\n",
    "\n",
    "# ✅ โหลด weights ของ DKICNet\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"✅ Loaded model weights from 'model_state_dict' key.\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"✅ Loaded model weights directly.\")\n",
    "\n",
    "# --- เตรียมอุปกรณ์ & eval mode ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# ---- metrics, buffer ----\n",
    "bleu_scores, rouge1s, rouge2s, rougels, meteors, accs = [], [], [], [], [], []\n",
    "iou_scores, ap05_scores = [], []\n",
    "all_sev_preds, all_sev_true = [], []\n",
    "show = 0\n",
    "\n",
    "# ---- TEST LOOP ----\n",
    "for batch in test_loader:\n",
    "    images, captions, severity, bbox_targets = batch[:4]\n",
    "    image_paths = batch[4] if len(batch) > 4 else None\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    severity = severity.to(device)\n",
    "    bbox_targets = bbox_targets.to(device)\n",
    "    _, _, H, W = images.shape\n",
    "\n",
    "    for b in range(images.size(0)):\n",
    "        # ------- รองรับ ViT/Multimodal ได้หมด --------\n",
    "        img_tensor = images[b].unsqueeze(0)  # [1, 3, H, W] ส่งเข้า model.generate ทีละภาพ\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids, severity_pred, bbox_pred = model.generate(\n",
    "                img_tensor, max_len=50, device=device, top_k=5, temperature=0.8, repetition_penalty=1.6\n",
    "            )\n",
    "\n",
    "        pred_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "        ref_tokens  = decode_caption(captions[b].cpu().tolist(), inv_vocab_dict)\n",
    "        pred_str = \"\".join(pred_tokens)\n",
    "        gt_str = \"\".join(ref_tokens)\n",
    "        sev_pred = int(severity_pred)\n",
    "        sev_gt = int(severity[b].item())\n",
    "\n",
    "        img_display = image_paths[b] if image_paths is not None else images[b]\n",
    "\n",
    "        gt_box = bbox_targets[b].detach().cpu().numpy()\n",
    "        pred_box = np.array(bbox_pred)\n",
    "\n",
    "        if np.max(gt_box) <= 1.2:\n",
    "            gt_box = [gt_box[0]*W, gt_box[1]*H, gt_box[2]*W, gt_box[3]*H]\n",
    "        if np.max(pred_box) <= 1.2:\n",
    "            pred_box = [pred_box[0]*W, pred_box[1]*H, pred_box[2]*W, pred_box[3]*H]\n",
    "\n",
    "        iou = box_iou(pred_box, gt_box)\n",
    "        ap_05 = 1.0 if iou > 0.5 else 0.0\n",
    "\n",
    "        bleu, r1, r2, rl, meteor = compute_metrics(pred_tokens, ref_tokens)\n",
    "        acc = 1 if sev_pred == sev_gt else 0\n",
    "\n",
    "        all_sev_preds.append(sev_pred)\n",
    "        all_sev_true.append(sev_gt)\n",
    "        iou_scores.append(iou)\n",
    "        ap05_scores.append(ap_05)\n",
    "\n",
    "        with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([img_display, bleu, r1, r2, rl, meteor, acc, iou, ap_05, pred_str, gt_str, sev_pred, sev_gt])\n",
    "\n",
    "        bleu_scores.append(bleu)\n",
    "        rouge1s.append(r1)\n",
    "        rouge2s.append(r2)\n",
    "        rougels.append(rl)\n",
    "        meteors.append(meteor)\n",
    "        accs.append(acc)\n",
    "\n",
    "\n",
    "        # ---- GT vs Pred plot (แสดง 20 รูปแรก) ----\n",
    "        if show < 20:\n",
    "            plot_bbox_gt_pred(img_display, gt_box, pred_box, pred_score=None, threshold=0.16)\n",
    "            # GradCAM (optional)\n",
    "            img_t = images[b].unsqueeze(0)\n",
    "            cam = generate_gradcam(model.cnn_encoder, img_t)\n",
    "            visualize_result(\n",
    "                img_display, pred_str, gt_str, sev_pred, sev_gt,\n",
    "                cam=cam,\n",
    "                boxes=None,\n",
    "                box_labels=None,\n",
    "                box_scores=None,\n",
    "                box_threshold=0.16\n",
    "            )\n",
    "            try:\n",
    "                plot_vit_attention(model, img_t, image_display=img_display)\n",
    "\n",
    "                if torch.is_tensor(img_t):\n",
    "                    img = tensor_to_pil(img_t)\n",
    "                    show_vit_attention_map(\n",
    "                        img=img,                    # PIL\n",
    "                        vit_model=model.vit_encoder,\n",
    "                        img_tensor=img_tensor,\n",
    "                        patch_size=16,              # หรือ 14, 32 แล้วแต่ vit\n",
    "                        head=0                      # หรือเปลี่ยนเป็น 1,2,... ดูแต่ละ head ได้\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(\"ViT attention plot error:\", e)\n",
    "            show += 1\n",
    "\n",
    "\n",
    "# ---- SUMMARY REPORT ----\n",
    "def report_metric(name, arr):\n",
    "    arr = np.array(arr)\n",
    "    print(f\"{name:10s} | Mean: {arr.mean():.4f} | Std: {arr.std():.4f}\")\n",
    "\n",
    "print(\"\\n====== SUMMARY TEST RESULT ======\")\n",
    "report_metric(\"BLEU\", bleu_scores)\n",
    "report_metric(\"ROUGE-1\", rouge1s)\n",
    "report_metric(\"ROUGE-2\", rouge2s)\n",
    "report_metric(\"ROUGE-L\", rougels)\n",
    "report_metric(\"METEOR\", meteors)\n",
    "report_metric(\"SeverityAcc\", accs)\n",
    "report_metric(\"IOU\", iou_scores)\n",
    "report_metric(\"AP@0.5\", ap05_scores)\n",
    "\n",
    "# ----- F1, Precision, Recall, Classification Report -----\n",
    "precision, recall, f1, support = precision_recall_fscore_support(all_sev_true, all_sev_preds, average=None, zero_division=0)\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_sev_true, all_sev_preds, average='macro', zero_division=0)\n",
    "print(\"\\n--- Severity Classification Metrics (per class) ---\")\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i} | Precision: {precision[i]:.4f} | Recall: {recall[i]:.4f} | F1: {f1[i]:.4f} | Support: {support[i]}\")\n",
    "print(\"\\nMacro  | Precision: {:.4f} | Recall: {:.4f} | F1: {:.4f}\".format(precision_macro, recall_macro, f1_macro))\n",
    "print(\"\\nClassification report\\n\", classification_report(all_sev_true, all_sev_preds, digits=4))\n",
    "\n",
    "# ---- CONFUSION MATRIX ----\n",
    "cm = confusion_matrix(all_sev_true, all_sev_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Severity Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ---- HISTOGRAM ----\n",
    "plt.figure()\n",
    "plt.hist(bleu_scores, bins=20, alpha=0.7, label='BLEU')\n",
    "plt.hist(rouge1s, bins=20, alpha=0.7, label='ROUGE-1')\n",
    "plt.hist(rouge2s, bins=20, alpha=0.7, label='ROUGE-2')\n",
    "plt.hist(rougels, bins=20, alpha=0.7, label='ROUGE-L')\n",
    "plt.hist(meteors, bins=20, alpha=0.7, label='METEOR')\n",
    "# plt.hist(iou_scores, bins=20, alpha=0.7, label='IOU')\n",
    "# plt.hist(ap05_scores, bins=2, alpha=0.7, label='AP@0.5')\n",
    "plt.legend()\n",
    "plt.title(\"Test Set Metrics Distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYeoO0odFyb_"
   },
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHEYwrJga7LP"
   },
   "source": [
    "### Load pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYYF-Skla6Ok"
   },
   "outputs": [],
   "source": [
    "# --- อัปเดตเส้นทางของ weights ---\n",
    "cnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/RestNet50/best_resnet50_model_20250608_123252.pth\"\n",
    "frcnn_weight_path = \"/content/drive/MyDrive/Final_Deep_project/FasterRCNN/fasterrcnn_trained.pth\"\n",
    "vit_pretrained_path=\"/content/drive/MyDrive/Final_Deep_project/experiment-log/vit_finetuned.pth\"\n",
    "main_model_path = \"/content/drive/MyDrive/Final_Deep_project/experiment-log/bestmodel_20250613_023021.pth\"  # main DKICNet weight\n",
    "decoder_type=\"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yJsS_Nja_k_"
   },
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6in3mNFpGjAv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def decode_caption(cap_ids, inv_vocab_dict, min_length=16):\n",
    "    tokens = [inv_vocab_dict.get(idx, '<unk>') for idx in cap_ids]\n",
    "    clean = []\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w in {'<pad>', '<sos>'}:\n",
    "            continue\n",
    "        if w == '<eos>':\n",
    "            if len(clean) >= min_length:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        clean.append(w)\n",
    "    return clean\n",
    "\n",
    "def plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred, threshold=0.1):\n",
    "    # แปลง tensor → PIL\n",
    "    if image_tensor.dim() == 4:\n",
    "        image_tensor = image_tensor[0]\n",
    "    img_pil = to_pil_image(image_tensor.cpu())\n",
    "\n",
    "    # สร้าง plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "    # วาด bounding box (ถ้าพิกัดไม่เป็นศูนย์หมด)\n",
    "    if bbox_pred and not all(v == 0 for v in bbox_pred):\n",
    "        x1, y1, x2, y2 = bbox_pred\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none', label='Pred Box')\n",
    "        ax.add_patch(rect)\n",
    "        plt.legend()\n",
    "\n",
    "    # ใส่ข้อความ caption + severity บนภาพ\n",
    "    ax.text(5, 20, f\"Caption: {caption_str}\", fontsize=12, color='white', backgroundcolor='black')\n",
    "    ax.text(5, 45, f\"Severity: {severity_cls}\", fontsize=12, color='white', backgroundcolor='black')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Model Inference Result\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJvzDX2LbC34"
   },
   "source": [
    "### plot test unseen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "executionInfo": {
     "elapsed": 26922,
     "status": "ok",
     "timestamp": 1749783097304,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "anhyVHcuBOPd",
    "outputId": "e5f649bb-7f0a-4406-96b6-7f6a4be1c663"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# --- สร้างโมเดล DKICNet ---\n",
    "vocab_size = 265  # ต้องตรงกับตอนเทรน\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    vit_pretrained_path=vit_pretrained_path,\n",
    "    decoder_type=decoder_type\n",
    ")\n",
    "\n",
    "# ✅ โหลด weights ของ DKICNet\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"✅ Loaded model weights from 'model_state_dict' key.\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(\"✅ Loaded model weights directly.\")\n",
    "\n",
    "# --- เตรียมอุปกรณ์ & eval mode ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- เตรียมภาพสำหรับ inference ---\n",
    "# image_path = \"/content/drive/MyDrive/Final_Deep_project/Traffy/Traffy_cracked_image/backup/2025-733RNV.jpg\"\n",
    "\n",
    "\n",
    "image_path = \"/content/drive/MyDrive/Final_Deep_project/Traffy/Traffy_cracked_image/selected/2025-ZWW3XT.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image).to(device)\n",
    "\n",
    "# --- Run inference ---\n",
    "generated_ids, severity_cls, bbox_pred =  model.generate(\n",
    "    img_tensor[0], max_len=50, device=device, top_k=1, temperature=1\n",
    ")\n",
    "# --- Decode คำบรรยาย ---\n",
    "caption_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "caption_str = \"\".join(caption_tokens)\n",
    "\n",
    "# --- แปลง bbox เป็นพิกัดภาพ (ถ้า normalized) ---\n",
    "_, H, W = image_tensor.shape\n",
    "if max(bbox_pred) <= 1.2:\n",
    "    bbox_pred = [bbox_pred[0]*W, bbox_pred[1]*H, bbox_pred[2]*W, bbox_pred[3]*H]\n",
    "\n",
    "# --- แสดงผลลัพธ์ ---\n",
    "print(\"== DEBUG ==\")\n",
    "print(\"Raw IDs               :\", generated_ids)\n",
    "print(\"Decoded tokens        :\", caption_tokens)\n",
    "print(\"Predicted Caption     :\", caption_str)\n",
    "print(\"Predicted Severity    :\", severity_cls)\n",
    "print(\"Predicted BoundingBox :\", bbox_pred)\n",
    "\n",
    "# --- Plot ผลลัพธ์ ---\n",
    "plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpBYfxWPw-0p"
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "executionInfo": {
     "elapsed": 4838,
     "status": "ok",
     "timestamp": 1749783249228,
     "user": {
      "displayName": "Max Vorabhol",
      "userId": "04711738882067948674"
     },
     "user_tz": -420
    },
    "id": "oNwGo9LxHO7E",
    "outputId": "a9696827-5ecb-44f1-8880-b24e4c5773a6"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "torch.manual_seed(314)\n",
    "\n",
    "# --- Import module/model ที่คุณสร้างเอง ตรงนี้ต้อง import ให้ถูก ---\n",
    "# from yourmodule import DKICNet, decode_caption, inv_vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "# === โหลด vocab dict ===\n",
    "# inv_vocab_dict = torch.load(\"/content/drive/MyDrive/Final_Deep_project/vocab_inv_dict.pth\") # ถ้ามีไฟล์นี้\n",
    "\n",
    "# === สร้างและโหลดโมเดล ===\n",
    "model = DKICNet(\n",
    "    cnn_backbone=\"resnet50\",\n",
    "    num_classes=4,\n",
    "    vocab_size=vocab_size,\n",
    "    severity_classes=4,\n",
    "    cnn_pretrained_path=cnn_weight_path,\n",
    "    frcnn_pretrained_path=frcnn_weight_path,\n",
    "    vit_pretrained_path=vit_pretrained_path,\n",
    "    decoder_type=decoder_type\n",
    ")\n",
    "checkpoint = torch.load(main_model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint.get(\"model_state_dict\", checkpoint))\n",
    "print(\"✅ Loaded model weights.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Transform ภาพ ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === ฟังก์ชันวาดผลลัพธ์ ===\n",
    "def plot_inference_result(image_tensor, caption_str, severity_cls, bbox_pred):\n",
    "    img_pil = to_pil_image(image_tensor.cpu())\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(img_pil)\n",
    "\n",
    "    if bbox_pred and not all(v == 0 for v in bbox_pred):\n",
    "        x1, y1, x2, y2 = bbox_pred\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.text(5, 20, f\"Caption: {caption_str}\", fontsize=10, color='white', backgroundcolor='black')\n",
    "    ax.text(5, 45, f\"Severity: {severity_cls}\", fontsize=10, color='white', backgroundcolor='black')\n",
    "    ax.axis('off')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # -- fixed: ใช้ buffer_rgba แทน tostring_rgb\n",
    "    img_np = np.array(fig.canvas.buffer_rgba())[..., :3]\n",
    "    plt.close(fig)\n",
    "    return img_np\n",
    "\n",
    "# === ฟังก์ชันหลักสำหรับ inference (เพิ่ม top_k, temperature) ===\n",
    "def gradio_inference(image, top_k, temperature, repetitive_penalty):\n",
    "    try:\n",
    "        # 1. Transform\n",
    "        image_tensor = transform(image).to(device)\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "        # 2. Run inference (ส่ง top_k, temperature)\n",
    "        generated_ids, severity_cls, bbox_pred = model.generate(\n",
    "            image_tensor[0], max_len=50, device=device, top_k=int(top_k), temperature=float(temperature), repetition_penalty=float(repetitive_penalty)\n",
    "        )\n",
    "\n",
    "        caption_tokens = decode_caption(generated_ids, inv_vocab_dict)\n",
    "        caption_str = \"\".join(caption_tokens)\n",
    "\n",
    "        _, H, W = image_tensor.shape[-3:]\n",
    "        if max(bbox_pred) <= 1.2:\n",
    "            bbox_pred = [bbox_pred[0]*W, bbox_pred[1]*H, bbox_pred[2]*W, bbox_pred[3]*H]\n",
    "\n",
    "        img_np = plot_inference_result(image_tensor[0], caption_str, severity_cls, bbox_pred)\n",
    "        return img_np, caption_str, str(severity_cls), str(bbox_pred)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        err = f\"[ERROR]\\n{traceback.format_exc()}\"\n",
    "        print(err)\n",
    "        return np.zeros((224,224,3),dtype=np.uint8), err, \"ERROR\", \"ERROR\"\n",
    "\n",
    "# === Gradio interface (เพิ่ม slider สำหรับ top_k, temperature) ===\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_inference,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"📷 Upload bridge image\"),\n",
    "        gr.Slider(1, 20, value=1, step=1, label=\"top_k (sampling)\"),\n",
    "        gr.Slider(0.1, 2.0, value=1.0, step=0.05, label=\"temperature (sampling)\"),\n",
    "        gr.Slider(1,3, value=1.0, step=0.05, label=\"repetitive penalty\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(type=\"numpy\", label=\"📌 Prediction Result\"),\n",
    "        gr.Textbox(label=\"📝 Caption\"),\n",
    "        gr.Textbox(label=\"🔥 Severity Class\"),\n",
    "        gr.Textbox(label=\"🧭 Bounding Box\")\n",
    "    ],\n",
    "    title=\"Crack Damage Caption Generator\",\n",
    "    description=\"อัปโหลด  → ระบบจะอธิบายความเสียหาย พร้อมพยากรณ์ความรุนแรง และกรอบตำแหน่ง\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPlLeRaqmKPB7k2Wmax/IB5",
   "gpuType": "L4",
   "provenance": [
    {
     "file_id": "1zh7Zu9uwwRP-T01zgnUFavhqdlBXvX8L",
     "timestamp": 1749699607378
    },
    {
     "file_id": "11N0iPsQsBvNXqloZ0GGHKQkecePy9_AU",
     "timestamp": 1749692538802
    },
    {
     "file_id": "1ikpa4adMRiggogf28UPumCzcfwW2_0vg",
     "timestamp": 1749648377203
    },
    {
     "file_id": "1Ni1kpMwOyPzEVGuXEqvkaPSxouee5J0L",
     "timestamp": 1749645974509
    },
    {
     "file_id": "19vbINb0bKgb_rvYBfL7U62YcZOgLpfLg",
     "timestamp": 1749522228176
    },
    {
     "file_id": "1NcuU4ag0XBS0UrZlFN_nSq7OYuMMzIS_",
     "timestamp": 1749512402692
    },
    {
     "file_id": "1460tRfu5PAgva40B_zz5kuO63t0qlWBV",
     "timestamp": 1749476801985
    },
    {
     "file_id": "1WRfPmo51tI8I286iC8-4vsXqG9gjfvE5",
     "timestamp": 1749453036817
    },
    {
     "file_id": "1OjTurtXyu_J0x5zhW-qOhwn41GVknOUm",
     "timestamp": 1749432614736
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
